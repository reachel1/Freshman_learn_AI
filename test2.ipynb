{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPALeRXLejnfBDV4i2hUYeG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reachel1/Freshman_learn_AI/blob/main/test2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdOhHLLNS-pf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHO9ZcicUtvv",
        "outputId": "52c15fc4-3609-490b-ede2-6081159047e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jul 12 06:47:29 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "# Importing drive method from colab for accessing google drive\n",
        "from google.colab import drive\n",
        "# Mounting drive\n",
        "# This will require authentication : Follow the steps as guided\n",
        "drive.mount('/Data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP7T3VlxWiCp",
        "outputId": "879342cb-fb79-48ee-9538-5e5d6e6286e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /Data/'My Drive'/dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBSdroL7fSA8",
        "outputId": "d04988f8-8258-4b57-e0ef-c6d521c6ddcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " cifar-10-batches-py   cifar-10-python.tar.gz  'LeGO-LOAM Public Dataset'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "import gradio as gr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ9X6NLculhz",
        "outputId": "1fb2663b-9941-4a21-e897-c5846869b259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-3.36.1-py3-none-any.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles (from gradio)\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from gradio) (3.8.4)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.100.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.2.7 (from gradio)\n",
            "  Downloading gradio_client-0.2.8-py3-none-any.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.8/288.8 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.14.0 (from gradio)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.0.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gradio) (1.22.4)\n",
            "Collecting orjson (from gradio)\n",
            "  Downloading orjson-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio) (8.4.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.11)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.14.0)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gradio) (2.27.1)\n",
            "Collecting semantic-version (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.0 (from gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (2023.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (23.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.65.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio)\n",
            "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2022.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.4)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (2023.5.7)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio)\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (3.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gradio) (1.26.16)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio)\n",
            "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->gradio) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio) (1.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4694 sha256=98395a03161d24f9575d6369d76e4ea1188cba3218b8f857f3f7a70c199e2b2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/c2/0e/3b9c6845c6a4e35beb90910cc70d9ac9ab5d47402bd62af0df\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uc-micro-py, semantic-version, python-multipart, orjson, markdown-it-py, h11, aiofiles, uvicorn, starlette, mdit-py-plugins, linkify-it-py, huggingface-hub, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "Successfully installed aiofiles-23.1.0 fastapi-0.100.0 ffmpy-0.3.0 gradio-3.36.1 gradio-client-0.2.8 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 huggingface-hub-0.16.4 linkify-it-py-2.0.2 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 orjson-3.9.2 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.27.0 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt # plt 用于显示图片\n",
        "import matplotlib.image as mpimg # mpimg 用于读取图片\n",
        "import numpy as np\n",
        "\n",
        "#resize功能\n",
        "from scipy import misc\n",
        "from PIL import Image\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "num_classes = 10\n",
        "# 超参数设置\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "learning_rate = 0.005"
      ],
      "metadata": {
        "id": "y9tms4NwcMFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "# 数据预处理\n",
        "transform0 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "    ])\n",
        "transform1 = transforms.Compose([\n",
        "    #图形尺寸填充 填充至36x36\n",
        "    transforms.Pad(4),\n",
        "    #随机水平翻转\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    #随机裁剪 裁剪至32x32\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "    ])\n",
        "transform1 = transforms.Compose([\n",
        "    # 随机亮度调整\n",
        "    transforms.ColorJitter(brightness=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "TxoYrdwKits0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOAD_CIFAR = True\n",
        "DOWNLOAD_CIFAR = False\n",
        "# 从data继承读取数据集的类\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "#训练集数据导入 用两种方法实现数据集扩充\n",
        "train_data0 = torchvision.datasets.CIFAR10(root='/Data/My Drive/dataset',train=True,\n",
        "                     download=DOWNLOAD_CIFAR, transform=transform0)\n",
        "train_data1 = torchvision.datasets.CIFAR10(root='/Data/My Drive/dataset',train=True,\n",
        "                     download=DOWNLOAD_CIFAR, transform=transform1)\n",
        "train_data2 = torchvision.datasets.CIFAR10(root='/Data/My Drive/dataset',train=True,\n",
        "                     download=DOWNLOAD_CIFAR, transform=transform2)\n",
        "concat_trainset = torch.utils.data.ConcatDataset([train_data0, train_data1, train_data2])\n",
        "\n",
        "# 测试数据集\n",
        "test_data = torchvision.datasets.CIFAR10(\n",
        "    root='/Data/My Drive/dataset',\n",
        "    train=False,\n",
        "    transform=transform0\n",
        ")"
      ],
      "metadata": {
        "id": "c0IVXuIfcbtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练数据加载器\n",
        "train_loader = torch.utils.data.DataLoader(dataset=concat_trainset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "# 测试数据加载器\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "UJDkPPJiBbqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8zFHdGFA1AK"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "#网络定义Resnet\n",
        "\n",
        "#定义残差块\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1):  #需要判断是否需要1×1的卷积\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
        "                    kernel_size=3, padding=1, stride=strides)\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
        "                    kernel_size=3, padding=1)\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
        "                        kernel_size=1, stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        Y += X                                                 ###############################################可以去掉 不存在残差\n",
        "        return F.relu(Y)\n",
        "\n",
        "\n",
        "\n",
        "def resnet_block(input_channels, num_channels, num_residuals, first_block=False):\n",
        "    blk = []\n",
        "    for i in range(num_residuals):\n",
        "        if i == 0 and not first_block:\n",
        "            blk.append(Residual(input_channels, num_channels,\n",
        "                                use_1x1conv=True, strides=2))\n",
        "        else:\n",
        "            blk.append(Residual(num_channels, num_channels))\n",
        "    return blk\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.b1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
        "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
        "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "        self.b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
        "        self.b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
        "        self.b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
        "        self.b5 = nn.Sequential(*resnet_block(256, 512, 2))\n",
        "        self.linear = nn.Linear(512, 10)\n",
        "        self.Aavgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.b1(x)\n",
        "        x = self.b2(x)\n",
        "        x = self.b3(x)\n",
        "        x = self.b4(x)\n",
        "        x = self.b5(x)\n",
        "        x = self.Aavgpool(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 实例化一个模型\n",
        "model = ResNet()"
      ],
      "metadata": {
        "id": "VK07kYOHk2Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义损失函数和优化器\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# 自动调整学习率\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.9)"
      ],
      "metadata": {
        "id": "2qUHkTdhksvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置cuda-gpu\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTuV8mucco2z",
        "outputId": "38ea8ba1-e693-426d-bbc8-46c4ab6ac036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Wed Jul 12 08:46:23 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P0    28W /  70W |   1161MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 开始训练\n",
        "model = model.cuda()\n",
        "# 存储损失与精度\n",
        "loss_history = []\n",
        "accuracy_history = []\n",
        "loss_times_history = []\n",
        "\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        # 前向传播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 反向传播和优化\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #累计损失\n",
        "        running_loss += loss.item()\n",
        "        loss_times_history.append(loss.item())\n",
        "        # 计算准确率\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        if (i+1) % 1000 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "    #scheduler.step()\n",
        "    #计算各epoch中的平均损失值和准确率\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100.0 * correct_train / total_train\n",
        "    #存储平均损失值和准确率\n",
        "    loss_history.append(avg_loss)\n",
        "    accuracy_history.append(accuracy)\n",
        "    #输出学习率\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Learning Rate: {scheduler.get_lr()[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbRva835lY5b",
        "outputId": "1e51a854-566e-4daf-e0ba-ae8fbd096056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Step [1000/10000], Loss: 2.7288\n",
            "Epoch [1/100], Step [2000/10000], Loss: 2.6874\n",
            "Epoch [1/100], Step [3000/10000], Loss: 1.8255\n",
            "Epoch [1/100], Step [4000/10000], Loss: 1.4385\n",
            "Epoch [1/100], Step [5000/10000], Loss: 2.1798\n",
            "Epoch [1/100], Step [6000/10000], Loss: 1.6498\n",
            "Epoch [1/100], Step [7000/10000], Loss: 1.7780\n",
            "Epoch [1/100], Step [8000/10000], Loss: 1.5906\n",
            "Epoch [1/100], Step [9000/10000], Loss: 1.3219\n",
            "Epoch [1/100], Step [10000/10000], Loss: 1.3705\n",
            "Epoch [2/100], Step [1000/10000], Loss: 1.2136\n",
            "Epoch [2/100], Step [2000/10000], Loss: 1.7443\n",
            "Epoch [2/100], Step [3000/10000], Loss: 0.9180\n",
            "Epoch [2/100], Step [4000/10000], Loss: 0.9620\n",
            "Epoch [2/100], Step [5000/10000], Loss: 1.6429\n",
            "Epoch [2/100], Step [6000/10000], Loss: 1.0339\n",
            "Epoch [2/100], Step [7000/10000], Loss: 1.5249\n",
            "Epoch [2/100], Step [8000/10000], Loss: 0.7684\n",
            "Epoch [2/100], Step [9000/10000], Loss: 2.3522\n",
            "Epoch [2/100], Step [10000/10000], Loss: 0.8433\n",
            "Epoch [3/100], Step [1000/10000], Loss: 1.5524\n",
            "Epoch [3/100], Step [2000/10000], Loss: 1.0021\n",
            "Epoch [3/100], Step [3000/10000], Loss: 0.3811\n",
            "Epoch [3/100], Step [4000/10000], Loss: 0.8785\n",
            "Epoch [3/100], Step [5000/10000], Loss: 1.6305\n",
            "Epoch [3/100], Step [6000/10000], Loss: 2.8835\n",
            "Epoch [3/100], Step [7000/10000], Loss: 1.6989\n",
            "Epoch [3/100], Step [8000/10000], Loss: 1.2253\n",
            "Epoch [3/100], Step [9000/10000], Loss: 2.3024\n",
            "Epoch [3/100], Step [10000/10000], Loss: 0.7023\n",
            "Epoch [4/100], Step [1000/10000], Loss: 1.1789\n",
            "Epoch [4/100], Step [2000/10000], Loss: 1.0448\n",
            "Epoch [4/100], Step [3000/10000], Loss: 0.9762\n",
            "Epoch [4/100], Step [4000/10000], Loss: 2.9383\n",
            "Epoch [4/100], Step [5000/10000], Loss: 1.2026\n",
            "Epoch [4/100], Step [6000/10000], Loss: 0.8413\n",
            "Epoch [4/100], Step [7000/10000], Loss: 0.5990\n",
            "Epoch [4/100], Step [8000/10000], Loss: 0.4008\n",
            "Epoch [4/100], Step [9000/10000], Loss: 1.6289\n",
            "Epoch [4/100], Step [10000/10000], Loss: 2.0929\n",
            "Epoch [5/100], Step [1000/10000], Loss: 1.8649\n",
            "Epoch [5/100], Step [2000/10000], Loss: 1.7982\n",
            "Epoch [5/100], Step [3000/10000], Loss: 1.9070\n",
            "Epoch [5/100], Step [4000/10000], Loss: 1.3272\n",
            "Epoch [5/100], Step [5000/10000], Loss: 0.8303\n",
            "Epoch [5/100], Step [6000/10000], Loss: 0.9228\n",
            "Epoch [5/100], Step [7000/10000], Loss: 1.0676\n",
            "Epoch [5/100], Step [8000/10000], Loss: 1.4108\n",
            "Epoch [5/100], Step [9000/10000], Loss: 0.7810\n",
            "Epoch [5/100], Step [10000/10000], Loss: 1.0124\n",
            "Epoch [6/100], Step [1000/10000], Loss: 1.7105\n",
            "Epoch [6/100], Step [2000/10000], Loss: 0.7483\n",
            "Epoch [6/100], Step [3000/10000], Loss: 1.5407\n",
            "Epoch [6/100], Step [4000/10000], Loss: 0.8765\n",
            "Epoch [6/100], Step [5000/10000], Loss: 1.0707\n",
            "Epoch [6/100], Step [6000/10000], Loss: 0.8681\n",
            "Epoch [6/100], Step [7000/10000], Loss: 0.6091\n",
            "Epoch [6/100], Step [8000/10000], Loss: 0.7211\n",
            "Epoch [6/100], Step [9000/10000], Loss: 1.2787\n",
            "Epoch [6/100], Step [10000/10000], Loss: 0.3607\n",
            "Epoch [7/100], Step [1000/10000], Loss: 0.4305\n",
            "Epoch [7/100], Step [2000/10000], Loss: 1.3998\n",
            "Epoch [7/100], Step [3000/10000], Loss: 0.4640\n",
            "Epoch [7/100], Step [4000/10000], Loss: 1.6839\n",
            "Epoch [7/100], Step [5000/10000], Loss: 1.9672\n",
            "Epoch [7/100], Step [6000/10000], Loss: 2.2341\n",
            "Epoch [7/100], Step [7000/10000], Loss: 1.7873\n",
            "Epoch [7/100], Step [8000/10000], Loss: 0.3676\n",
            "Epoch [7/100], Step [9000/10000], Loss: 0.8288\n",
            "Epoch [7/100], Step [10000/10000], Loss: 1.2240\n",
            "Epoch [8/100], Step [1000/10000], Loss: 0.9079\n",
            "Epoch [8/100], Step [2000/10000], Loss: 0.5831\n",
            "Epoch [8/100], Step [3000/10000], Loss: 1.6355\n",
            "Epoch [8/100], Step [4000/10000], Loss: 0.5893\n",
            "Epoch [8/100], Step [5000/10000], Loss: 1.3540\n",
            "Epoch [8/100], Step [6000/10000], Loss: 0.1501\n",
            "Epoch [8/100], Step [7000/10000], Loss: 0.8474\n",
            "Epoch [8/100], Step [8000/10000], Loss: 0.9310\n",
            "Epoch [8/100], Step [9000/10000], Loss: 0.3765\n",
            "Epoch [8/100], Step [10000/10000], Loss: 1.6813\n",
            "Epoch [9/100], Step [1000/10000], Loss: 0.9698\n",
            "Epoch [9/100], Step [2000/10000], Loss: 2.0777\n",
            "Epoch [9/100], Step [3000/10000], Loss: 0.8117\n",
            "Epoch [9/100], Step [4000/10000], Loss: 0.7316\n",
            "Epoch [9/100], Step [5000/10000], Loss: 0.5961\n",
            "Epoch [9/100], Step [6000/10000], Loss: 1.2002\n",
            "Epoch [9/100], Step [7000/10000], Loss: 1.1174\n",
            "Epoch [9/100], Step [8000/10000], Loss: 0.9464\n",
            "Epoch [9/100], Step [9000/10000], Loss: 0.6390\n",
            "Epoch [9/100], Step [10000/10000], Loss: 0.7924\n",
            "Epoch [10/100], Step [1000/10000], Loss: 1.9954\n",
            "Epoch [10/100], Step [2000/10000], Loss: 0.7365\n",
            "Epoch [10/100], Step [3000/10000], Loss: 1.7935\n",
            "Epoch [10/100], Step [4000/10000], Loss: 0.5630\n",
            "Epoch [10/100], Step [5000/10000], Loss: 0.9006\n",
            "Epoch [10/100], Step [6000/10000], Loss: 0.8924\n",
            "Epoch [10/100], Step [7000/10000], Loss: 2.2081\n",
            "Epoch [10/100], Step [8000/10000], Loss: 1.6151\n",
            "Epoch [10/100], Step [9000/10000], Loss: 2.3491\n",
            "Epoch [10/100], Step [10000/10000], Loss: 0.3297\n",
            "Epoch [11/100], Step [1000/10000], Loss: 0.5632\n",
            "Epoch [11/100], Step [2000/10000], Loss: 1.0374\n",
            "Epoch [11/100], Step [3000/10000], Loss: 0.7119\n",
            "Epoch [11/100], Step [4000/10000], Loss: 0.6340\n",
            "Epoch [11/100], Step [5000/10000], Loss: 1.1147\n",
            "Epoch [11/100], Step [6000/10000], Loss: 1.2975\n",
            "Epoch [11/100], Step [7000/10000], Loss: 1.1003\n",
            "Epoch [11/100], Step [8000/10000], Loss: 0.3890\n",
            "Epoch [11/100], Step [9000/10000], Loss: 0.5489\n",
            "Epoch [11/100], Step [10000/10000], Loss: 0.2645\n",
            "Epoch [12/100], Step [1000/10000], Loss: 1.7841\n",
            "Epoch [12/100], Step [2000/10000], Loss: 2.2854\n",
            "Epoch [12/100], Step [3000/10000], Loss: 0.7983\n",
            "Epoch [12/100], Step [4000/10000], Loss: 0.4494\n",
            "Epoch [12/100], Step [5000/10000], Loss: 0.8692\n",
            "Epoch [12/100], Step [6000/10000], Loss: 0.9950\n",
            "Epoch [12/100], Step [7000/10000], Loss: 1.2424\n",
            "Epoch [12/100], Step [8000/10000], Loss: 0.6797\n",
            "Epoch [12/100], Step [9000/10000], Loss: 0.8585\n",
            "Epoch [12/100], Step [10000/10000], Loss: 0.6881\n",
            "Epoch [13/100], Step [1000/10000], Loss: 1.3811\n",
            "Epoch [13/100], Step [2000/10000], Loss: 0.6270\n",
            "Epoch [13/100], Step [3000/10000], Loss: 0.7362\n",
            "Epoch [13/100], Step [4000/10000], Loss: 1.4172\n",
            "Epoch [13/100], Step [5000/10000], Loss: 0.6440\n",
            "Epoch [13/100], Step [6000/10000], Loss: 1.1342\n",
            "Epoch [13/100], Step [7000/10000], Loss: 0.0713\n",
            "Epoch [13/100], Step [8000/10000], Loss: 1.0165\n",
            "Epoch [13/100], Step [9000/10000], Loss: 1.7631\n",
            "Epoch [13/100], Step [10000/10000], Loss: 1.0793\n",
            "Epoch [14/100], Step [1000/10000], Loss: 1.0381\n",
            "Epoch [14/100], Step [2000/10000], Loss: 0.5708\n",
            "Epoch [14/100], Step [3000/10000], Loss: 0.6548\n",
            "Epoch [14/100], Step [4000/10000], Loss: 1.2877\n",
            "Epoch [14/100], Step [5000/10000], Loss: 2.4185\n",
            "Epoch [14/100], Step [6000/10000], Loss: 0.3017\n",
            "Epoch [14/100], Step [7000/10000], Loss: 0.9305\n",
            "Epoch [14/100], Step [8000/10000], Loss: 0.5961\n",
            "Epoch [14/100], Step [9000/10000], Loss: 0.6733\n",
            "Epoch [14/100], Step [10000/10000], Loss: 0.0574\n",
            "Epoch [15/100], Step [1000/10000], Loss: 0.3025\n",
            "Epoch [15/100], Step [2000/10000], Loss: 1.2518\n",
            "Epoch [15/100], Step [3000/10000], Loss: 0.6750\n",
            "Epoch [15/100], Step [4000/10000], Loss: 0.6076\n",
            "Epoch [15/100], Step [5000/10000], Loss: 0.4002\n",
            "Epoch [15/100], Step [6000/10000], Loss: 0.9162\n",
            "Epoch [15/100], Step [7000/10000], Loss: 1.2542\n",
            "Epoch [15/100], Step [8000/10000], Loss: 0.6900\n",
            "Epoch [15/100], Step [9000/10000], Loss: 3.1443\n",
            "Epoch [15/100], Step [10000/10000], Loss: 0.8979\n",
            "Epoch [16/100], Step [1000/10000], Loss: 0.9505\n",
            "Epoch [16/100], Step [2000/10000], Loss: 0.2827\n",
            "Epoch [16/100], Step [3000/10000], Loss: 0.7244\n",
            "Epoch [16/100], Step [4000/10000], Loss: 0.8741\n",
            "Epoch [16/100], Step [5000/10000], Loss: 0.7357\n",
            "Epoch [16/100], Step [6000/10000], Loss: 0.5199\n",
            "Epoch [16/100], Step [7000/10000], Loss: 1.1989\n",
            "Epoch [16/100], Step [8000/10000], Loss: 0.1173\n",
            "Epoch [16/100], Step [9000/10000], Loss: 0.5699\n",
            "Epoch [16/100], Step [10000/10000], Loss: 0.6395\n",
            "Epoch [17/100], Step [1000/10000], Loss: 0.9459\n",
            "Epoch [17/100], Step [2000/10000], Loss: 0.8179\n",
            "Epoch [17/100], Step [3000/10000], Loss: 0.5881\n",
            "Epoch [17/100], Step [4000/10000], Loss: 0.7411\n",
            "Epoch [17/100], Step [5000/10000], Loss: 1.1088\n",
            "Epoch [17/100], Step [6000/10000], Loss: 0.7841\n",
            "Epoch [17/100], Step [7000/10000], Loss: 1.2600\n",
            "Epoch [17/100], Step [8000/10000], Loss: 2.7138\n",
            "Epoch [17/100], Step [9000/10000], Loss: 0.7547\n",
            "Epoch [17/100], Step [10000/10000], Loss: 0.3389\n",
            "Epoch [18/100], Step [1000/10000], Loss: 0.6585\n",
            "Epoch [18/100], Step [2000/10000], Loss: 1.7443\n",
            "Epoch [18/100], Step [3000/10000], Loss: 0.4224\n",
            "Epoch [18/100], Step [4000/10000], Loss: 0.3371\n",
            "Epoch [18/100], Step [5000/10000], Loss: 0.6760\n",
            "Epoch [18/100], Step [6000/10000], Loss: 1.3027\n",
            "Epoch [18/100], Step [7000/10000], Loss: 0.2539\n",
            "Epoch [18/100], Step [8000/10000], Loss: 0.0907\n",
            "Epoch [18/100], Step [9000/10000], Loss: 0.7633\n",
            "Epoch [18/100], Step [10000/10000], Loss: 1.4994\n",
            "Epoch [19/100], Step [1000/10000], Loss: 0.8490\n",
            "Epoch [19/100], Step [2000/10000], Loss: 0.5034\n",
            "Epoch [19/100], Step [3000/10000], Loss: 0.8397\n",
            "Epoch [19/100], Step [4000/10000], Loss: 0.8226\n",
            "Epoch [19/100], Step [5000/10000], Loss: 0.6559\n",
            "Epoch [19/100], Step [6000/10000], Loss: 0.4435\n",
            "Epoch [19/100], Step [7000/10000], Loss: 0.5595\n",
            "Epoch [19/100], Step [8000/10000], Loss: 0.7372\n",
            "Epoch [19/100], Step [9000/10000], Loss: 0.8258\n",
            "Epoch [19/100], Step [10000/10000], Loss: 0.4693\n",
            "Epoch [20/100], Step [1000/10000], Loss: 0.5346\n",
            "Epoch [20/100], Step [2000/10000], Loss: 1.1816\n",
            "Epoch [20/100], Step [3000/10000], Loss: 0.4455\n",
            "Epoch [20/100], Step [4000/10000], Loss: 1.5625\n",
            "Epoch [20/100], Step [5000/10000], Loss: 0.5917\n",
            "Epoch [20/100], Step [6000/10000], Loss: 1.3048\n",
            "Epoch [20/100], Step [7000/10000], Loss: 0.4881\n",
            "Epoch [20/100], Step [8000/10000], Loss: 0.1067\n",
            "Epoch [20/100], Step [9000/10000], Loss: 1.6660\n",
            "Epoch [20/100], Step [10000/10000], Loss: 1.1450\n",
            "Epoch [21/100], Step [1000/10000], Loss: 1.7884\n",
            "Epoch [21/100], Step [2000/10000], Loss: 0.7134\n",
            "Epoch [21/100], Step [3000/10000], Loss: 0.2053\n",
            "Epoch [21/100], Step [4000/10000], Loss: 0.8566\n",
            "Epoch [21/100], Step [5000/10000], Loss: 1.6412\n",
            "Epoch [21/100], Step [6000/10000], Loss: 0.8932\n",
            "Epoch [21/100], Step [7000/10000], Loss: 0.8033\n",
            "Epoch [21/100], Step [8000/10000], Loss: 0.2889\n",
            "Epoch [21/100], Step [9000/10000], Loss: 1.3680\n",
            "Epoch [21/100], Step [10000/10000], Loss: 2.0300\n",
            "Epoch [22/100], Step [1000/10000], Loss: 0.4169\n",
            "Epoch [22/100], Step [2000/10000], Loss: 1.2118\n",
            "Epoch [22/100], Step [3000/10000], Loss: 0.9617\n",
            "Epoch [22/100], Step [4000/10000], Loss: 1.4415\n",
            "Epoch [22/100], Step [5000/10000], Loss: 1.0241\n",
            "Epoch [22/100], Step [6000/10000], Loss: 0.1587\n",
            "Epoch [22/100], Step [7000/10000], Loss: 1.0120\n",
            "Epoch [22/100], Step [8000/10000], Loss: 1.0037\n",
            "Epoch [22/100], Step [9000/10000], Loss: 0.4968\n",
            "Epoch [22/100], Step [10000/10000], Loss: 0.6180\n",
            "Epoch [23/100], Step [1000/10000], Loss: 0.3366\n",
            "Epoch [23/100], Step [2000/10000], Loss: 0.5569\n",
            "Epoch [23/100], Step [3000/10000], Loss: 0.9356\n",
            "Epoch [23/100], Step [4000/10000], Loss: 0.0873\n",
            "Epoch [23/100], Step [5000/10000], Loss: 0.2361\n",
            "Epoch [23/100], Step [6000/10000], Loss: 0.8622\n",
            "Epoch [23/100], Step [7000/10000], Loss: 0.7142\n",
            "Epoch [23/100], Step [8000/10000], Loss: 0.8470\n",
            "Epoch [23/100], Step [9000/10000], Loss: 0.7788\n",
            "Epoch [23/100], Step [10000/10000], Loss: 0.9399\n",
            "Epoch [24/100], Step [1000/10000], Loss: 0.5793\n",
            "Epoch [24/100], Step [2000/10000], Loss: 1.4045\n",
            "Epoch [24/100], Step [3000/10000], Loss: 0.6704\n",
            "Epoch [24/100], Step [4000/10000], Loss: 0.1898\n",
            "Epoch [24/100], Step [5000/10000], Loss: 0.8209\n",
            "Epoch [24/100], Step [6000/10000], Loss: 0.1794\n",
            "Epoch [24/100], Step [7000/10000], Loss: 0.6736\n",
            "Epoch [24/100], Step [8000/10000], Loss: 0.3501\n",
            "Epoch [24/100], Step [9000/10000], Loss: 1.2599\n",
            "Epoch [24/100], Step [10000/10000], Loss: 0.3849\n",
            "Epoch [25/100], Step [1000/10000], Loss: 0.6314\n",
            "Epoch [25/100], Step [2000/10000], Loss: 0.6913\n",
            "Epoch [25/100], Step [3000/10000], Loss: 0.9292\n",
            "Epoch [25/100], Step [4000/10000], Loss: 0.9577\n",
            "Epoch [25/100], Step [5000/10000], Loss: 0.9334\n",
            "Epoch [25/100], Step [6000/10000], Loss: 0.0398\n",
            "Epoch [25/100], Step [7000/10000], Loss: 0.6954\n",
            "Epoch [25/100], Step [8000/10000], Loss: 1.3371\n",
            "Epoch [25/100], Step [9000/10000], Loss: 0.4194\n",
            "Epoch [25/100], Step [10000/10000], Loss: 0.4686\n",
            "Epoch [26/100], Step [1000/10000], Loss: 1.3296\n",
            "Epoch [26/100], Step [2000/10000], Loss: 0.5960\n",
            "Epoch [26/100], Step [3000/10000], Loss: 0.1675\n",
            "Epoch [26/100], Step [4000/10000], Loss: 1.0371\n",
            "Epoch [26/100], Step [5000/10000], Loss: 0.0424\n",
            "Epoch [26/100], Step [6000/10000], Loss: 0.8625\n",
            "Epoch [26/100], Step [7000/10000], Loss: 0.3587\n",
            "Epoch [26/100], Step [8000/10000], Loss: 0.5490\n",
            "Epoch [26/100], Step [9000/10000], Loss: 1.0914\n",
            "Epoch [26/100], Step [10000/10000], Loss: 3.1424\n",
            "Epoch [27/100], Step [1000/10000], Loss: 0.7556\n",
            "Epoch [27/100], Step [2000/10000], Loss: 0.4305\n",
            "Epoch [27/100], Step [3000/10000], Loss: 0.7455\n",
            "Epoch [27/100], Step [4000/10000], Loss: 0.2407\n",
            "Epoch [27/100], Step [5000/10000], Loss: 0.1281\n",
            "Epoch [27/100], Step [6000/10000], Loss: 0.4190\n",
            "Epoch [27/100], Step [7000/10000], Loss: 0.4834\n",
            "Epoch [27/100], Step [8000/10000], Loss: 0.1786\n",
            "Epoch [27/100], Step [9000/10000], Loss: 1.7531\n",
            "Epoch [27/100], Step [10000/10000], Loss: 0.5310\n",
            "Epoch [28/100], Step [1000/10000], Loss: 0.4834\n",
            "Epoch [28/100], Step [2000/10000], Loss: 1.2854\n",
            "Epoch [28/100], Step [3000/10000], Loss: 0.5196\n",
            "Epoch [28/100], Step [4000/10000], Loss: 0.2317\n",
            "Epoch [28/100], Step [5000/10000], Loss: 0.9556\n",
            "Epoch [28/100], Step [6000/10000], Loss: 0.8127\n",
            "Epoch [28/100], Step [7000/10000], Loss: 0.4906\n",
            "Epoch [28/100], Step [8000/10000], Loss: 1.5549\n",
            "Epoch [28/100], Step [9000/10000], Loss: 0.3446\n",
            "Epoch [28/100], Step [10000/10000], Loss: 1.6992\n",
            "Epoch [29/100], Step [1000/10000], Loss: 0.7364\n",
            "Epoch [29/100], Step [2000/10000], Loss: 0.3162\n",
            "Epoch [29/100], Step [4000/10000], Loss: 0.6028\n",
            "Epoch [29/100], Step [5000/10000], Loss: 0.8594\n",
            "Epoch [29/100], Step [6000/10000], Loss: 1.8258\n",
            "Epoch [29/100], Step [7000/10000], Loss: 0.9191\n",
            "Epoch [29/100], Step [8000/10000], Loss: 1.6893\n",
            "Epoch [29/100], Step [9000/10000], Loss: 1.5964\n",
            "Epoch [29/100], Step [10000/10000], Loss: 0.1384\n",
            "Epoch [30/100], Step [1000/10000], Loss: 0.6995\n",
            "Epoch [30/100], Step [2000/10000], Loss: 0.3923\n",
            "Epoch [30/100], Step [3000/10000], Loss: 1.0992\n",
            "Epoch [30/100], Step [4000/10000], Loss: 1.5778\n",
            "Epoch [30/100], Step [5000/10000], Loss: 0.2219\n",
            "Epoch [30/100], Step [6000/10000], Loss: 0.6205\n",
            "Epoch [30/100], Step [7000/10000], Loss: 0.5049\n",
            "Epoch [30/100], Step [8000/10000], Loss: 0.0951\n",
            "Epoch [30/100], Step [9000/10000], Loss: 0.5248\n",
            "Epoch [30/100], Step [10000/10000], Loss: 0.5438\n",
            "Epoch [31/100], Step [1000/10000], Loss: 0.3752\n",
            "Epoch [31/100], Step [2000/10000], Loss: 0.5879\n",
            "Epoch [31/100], Step [3000/10000], Loss: 1.1166\n",
            "Epoch [31/100], Step [4000/10000], Loss: 1.2621\n",
            "Epoch [31/100], Step [5000/10000], Loss: 0.6341\n",
            "Epoch [31/100], Step [6000/10000], Loss: 0.6046\n",
            "Epoch [31/100], Step [7000/10000], Loss: 0.2205\n",
            "Epoch [31/100], Step [8000/10000], Loss: 1.1612\n",
            "Epoch [31/100], Step [9000/10000], Loss: 0.4289\n",
            "Epoch [31/100], Step [10000/10000], Loss: 0.4500\n",
            "Epoch [32/100], Step [1000/10000], Loss: 0.4541\n",
            "Epoch [32/100], Step [2000/10000], Loss: 0.2046\n",
            "Epoch [32/100], Step [3000/10000], Loss: 2.0493\n",
            "Epoch [32/100], Step [4000/10000], Loss: 0.6096\n",
            "Epoch [32/100], Step [5000/10000], Loss: 0.3878\n",
            "Epoch [32/100], Step [6000/10000], Loss: 0.3362\n",
            "Epoch [32/100], Step [7000/10000], Loss: 0.2769\n",
            "Epoch [32/100], Step [8000/10000], Loss: 0.5565\n",
            "Epoch [32/100], Step [9000/10000], Loss: 2.0227\n",
            "Epoch [32/100], Step [10000/10000], Loss: 0.7255\n",
            "Epoch [33/100], Step [1000/10000], Loss: 1.6746\n",
            "Epoch [33/100], Step [2000/10000], Loss: 0.4009\n",
            "Epoch [33/100], Step [3000/10000], Loss: 0.2636\n",
            "Epoch [33/100], Step [4000/10000], Loss: 0.9148\n",
            "Epoch [33/100], Step [5000/10000], Loss: 0.4880\n",
            "Epoch [33/100], Step [6000/10000], Loss: 1.3844\n",
            "Epoch [33/100], Step [7000/10000], Loss: 1.2039\n",
            "Epoch [33/100], Step [8000/10000], Loss: 1.3305\n",
            "Epoch [33/100], Step [9000/10000], Loss: 0.4512\n",
            "Epoch [33/100], Step [10000/10000], Loss: 1.2373\n",
            "Epoch [34/100], Step [1000/10000], Loss: 0.7522\n",
            "Epoch [34/100], Step [2000/10000], Loss: 1.0702\n",
            "Epoch [34/100], Step [3000/10000], Loss: 0.6949\n",
            "Epoch [34/100], Step [4000/10000], Loss: 0.3595\n",
            "Epoch [34/100], Step [5000/10000], Loss: 0.7793\n",
            "Epoch [34/100], Step [6000/10000], Loss: 0.1335\n",
            "Epoch [34/100], Step [7000/10000], Loss: 1.9093\n",
            "Epoch [34/100], Step [8000/10000], Loss: 1.1821\n",
            "Epoch [34/100], Step [9000/10000], Loss: 1.0137\n",
            "Epoch [34/100], Step [10000/10000], Loss: 1.0425\n",
            "Epoch [35/100], Step [1000/10000], Loss: 1.1926\n",
            "Epoch [35/100], Step [2000/10000], Loss: 0.0761\n",
            "Epoch [35/100], Step [3000/10000], Loss: 0.0643\n",
            "Epoch [35/100], Step [4000/10000], Loss: 0.3754\n",
            "Epoch [35/100], Step [5000/10000], Loss: 0.8513\n",
            "Epoch [35/100], Step [6000/10000], Loss: 0.7585\n",
            "Epoch [35/100], Step [7000/10000], Loss: 0.4909\n",
            "Epoch [35/100], Step [8000/10000], Loss: 0.2786\n",
            "Epoch [35/100], Step [9000/10000], Loss: 0.5060\n",
            "Epoch [35/100], Step [10000/10000], Loss: 0.7315\n",
            "Epoch [36/100], Step [1000/10000], Loss: 0.5179\n",
            "Epoch [36/100], Step [2000/10000], Loss: 0.6009\n",
            "Epoch [36/100], Step [3000/10000], Loss: 0.3633\n",
            "Epoch [36/100], Step [4000/10000], Loss: 0.8282\n",
            "Epoch [36/100], Step [5000/10000], Loss: 0.2558\n",
            "Epoch [36/100], Step [6000/10000], Loss: 0.3902\n",
            "Epoch [36/100], Step [7000/10000], Loss: 1.0556\n",
            "Epoch [36/100], Step [8000/10000], Loss: 0.7304\n",
            "Epoch [36/100], Step [9000/10000], Loss: 0.5025\n",
            "Epoch [36/100], Step [10000/10000], Loss: 0.9365\n",
            "Epoch [37/100], Step [1000/10000], Loss: 1.2094\n",
            "Epoch [37/100], Step [2000/10000], Loss: 0.3887\n",
            "Epoch [37/100], Step [3000/10000], Loss: 0.5746\n",
            "Epoch [37/100], Step [4000/10000], Loss: 1.4263\n",
            "Epoch [37/100], Step [5000/10000], Loss: 0.3256\n",
            "Epoch [37/100], Step [6000/10000], Loss: 0.0725\n",
            "Epoch [37/100], Step [7000/10000], Loss: 0.4228\n",
            "Epoch [37/100], Step [8000/10000], Loss: 0.9230\n",
            "Epoch [37/100], Step [9000/10000], Loss: 0.2075\n",
            "Epoch [37/100], Step [10000/10000], Loss: 0.0836\n",
            "Epoch [38/100], Step [1000/10000], Loss: 0.0893\n",
            "Epoch [38/100], Step [2000/10000], Loss: 0.2516\n",
            "Epoch [38/100], Step [3000/10000], Loss: 0.7119\n",
            "Epoch [38/100], Step [4000/10000], Loss: 3.5565\n",
            "Epoch [38/100], Step [5000/10000], Loss: 0.7047\n",
            "Epoch [38/100], Step [6000/10000], Loss: 0.6009\n",
            "Epoch [38/100], Step [7000/10000], Loss: 0.2030\n",
            "Epoch [38/100], Step [8000/10000], Loss: 0.3533\n",
            "Epoch [38/100], Step [9000/10000], Loss: 0.6194\n",
            "Epoch [38/100], Step [10000/10000], Loss: 0.1307\n",
            "Epoch [39/100], Step [1000/10000], Loss: 0.8463\n",
            "Epoch [39/100], Step [2000/10000], Loss: 1.8388\n",
            "Epoch [39/100], Step [3000/10000], Loss: 0.3860\n",
            "Epoch [39/100], Step [4000/10000], Loss: 1.2985\n",
            "Epoch [39/100], Step [5000/10000], Loss: 1.1741\n",
            "Epoch [39/100], Step [6000/10000], Loss: 1.2813\n",
            "Epoch [39/100], Step [7000/10000], Loss: 1.1506\n",
            "Epoch [39/100], Step [8000/10000], Loss: 0.4765\n",
            "Epoch [39/100], Step [9000/10000], Loss: 0.8478\n",
            "Epoch [39/100], Step [10000/10000], Loss: 0.2166\n",
            "Epoch [40/100], Step [1000/10000], Loss: 1.0635\n",
            "Epoch [40/100], Step [2000/10000], Loss: 0.3359\n",
            "Epoch [40/100], Step [3000/10000], Loss: 0.7605\n",
            "Epoch [40/100], Step [4000/10000], Loss: 1.2035\n",
            "Epoch [40/100], Step [5000/10000], Loss: 0.0403\n",
            "Epoch [40/100], Step [6000/10000], Loss: 2.0590\n",
            "Epoch [40/100], Step [7000/10000], Loss: 1.6720\n",
            "Epoch [40/100], Step [8000/10000], Loss: 1.0802\n",
            "Epoch [40/100], Step [9000/10000], Loss: 0.1240\n",
            "Epoch [40/100], Step [10000/10000], Loss: 1.5820\n",
            "Epoch [41/100], Step [1000/10000], Loss: 1.4644\n",
            "Epoch [41/100], Step [2000/10000], Loss: 0.8567\n",
            "Epoch [41/100], Step [3000/10000], Loss: 0.7746\n",
            "Epoch [41/100], Step [4000/10000], Loss: 1.0919\n",
            "Epoch [41/100], Step [5000/10000], Loss: 0.6963\n",
            "Epoch [41/100], Step [6000/10000], Loss: 0.8658\n",
            "Epoch [41/100], Step [7000/10000], Loss: 1.3755\n",
            "Epoch [41/100], Step [8000/10000], Loss: 0.5831\n",
            "Epoch [41/100], Step [9000/10000], Loss: 1.4437\n",
            "Epoch [41/100], Step [10000/10000], Loss: 0.1776\n",
            "Epoch [42/100], Step [1000/10000], Loss: 2.2655\n",
            "Epoch [42/100], Step [2000/10000], Loss: 0.6446\n",
            "Epoch [42/100], Step [3000/10000], Loss: 1.0220\n",
            "Epoch [42/100], Step [4000/10000], Loss: 0.3849\n",
            "Epoch [42/100], Step [5000/10000], Loss: 0.8052\n",
            "Epoch [42/100], Step [6000/10000], Loss: 0.6445\n",
            "Epoch [42/100], Step [7000/10000], Loss: 1.4375\n",
            "Epoch [42/100], Step [8000/10000], Loss: 0.5001\n",
            "Epoch [42/100], Step [9000/10000], Loss: 0.9989\n",
            "Epoch [42/100], Step [10000/10000], Loss: 0.3896\n",
            "Epoch [43/100], Step [1000/10000], Loss: 0.2122\n",
            "Epoch [43/100], Step [2000/10000], Loss: 1.8665\n",
            "Epoch [43/100], Step [3000/10000], Loss: 1.0093\n",
            "Epoch [43/100], Step [4000/10000], Loss: 0.8938\n",
            "Epoch [43/100], Step [5000/10000], Loss: 0.9229\n",
            "Epoch [43/100], Step [6000/10000], Loss: 0.5524\n",
            "Epoch [43/100], Step [7000/10000], Loss: 0.9327\n",
            "Epoch [43/100], Step [8000/10000], Loss: 0.2810\n",
            "Epoch [43/100], Step [9000/10000], Loss: 1.5580\n",
            "Epoch [43/100], Step [10000/10000], Loss: 1.0468\n",
            "Epoch [44/100], Step [1000/10000], Loss: 0.0992\n",
            "Epoch [44/100], Step [2000/10000], Loss: 1.1679\n",
            "Epoch [44/100], Step [3000/10000], Loss: 1.1141\n",
            "Epoch [44/100], Step [4000/10000], Loss: 0.4619\n",
            "Epoch [44/100], Step [5000/10000], Loss: 1.2068\n",
            "Epoch [44/100], Step [6000/10000], Loss: 1.6829\n",
            "Epoch [44/100], Step [7000/10000], Loss: 1.2522\n",
            "Epoch [44/100], Step [8000/10000], Loss: 2.7728\n",
            "Epoch [44/100], Step [9000/10000], Loss: 0.5822\n",
            "Epoch [44/100], Step [10000/10000], Loss: 0.7364\n",
            "Epoch [45/100], Step [1000/10000], Loss: 0.2769\n",
            "Epoch [45/100], Step [2000/10000], Loss: 0.7789\n",
            "Epoch [45/100], Step [3000/10000], Loss: 0.6963\n",
            "Epoch [45/100], Step [4000/10000], Loss: 0.6306\n",
            "Epoch [45/100], Step [5000/10000], Loss: 0.0530\n",
            "Epoch [45/100], Step [6000/10000], Loss: 0.5115\n",
            "Epoch [45/100], Step [7000/10000], Loss: 0.9038\n",
            "Epoch [45/100], Step [8000/10000], Loss: 1.2544\n",
            "Epoch [45/100], Step [9000/10000], Loss: 1.1953\n",
            "Epoch [45/100], Step [10000/10000], Loss: 0.9461\n",
            "Epoch [46/100], Step [1000/10000], Loss: 1.6928\n",
            "Epoch [46/100], Step [2000/10000], Loss: 1.3961\n",
            "Epoch [46/100], Step [3000/10000], Loss: 1.5087\n",
            "Epoch [46/100], Step [4000/10000], Loss: 0.4987\n",
            "Epoch [46/100], Step [5000/10000], Loss: 0.4793\n",
            "Epoch [46/100], Step [6000/10000], Loss: 0.7795\n",
            "Epoch [46/100], Step [7000/10000], Loss: 1.7554\n",
            "Epoch [46/100], Step [8000/10000], Loss: 0.9484\n",
            "Epoch [46/100], Step [9000/10000], Loss: 1.2270\n",
            "Epoch [46/100], Step [10000/10000], Loss: 0.1133\n",
            "Epoch [47/100], Step [1000/10000], Loss: 0.5855\n",
            "Epoch [47/100], Step [2000/10000], Loss: 1.0744\n",
            "Epoch [47/100], Step [3000/10000], Loss: 0.7338\n",
            "Epoch [47/100], Step [4000/10000], Loss: 1.3518\n",
            "Epoch [47/100], Step [5000/10000], Loss: 0.1954\n",
            "Epoch [47/100], Step [6000/10000], Loss: 0.4515\n",
            "Epoch [47/100], Step [7000/10000], Loss: 0.1631\n",
            "Epoch [47/100], Step [8000/10000], Loss: 0.2684\n",
            "Epoch [47/100], Step [9000/10000], Loss: 1.2372\n",
            "Epoch [47/100], Step [10000/10000], Loss: 1.4253\n",
            "Epoch [48/100], Step [1000/10000], Loss: 0.2123\n",
            "Epoch [48/100], Step [2000/10000], Loss: 0.5117\n",
            "Epoch [48/100], Step [3000/10000], Loss: 0.6792\n",
            "Epoch [48/100], Step [4000/10000], Loss: 1.1876\n",
            "Epoch [48/100], Step [5000/10000], Loss: 0.3980\n",
            "Epoch [48/100], Step [6000/10000], Loss: 1.0630\n",
            "Epoch [48/100], Step [7000/10000], Loss: 0.9742\n",
            "Epoch [48/100], Step [8000/10000], Loss: 0.3719\n",
            "Epoch [48/100], Step [9000/10000], Loss: 0.6717\n",
            "Epoch [48/100], Step [10000/10000], Loss: 0.3276\n",
            "Epoch [49/100], Step [1000/10000], Loss: 1.3856\n",
            "Epoch [49/100], Step [2000/10000], Loss: 1.0111\n",
            "Epoch [49/100], Step [3000/10000], Loss: 0.5897\n",
            "Epoch [49/100], Step [4000/10000], Loss: 0.0894\n",
            "Epoch [49/100], Step [5000/10000], Loss: 0.1081\n",
            "Epoch [49/100], Step [6000/10000], Loss: 0.2793\n",
            "Epoch [49/100], Step [7000/10000], Loss: 1.4970\n",
            "Epoch [49/100], Step [8000/10000], Loss: 0.0633\n",
            "Epoch [49/100], Step [9000/10000], Loss: 1.3985\n",
            "Epoch [49/100], Step [10000/10000], Loss: 0.5529\n",
            "Epoch [50/100], Step [1000/10000], Loss: 0.3998\n",
            "Epoch [50/100], Step [2000/10000], Loss: 0.5174\n",
            "Epoch [50/100], Step [3000/10000], Loss: 1.2172\n",
            "Epoch [50/100], Step [4000/10000], Loss: 0.3348\n",
            "Epoch [50/100], Step [5000/10000], Loss: 0.1340\n",
            "Epoch [50/100], Step [6000/10000], Loss: 2.5749\n",
            "Epoch [50/100], Step [7000/10000], Loss: 0.6015\n",
            "Epoch [50/100], Step [8000/10000], Loss: 1.3299\n",
            "Epoch [50/100], Step [9000/10000], Loss: 0.0919\n",
            "Epoch [50/100], Step [10000/10000], Loss: 0.4450\n",
            "Epoch [51/100], Step [1000/10000], Loss: 0.4457\n",
            "Epoch [51/100], Step [2000/10000], Loss: 0.2793\n",
            "Epoch [51/100], Step [3000/10000], Loss: 0.3042\n",
            "Epoch [51/100], Step [4000/10000], Loss: 0.2163\n",
            "Epoch [51/100], Step [5000/10000], Loss: 0.1952\n",
            "Epoch [51/100], Step [6000/10000], Loss: 0.0455\n",
            "Epoch [51/100], Step [7000/10000], Loss: 0.4331\n",
            "Epoch [51/100], Step [8000/10000], Loss: 0.7407\n",
            "Epoch [51/100], Step [9000/10000], Loss: 0.6349\n",
            "Epoch [51/100], Step [10000/10000], Loss: 0.4342\n",
            "Epoch [52/100], Step [1000/10000], Loss: 1.5654\n",
            "Epoch [52/100], Step [2000/10000], Loss: 0.5893\n",
            "Epoch [52/100], Step [3000/10000], Loss: 0.4768\n",
            "Epoch [52/100], Step [4000/10000], Loss: 0.9542\n",
            "Epoch [52/100], Step [5000/10000], Loss: 1.0949\n",
            "Epoch [52/100], Step [6000/10000], Loss: 0.5331\n",
            "Epoch [52/100], Step [7000/10000], Loss: 1.3644\n",
            "Epoch [52/100], Step [8000/10000], Loss: 1.3587\n",
            "Epoch [52/100], Step [9000/10000], Loss: 0.8331\n",
            "Epoch [52/100], Step [10000/10000], Loss: 0.1699\n",
            "Epoch [53/100], Step [1000/10000], Loss: 1.1388\n",
            "Epoch [53/100], Step [2000/10000], Loss: 0.9426\n",
            "Epoch [53/100], Step [3000/10000], Loss: 0.7673\n",
            "Epoch [53/100], Step [4000/10000], Loss: 0.7249\n",
            "Epoch [53/100], Step [5000/10000], Loss: 1.0362\n",
            "Epoch [53/100], Step [6000/10000], Loss: 0.0538\n",
            "Epoch [53/100], Step [7000/10000], Loss: 0.5127\n",
            "Epoch [53/100], Step [8000/10000], Loss: 0.3581\n",
            "Epoch [53/100], Step [9000/10000], Loss: 0.3607\n",
            "Epoch [53/100], Step [10000/10000], Loss: 0.9215\n",
            "Epoch [54/100], Step [1000/10000], Loss: 1.9214\n",
            "Epoch [54/100], Step [2000/10000], Loss: 0.4794\n",
            "Epoch [54/100], Step [3000/10000], Loss: 0.5395\n",
            "Epoch [54/100], Step [4000/10000], Loss: 1.3876\n",
            "Epoch [54/100], Step [5000/10000], Loss: 1.0538\n",
            "Epoch [54/100], Step [6000/10000], Loss: 0.2182\n",
            "Epoch [54/100], Step [7000/10000], Loss: 0.7317\n",
            "Epoch [54/100], Step [8000/10000], Loss: 0.7249\n",
            "Epoch [54/100], Step [9000/10000], Loss: 0.2749\n",
            "Epoch [54/100], Step [10000/10000], Loss: 0.8603\n",
            "Epoch [55/100], Step [1000/10000], Loss: 1.3170\n",
            "Epoch [55/100], Step [2000/10000], Loss: 0.4061\n",
            "Epoch [55/100], Step [3000/10000], Loss: 0.7134\n",
            "Epoch [55/100], Step [4000/10000], Loss: 0.3056\n",
            "Epoch [55/100], Step [5000/10000], Loss: 0.8910\n",
            "Epoch [55/100], Step [6000/10000], Loss: 0.7500\n",
            "Epoch [55/100], Step [7000/10000], Loss: 0.1962\n",
            "Epoch [55/100], Step [8000/10000], Loss: 0.0605\n",
            "Epoch [55/100], Step [9000/10000], Loss: 0.4039\n",
            "Epoch [55/100], Step [10000/10000], Loss: 0.1659\n",
            "Epoch [56/100], Step [1000/10000], Loss: 0.9750\n",
            "Epoch [56/100], Step [2000/10000], Loss: 0.7573\n",
            "Epoch [56/100], Step [3000/10000], Loss: 0.6171\n",
            "Epoch [56/100], Step [4000/10000], Loss: 0.7189\n",
            "Epoch [56/100], Step [5000/10000], Loss: 0.8090\n",
            "Epoch [56/100], Step [6000/10000], Loss: 0.7583\n",
            "Epoch [56/100], Step [7000/10000], Loss: 1.7066\n",
            "Epoch [56/100], Step [8000/10000], Loss: 1.7414\n",
            "Epoch [56/100], Step [9000/10000], Loss: 0.7083\n",
            "Epoch [56/100], Step [10000/10000], Loss: 0.1795\n",
            "Epoch [57/100], Step [1000/10000], Loss: 0.0566\n",
            "Epoch [57/100], Step [2000/10000], Loss: 0.0120\n",
            "Epoch [57/100], Step [3000/10000], Loss: 1.0542\n",
            "Epoch [57/100], Step [4000/10000], Loss: 0.5369\n",
            "Epoch [57/100], Step [5000/10000], Loss: 1.1543\n",
            "Epoch [57/100], Step [6000/10000], Loss: 0.1141\n",
            "Epoch [57/100], Step [7000/10000], Loss: 0.7381\n",
            "Epoch [57/100], Step [8000/10000], Loss: 1.0846\n",
            "Epoch [57/100], Step [9000/10000], Loss: 1.3199\n",
            "Epoch [57/100], Step [10000/10000], Loss: 1.2933\n",
            "Epoch [58/100], Step [1000/10000], Loss: 0.5427\n",
            "Epoch [58/100], Step [2000/10000], Loss: 0.3819\n",
            "Epoch [58/100], Step [3000/10000], Loss: 0.8868\n",
            "Epoch [58/100], Step [4000/10000], Loss: 0.2306\n",
            "Epoch [58/100], Step [5000/10000], Loss: 0.4319\n",
            "Epoch [58/100], Step [6000/10000], Loss: 0.1252\n",
            "Epoch [58/100], Step [7000/10000], Loss: 0.3991\n",
            "Epoch [58/100], Step [8000/10000], Loss: 0.7436\n",
            "Epoch [58/100], Step [9000/10000], Loss: 0.9262\n",
            "Epoch [58/100], Step [10000/10000], Loss: 0.2370\n",
            "Epoch [59/100], Step [1000/10000], Loss: 0.9183\n",
            "Epoch [59/100], Step [2000/10000], Loss: 0.5443\n",
            "Epoch [59/100], Step [3000/10000], Loss: 2.2104\n",
            "Epoch [59/100], Step [4000/10000], Loss: 0.9016\n",
            "Epoch [59/100], Step [5000/10000], Loss: 1.1302\n",
            "Epoch [59/100], Step [6000/10000], Loss: 0.4330\n",
            "Epoch [59/100], Step [7000/10000], Loss: 0.6356\n",
            "Epoch [59/100], Step [8000/10000], Loss: 0.2234\n",
            "Epoch [59/100], Step [9000/10000], Loss: 0.5687\n",
            "Epoch [59/100], Step [10000/10000], Loss: 0.5876\n",
            "Epoch [60/100], Step [1000/10000], Loss: 0.4339\n",
            "Epoch [60/100], Step [2000/10000], Loss: 0.6879\n",
            "Epoch [60/100], Step [3000/10000], Loss: 0.7854\n",
            "Epoch [60/100], Step [4000/10000], Loss: 0.7223\n",
            "Epoch [60/100], Step [5000/10000], Loss: 0.3592\n",
            "Epoch [60/100], Step [6000/10000], Loss: 0.3136\n",
            "Epoch [60/100], Step [7000/10000], Loss: 0.2887\n",
            "Epoch [60/100], Step [8000/10000], Loss: 0.1972\n",
            "Epoch [60/100], Step [9000/10000], Loss: 0.4997\n",
            "Epoch [60/100], Step [10000/10000], Loss: 1.4697\n",
            "Epoch [61/100], Step [1000/10000], Loss: 0.6908\n",
            "Epoch [61/100], Step [2000/10000], Loss: 0.1889\n",
            "Epoch [61/100], Step [3000/10000], Loss: 1.1889\n",
            "Epoch [61/100], Step [4000/10000], Loss: 0.5346\n",
            "Epoch [61/100], Step [5000/10000], Loss: 0.8035\n",
            "Epoch [61/100], Step [6000/10000], Loss: 0.3632\n",
            "Epoch [61/100], Step [7000/10000], Loss: 2.5615\n",
            "Epoch [61/100], Step [8000/10000], Loss: 0.9974\n",
            "Epoch [61/100], Step [9000/10000], Loss: 1.5723\n",
            "Epoch [61/100], Step [10000/10000], Loss: 0.0580\n",
            "Epoch [62/100], Step [1000/10000], Loss: 0.5417\n",
            "Epoch [62/100], Step [2000/10000], Loss: 0.0595\n",
            "Epoch [62/100], Step [3000/10000], Loss: 0.6657\n",
            "Epoch [62/100], Step [4000/10000], Loss: 0.2070\n",
            "Epoch [62/100], Step [5000/10000], Loss: 0.4819\n",
            "Epoch [62/100], Step [6000/10000], Loss: 0.4679\n",
            "Epoch [62/100], Step [7000/10000], Loss: 0.9300\n",
            "Epoch [62/100], Step [8000/10000], Loss: 0.7476\n",
            "Epoch [62/100], Step [9000/10000], Loss: 1.3658\n",
            "Epoch [62/100], Step [10000/10000], Loss: 0.4082\n",
            "Epoch [63/100], Step [1000/10000], Loss: 0.2825\n",
            "Epoch [63/100], Step [2000/10000], Loss: 0.4474\n",
            "Epoch [63/100], Step [3000/10000], Loss: 0.5221\n",
            "Epoch [63/100], Step [4000/10000], Loss: 0.7385\n",
            "Epoch [63/100], Step [5000/10000], Loss: 0.8521\n",
            "Epoch [63/100], Step [6000/10000], Loss: 0.3812\n",
            "Epoch [63/100], Step [7000/10000], Loss: 1.5778\n",
            "Epoch [63/100], Step [8000/10000], Loss: 0.1917\n",
            "Epoch [63/100], Step [9000/10000], Loss: 0.5548\n",
            "Epoch [63/100], Step [10000/10000], Loss: 0.1675\n",
            "Epoch [64/100], Step [1000/10000], Loss: 0.5866\n",
            "Epoch [64/100], Step [2000/10000], Loss: 1.0542\n",
            "Epoch [64/100], Step [3000/10000], Loss: 0.4541\n",
            "Epoch [64/100], Step [4000/10000], Loss: 0.7062\n",
            "Epoch [64/100], Step [5000/10000], Loss: 0.7236\n",
            "Epoch [64/100], Step [6000/10000], Loss: 0.3681\n",
            "Epoch [64/100], Step [7000/10000], Loss: 0.2176\n",
            "Epoch [64/100], Step [8000/10000], Loss: 0.5370\n",
            "Epoch [64/100], Step [9000/10000], Loss: 0.2978\n",
            "Epoch [64/100], Step [10000/10000], Loss: 0.5958\n",
            "Epoch [65/100], Step [1000/10000], Loss: 0.8551\n",
            "Epoch [65/100], Step [2000/10000], Loss: 0.8283\n",
            "Epoch [65/100], Step [3000/10000], Loss: 1.0156\n",
            "Epoch [65/100], Step [4000/10000], Loss: 0.7786\n",
            "Epoch [65/100], Step [5000/10000], Loss: 0.5308\n",
            "Epoch [65/100], Step [6000/10000], Loss: 0.8837\n",
            "Epoch [65/100], Step [7000/10000], Loss: 0.2402\n",
            "Epoch [65/100], Step [8000/10000], Loss: 0.6899\n",
            "Epoch [65/100], Step [9000/10000], Loss: 0.2782\n",
            "Epoch [65/100], Step [10000/10000], Loss: 0.2223\n",
            "Epoch [66/100], Step [1000/10000], Loss: 0.3396\n",
            "Epoch [66/100], Step [2000/10000], Loss: 0.3250\n",
            "Epoch [66/100], Step [3000/10000], Loss: 0.5756\n",
            "Epoch [66/100], Step [4000/10000], Loss: 0.1248\n",
            "Epoch [66/100], Step [5000/10000], Loss: 0.4165\n",
            "Epoch [66/100], Step [6000/10000], Loss: 0.5787\n",
            "Epoch [66/100], Step [7000/10000], Loss: 0.7123\n",
            "Epoch [66/100], Step [8000/10000], Loss: 1.3309\n",
            "Epoch [66/100], Step [9000/10000], Loss: 0.7557\n",
            "Epoch [66/100], Step [10000/10000], Loss: 0.1473\n",
            "Epoch [67/100], Step [1000/10000], Loss: 0.6386\n",
            "Epoch [67/100], Step [2000/10000], Loss: 0.3160\n",
            "Epoch [67/100], Step [3000/10000], Loss: 0.8586\n",
            "Epoch [67/100], Step [4000/10000], Loss: 0.9812\n",
            "Epoch [67/100], Step [5000/10000], Loss: 0.0967\n",
            "Epoch [67/100], Step [6000/10000], Loss: 0.4866\n",
            "Epoch [67/100], Step [7000/10000], Loss: 1.6276\n",
            "Epoch [67/100], Step [8000/10000], Loss: 1.0128\n",
            "Epoch [67/100], Step [9000/10000], Loss: 0.3647\n",
            "Epoch [67/100], Step [10000/10000], Loss: 0.8604\n",
            "Epoch [68/100], Step [1000/10000], Loss: 0.7728\n",
            "Epoch [68/100], Step [2000/10000], Loss: 0.0473\n",
            "Epoch [68/100], Step [3000/10000], Loss: 0.8541\n",
            "Epoch [68/100], Step [4000/10000], Loss: 0.2885\n",
            "Epoch [68/100], Step [5000/10000], Loss: 0.0992\n",
            "Epoch [68/100], Step [6000/10000], Loss: 0.3577\n",
            "Epoch [68/100], Step [7000/10000], Loss: 0.2609\n",
            "Epoch [68/100], Step [8000/10000], Loss: 1.9045\n",
            "Epoch [68/100], Step [9000/10000], Loss: 0.7869\n",
            "Epoch [68/100], Step [10000/10000], Loss: 0.3319\n",
            "Epoch [69/100], Step [1000/10000], Loss: 1.2183\n",
            "Epoch [69/100], Step [2000/10000], Loss: 0.5459\n",
            "Epoch [69/100], Step [3000/10000], Loss: 0.0888\n",
            "Epoch [69/100], Step [4000/10000], Loss: 0.8906\n",
            "Epoch [69/100], Step [5000/10000], Loss: 0.3136\n",
            "Epoch [69/100], Step [6000/10000], Loss: 0.9783\n",
            "Epoch [69/100], Step [7000/10000], Loss: 0.5755\n",
            "Epoch [69/100], Step [8000/10000], Loss: 1.0934\n",
            "Epoch [69/100], Step [9000/10000], Loss: 0.7247\n",
            "Epoch [69/100], Step [10000/10000], Loss: 1.3286\n",
            "Epoch [70/100], Step [1000/10000], Loss: 0.4302\n",
            "Epoch [70/100], Step [2000/10000], Loss: 0.6630\n",
            "Epoch [70/100], Step [3000/10000], Loss: 0.5474\n",
            "Epoch [70/100], Step [4000/10000], Loss: 0.1000\n",
            "Epoch [70/100], Step [5000/10000], Loss: 0.5401\n",
            "Epoch [70/100], Step [6000/10000], Loss: 0.5550\n",
            "Epoch [70/100], Step [7000/10000], Loss: 0.8588\n",
            "Epoch [70/100], Step [8000/10000], Loss: 0.9347\n",
            "Epoch [70/100], Step [9000/10000], Loss: 2.2788\n",
            "Epoch [70/100], Step [10000/10000], Loss: 0.7147\n",
            "Epoch [71/100], Step [1000/10000], Loss: 0.8142\n",
            "Epoch [71/100], Step [2000/10000], Loss: 1.3236\n",
            "Epoch [71/100], Step [3000/10000], Loss: 0.8696\n",
            "Epoch [71/100], Step [4000/10000], Loss: 1.5295\n",
            "Epoch [71/100], Step [5000/10000], Loss: 0.0934\n",
            "Epoch [71/100], Step [6000/10000], Loss: 0.7407\n",
            "Epoch [71/100], Step [7000/10000], Loss: 0.3372\n",
            "Epoch [71/100], Step [8000/10000], Loss: 0.4700\n",
            "Epoch [71/100], Step [9000/10000], Loss: 0.4209\n",
            "Epoch [71/100], Step [10000/10000], Loss: 0.7539\n",
            "Epoch [72/100], Step [1000/10000], Loss: 1.0754\n",
            "Epoch [72/100], Step [2000/10000], Loss: 0.5460\n",
            "Epoch [72/100], Step [3000/10000], Loss: 1.6189\n",
            "Epoch [72/100], Step [4000/10000], Loss: 0.6956\n",
            "Epoch [72/100], Step [5000/10000], Loss: 0.5990\n",
            "Epoch [72/100], Step [6000/10000], Loss: 1.2154\n",
            "Epoch [72/100], Step [7000/10000], Loss: 0.3623\n",
            "Epoch [72/100], Step [8000/10000], Loss: 0.2907\n",
            "Epoch [72/100], Step [9000/10000], Loss: 0.4042\n",
            "Epoch [72/100], Step [10000/10000], Loss: 0.4513\n",
            "Epoch [73/100], Step [1000/10000], Loss: 0.2611\n",
            "Epoch [73/100], Step [2000/10000], Loss: 0.9099\n",
            "Epoch [73/100], Step [3000/10000], Loss: 0.1224\n",
            "Epoch [73/100], Step [4000/10000], Loss: 0.7588\n",
            "Epoch [73/100], Step [5000/10000], Loss: 0.9364\n",
            "Epoch [73/100], Step [6000/10000], Loss: 1.3527\n",
            "Epoch [73/100], Step [7000/10000], Loss: 0.0016\n",
            "Epoch [73/100], Step [8000/10000], Loss: 1.0147\n",
            "Epoch [73/100], Step [9000/10000], Loss: 0.3736\n",
            "Epoch [73/100], Step [10000/10000], Loss: 0.1431\n",
            "Epoch [74/100], Step [1000/10000], Loss: 0.0021\n",
            "Epoch [74/100], Step [2000/10000], Loss: 0.3521\n",
            "Epoch [74/100], Step [3000/10000], Loss: 0.6882\n",
            "Epoch [74/100], Step [4000/10000], Loss: 0.0160\n",
            "Epoch [74/100], Step [5000/10000], Loss: 0.1098\n",
            "Epoch [74/100], Step [6000/10000], Loss: 0.6530\n",
            "Epoch [74/100], Step [7000/10000], Loss: 0.4179\n",
            "Epoch [74/100], Step [8000/10000], Loss: 0.4073\n",
            "Epoch [74/100], Step [9000/10000], Loss: 0.3211\n",
            "Epoch [74/100], Step [10000/10000], Loss: 0.6356\n",
            "Epoch [75/100], Step [1000/10000], Loss: 0.1902\n",
            "Epoch [75/100], Step [2000/10000], Loss: 0.8701\n",
            "Epoch [75/100], Step [3000/10000], Loss: 0.2505\n",
            "Epoch [75/100], Step [4000/10000], Loss: 0.9662\n",
            "Epoch [75/100], Step [5000/10000], Loss: 2.0619\n",
            "Epoch [75/100], Step [6000/10000], Loss: 1.0283\n",
            "Epoch [75/100], Step [7000/10000], Loss: 0.3296\n",
            "Epoch [75/100], Step [8000/10000], Loss: 0.4212\n",
            "Epoch [75/100], Step [9000/10000], Loss: 0.6831\n",
            "Epoch [75/100], Step [10000/10000], Loss: 0.3220\n",
            "Epoch [76/100], Step [1000/10000], Loss: 0.5601\n",
            "Epoch [76/100], Step [2000/10000], Loss: 0.4842\n",
            "Epoch [76/100], Step [3000/10000], Loss: 0.4486\n",
            "Epoch [76/100], Step [4000/10000], Loss: 1.3137\n",
            "Epoch [76/100], Step [5000/10000], Loss: 0.7046\n",
            "Epoch [76/100], Step [6000/10000], Loss: 1.8046\n",
            "Epoch [76/100], Step [7000/10000], Loss: 0.6964\n",
            "Epoch [76/100], Step [8000/10000], Loss: 0.0622\n",
            "Epoch [76/100], Step [9000/10000], Loss: 0.6826\n",
            "Epoch [76/100], Step [10000/10000], Loss: 0.1098\n",
            "Epoch [77/100], Step [1000/10000], Loss: 0.3719\n",
            "Epoch [77/100], Step [2000/10000], Loss: 0.7070\n",
            "Epoch [77/100], Step [3000/10000], Loss: 1.0429\n",
            "Epoch [77/100], Step [4000/10000], Loss: 0.2665\n",
            "Epoch [77/100], Step [5000/10000], Loss: 0.3797\n",
            "Epoch [77/100], Step [6000/10000], Loss: 0.7512\n",
            "Epoch [77/100], Step [7000/10000], Loss: 0.6047\n",
            "Epoch [77/100], Step [8000/10000], Loss: 1.1719\n",
            "Epoch [77/100], Step [9000/10000], Loss: 0.2579\n",
            "Epoch [77/100], Step [10000/10000], Loss: 0.3493\n",
            "Epoch [78/100], Step [1000/10000], Loss: 0.4777\n",
            "Epoch [78/100], Step [2000/10000], Loss: 0.1273\n",
            "Epoch [78/100], Step [3000/10000], Loss: 1.6632\n",
            "Epoch [78/100], Step [4000/10000], Loss: 0.2435\n",
            "Epoch [78/100], Step [5000/10000], Loss: 1.5791\n",
            "Epoch [78/100], Step [6000/10000], Loss: 0.6975\n",
            "Epoch [78/100], Step [7000/10000], Loss: 0.0701\n",
            "Epoch [78/100], Step [8000/10000], Loss: 0.5193\n",
            "Epoch [78/100], Step [9000/10000], Loss: 0.1497\n",
            "Epoch [78/100], Step [10000/10000], Loss: 0.5199\n",
            "Epoch [79/100], Step [1000/10000], Loss: 0.8313\n",
            "Epoch [79/100], Step [2000/10000], Loss: 0.0892\n",
            "Epoch [79/100], Step [3000/10000], Loss: 1.1822\n",
            "Epoch [79/100], Step [4000/10000], Loss: 0.4968\n",
            "Epoch [79/100], Step [5000/10000], Loss: 0.3247\n",
            "Epoch [79/100], Step [6000/10000], Loss: 0.6115\n",
            "Epoch [79/100], Step [7000/10000], Loss: 0.3876\n",
            "Epoch [79/100], Step [8000/10000], Loss: 0.4441\n",
            "Epoch [79/100], Step [9000/10000], Loss: 0.1943\n",
            "Epoch [79/100], Step [10000/10000], Loss: 0.3459\n",
            "Epoch [80/100], Step [1000/10000], Loss: 0.6671\n",
            "Epoch [80/100], Step [2000/10000], Loss: 1.0566\n",
            "Epoch [80/100], Step [3000/10000], Loss: 0.7060\n",
            "Epoch [80/100], Step [4000/10000], Loss: 0.8153\n",
            "Epoch [80/100], Step [5000/10000], Loss: 1.4346\n",
            "Epoch [80/100], Step [6000/10000], Loss: 0.2871\n",
            "Epoch [80/100], Step [7000/10000], Loss: 0.7518\n",
            "Epoch [80/100], Step [8000/10000], Loss: 0.7941\n",
            "Epoch [80/100], Step [9000/10000], Loss: 0.4171\n",
            "Epoch [80/100], Step [10000/10000], Loss: 0.2370\n",
            "Epoch [81/100], Step [1000/10000], Loss: 0.2912\n",
            "Epoch [81/100], Step [2000/10000], Loss: 1.9908\n",
            "Epoch [81/100], Step [3000/10000], Loss: 1.9389\n",
            "Epoch [81/100], Step [4000/10000], Loss: 0.2122\n",
            "Epoch [81/100], Step [5000/10000], Loss: 0.9578\n",
            "Epoch [81/100], Step [6000/10000], Loss: 0.8050\n",
            "Epoch [81/100], Step [7000/10000], Loss: 0.8457\n",
            "Epoch [81/100], Step [8000/10000], Loss: 0.5279\n",
            "Epoch [81/100], Step [9000/10000], Loss: 1.3677\n",
            "Epoch [81/100], Step [10000/10000], Loss: 0.7391\n",
            "Epoch [82/100], Step [1000/10000], Loss: 1.2443\n",
            "Epoch [82/100], Step [2000/10000], Loss: 0.6056\n",
            "Epoch [82/100], Step [3000/10000], Loss: 0.4321\n",
            "Epoch [82/100], Step [4000/10000], Loss: 1.8173\n",
            "Epoch [82/100], Step [5000/10000], Loss: 0.4377\n",
            "Epoch [82/100], Step [6000/10000], Loss: 0.6004\n",
            "Epoch [82/100], Step [7000/10000], Loss: 0.2929\n",
            "Epoch [82/100], Step [8000/10000], Loss: 0.3871\n",
            "Epoch [82/100], Step [9000/10000], Loss: 0.5432\n",
            "Epoch [82/100], Step [10000/10000], Loss: 0.5442\n",
            "Epoch [83/100], Step [1000/10000], Loss: 0.6333\n",
            "Epoch [83/100], Step [2000/10000], Loss: 0.7226\n",
            "Epoch [83/100], Step [3000/10000], Loss: 0.2463\n",
            "Epoch [83/100], Step [4000/10000], Loss: 0.7391\n",
            "Epoch [83/100], Step [5000/10000], Loss: 0.2166\n",
            "Epoch [83/100], Step [6000/10000], Loss: 0.4397\n",
            "Epoch [83/100], Step [7000/10000], Loss: 0.5915\n",
            "Epoch [83/100], Step [8000/10000], Loss: 0.1233\n",
            "Epoch [83/100], Step [9000/10000], Loss: 1.1016\n",
            "Epoch [83/100], Step [10000/10000], Loss: 0.6548\n",
            "Epoch [84/100], Step [1000/10000], Loss: 0.1348\n",
            "Epoch [84/100], Step [2000/10000], Loss: 0.1015\n",
            "Epoch [84/100], Step [3000/10000], Loss: 0.2978\n",
            "Epoch [84/100], Step [4000/10000], Loss: 1.2631\n",
            "Epoch [84/100], Step [5000/10000], Loss: 0.7318\n",
            "Epoch [84/100], Step [6000/10000], Loss: 0.5514\n",
            "Epoch [84/100], Step [7000/10000], Loss: 0.9418\n",
            "Epoch [84/100], Step [8000/10000], Loss: 0.6361\n",
            "Epoch [84/100], Step [9000/10000], Loss: 1.2482\n",
            "Epoch [84/100], Step [10000/10000], Loss: 0.8508\n",
            "Epoch [85/100], Step [1000/10000], Loss: 3.4121\n",
            "Epoch [85/100], Step [2000/10000], Loss: 0.1710\n",
            "Epoch [85/100], Step [3000/10000], Loss: 0.1301\n",
            "Epoch [85/100], Step [4000/10000], Loss: 1.0815\n",
            "Epoch [85/100], Step [5000/10000], Loss: 0.2004\n",
            "Epoch [85/100], Step [6000/10000], Loss: 0.6319\n",
            "Epoch [85/100], Step [7000/10000], Loss: 1.0956\n",
            "Epoch [85/100], Step [8000/10000], Loss: 0.4267\n",
            "Epoch [85/100], Step [9000/10000], Loss: 1.0744\n",
            "Epoch [85/100], Step [10000/10000], Loss: 0.7242\n",
            "Epoch [86/100], Step [1000/10000], Loss: 0.5322\n",
            "Epoch [86/100], Step [2000/10000], Loss: 0.1641\n",
            "Epoch [86/100], Step [3000/10000], Loss: 0.5794\n",
            "Epoch [86/100], Step [4000/10000], Loss: 0.1485\n",
            "Epoch [86/100], Step [5000/10000], Loss: 1.3452\n",
            "Epoch [86/100], Step [6000/10000], Loss: 1.9296\n",
            "Epoch [86/100], Step [7000/10000], Loss: 0.5382\n",
            "Epoch [86/100], Step [8000/10000], Loss: 0.8606\n",
            "Epoch [86/100], Step [9000/10000], Loss: 0.1036\n",
            "Epoch [86/100], Step [10000/10000], Loss: 0.0973\n",
            "Epoch [87/100], Step [1000/10000], Loss: 0.8952\n",
            "Epoch [87/100], Step [2000/10000], Loss: 1.7728\n",
            "Epoch [87/100], Step [3000/10000], Loss: 0.6123\n",
            "Epoch [87/100], Step [4000/10000], Loss: 0.3387\n",
            "Epoch [87/100], Step [5000/10000], Loss: 2.2303\n",
            "Epoch [87/100], Step [6000/10000], Loss: 0.2166\n",
            "Epoch [87/100], Step [7000/10000], Loss: 1.0249\n",
            "Epoch [87/100], Step [8000/10000], Loss: 0.4190\n",
            "Epoch [87/100], Step [9000/10000], Loss: 0.8656\n",
            "Epoch [87/100], Step [10000/10000], Loss: 0.0428\n",
            "Epoch [88/100], Step [1000/10000], Loss: 0.4198\n",
            "Epoch [88/100], Step [2000/10000], Loss: 1.4448\n",
            "Epoch [88/100], Step [3000/10000], Loss: 0.3486\n",
            "Epoch [88/100], Step [4000/10000], Loss: 0.6675\n",
            "Epoch [88/100], Step [5000/10000], Loss: 0.6611\n",
            "Epoch [88/100], Step [6000/10000], Loss: 1.1923\n",
            "Epoch [88/100], Step [7000/10000], Loss: 1.0196\n",
            "Epoch [88/100], Step [8000/10000], Loss: 0.4324\n",
            "Epoch [88/100], Step [9000/10000], Loss: 0.5867\n",
            "Epoch [88/100], Step [10000/10000], Loss: 1.2368\n",
            "Epoch [89/100], Step [1000/10000], Loss: 0.0328\n",
            "Epoch [89/100], Step [2000/10000], Loss: 1.1381\n",
            "Epoch [89/100], Step [3000/10000], Loss: 0.3102\n",
            "Epoch [89/100], Step [4000/10000], Loss: 0.7966\n",
            "Epoch [89/100], Step [5000/10000], Loss: 0.3484\n",
            "Epoch [89/100], Step [6000/10000], Loss: 1.2274\n",
            "Epoch [89/100], Step [7000/10000], Loss: 1.2518\n",
            "Epoch [89/100], Step [8000/10000], Loss: 0.8242\n",
            "Epoch [89/100], Step [9000/10000], Loss: 2.0710\n",
            "Epoch [89/100], Step [10000/10000], Loss: 0.6651\n",
            "Epoch [90/100], Step [1000/10000], Loss: 0.6126\n",
            "Epoch [90/100], Step [2000/10000], Loss: 0.7442\n",
            "Epoch [90/100], Step [3000/10000], Loss: 0.2474\n",
            "Epoch [90/100], Step [4000/10000], Loss: 0.6975\n",
            "Epoch [90/100], Step [5000/10000], Loss: 1.0023\n",
            "Epoch [90/100], Step [6000/10000], Loss: 0.2416\n",
            "Epoch [90/100], Step [7000/10000], Loss: 0.5540\n",
            "Epoch [90/100], Step [8000/10000], Loss: 0.6584\n",
            "Epoch [90/100], Step [9000/10000], Loss: 0.1606\n",
            "Epoch [90/100], Step [10000/10000], Loss: 0.8338\n",
            "Epoch [91/100], Step [1000/10000], Loss: 0.5973\n",
            "Epoch [91/100], Step [2000/10000], Loss: 0.0788\n",
            "Epoch [91/100], Step [3000/10000], Loss: 0.5834\n",
            "Epoch [91/100], Step [4000/10000], Loss: 0.2105\n",
            "Epoch [91/100], Step [5000/10000], Loss: 0.8031\n",
            "Epoch [91/100], Step [6000/10000], Loss: 0.0023\n",
            "Epoch [91/100], Step [7000/10000], Loss: 0.2027\n",
            "Epoch [91/100], Step [8000/10000], Loss: 0.2658\n",
            "Epoch [91/100], Step [9000/10000], Loss: 2.7077\n",
            "Epoch [91/100], Step [10000/10000], Loss: 0.6060\n",
            "Epoch [92/100], Step [1000/10000], Loss: 1.5376\n",
            "Epoch [92/100], Step [2000/10000], Loss: 0.5620\n",
            "Epoch [92/100], Step [3000/10000], Loss: 0.1787\n",
            "Epoch [92/100], Step [4000/10000], Loss: 1.0275\n",
            "Epoch [92/100], Step [5000/10000], Loss: 0.4536\n",
            "Epoch [92/100], Step [6000/10000], Loss: 0.4652\n",
            "Epoch [92/100], Step [7000/10000], Loss: 0.3814\n",
            "Epoch [92/100], Step [8000/10000], Loss: 0.6082\n",
            "Epoch [92/100], Step [9000/10000], Loss: 0.1992\n",
            "Epoch [92/100], Step [10000/10000], Loss: 0.5050\n",
            "Epoch [93/100], Step [1000/10000], Loss: 0.6016\n",
            "Epoch [93/100], Step [2000/10000], Loss: 0.0407\n",
            "Epoch [93/100], Step [3000/10000], Loss: 0.4391\n",
            "Epoch [93/100], Step [4000/10000], Loss: 0.9431\n",
            "Epoch [93/100], Step [5000/10000], Loss: 0.2409\n",
            "Epoch [93/100], Step [6000/10000], Loss: 1.6160\n",
            "Epoch [93/100], Step [7000/10000], Loss: 0.5139\n",
            "Epoch [93/100], Step [8000/10000], Loss: 0.4467\n",
            "Epoch [93/100], Step [9000/10000], Loss: 0.4081\n",
            "Epoch [93/100], Step [10000/10000], Loss: 1.4180\n",
            "Epoch [94/100], Step [1000/10000], Loss: 0.0350\n",
            "Epoch [94/100], Step [2000/10000], Loss: 0.5607\n",
            "Epoch [94/100], Step [3000/10000], Loss: 0.4028\n",
            "Epoch [94/100], Step [4000/10000], Loss: 0.0597\n",
            "Epoch [94/100], Step [5000/10000], Loss: 0.5582\n",
            "Epoch [94/100], Step [6000/10000], Loss: 0.9652\n",
            "Epoch [94/100], Step [7000/10000], Loss: 0.0011\n",
            "Epoch [94/100], Step [8000/10000], Loss: 1.4435\n",
            "Epoch [94/100], Step [9000/10000], Loss: 1.6993\n",
            "Epoch [94/100], Step [10000/10000], Loss: 0.7106\n",
            "Epoch [95/100], Step [1000/10000], Loss: 0.3462\n",
            "Epoch [95/100], Step [2000/10000], Loss: 0.3141\n",
            "Epoch [95/100], Step [3000/10000], Loss: 0.8721\n",
            "Epoch [95/100], Step [4000/10000], Loss: 1.6375\n",
            "Epoch [95/100], Step [5000/10000], Loss: 0.7117\n",
            "Epoch [95/100], Step [6000/10000], Loss: 0.4262\n",
            "Epoch [95/100], Step [7000/10000], Loss: 0.4913\n",
            "Epoch [95/100], Step [8000/10000], Loss: 0.6584\n",
            "Epoch [95/100], Step [9000/10000], Loss: 1.9035\n",
            "Epoch [95/100], Step [10000/10000], Loss: 2.2942\n",
            "Epoch [96/100], Step [1000/10000], Loss: 0.5786\n",
            "Epoch [96/100], Step [2000/10000], Loss: 0.5986\n",
            "Epoch [96/100], Step [3000/10000], Loss: 0.5085\n",
            "Epoch [96/100], Step [4000/10000], Loss: 0.6924\n",
            "Epoch [96/100], Step [5000/10000], Loss: 0.4726\n",
            "Epoch [96/100], Step [6000/10000], Loss: 0.9406\n",
            "Epoch [96/100], Step [7000/10000], Loss: 0.3891\n",
            "Epoch [96/100], Step [8000/10000], Loss: 1.1125\n",
            "Epoch [96/100], Step [9000/10000], Loss: 0.1740\n",
            "Epoch [96/100], Step [10000/10000], Loss: 0.5574\n",
            "Epoch [97/100], Step [1000/10000], Loss: 0.1344\n",
            "Epoch [97/100], Step [2000/10000], Loss: 0.0239\n",
            "Epoch [97/100], Step [3000/10000], Loss: 0.4299\n",
            "Epoch [97/100], Step [4000/10000], Loss: 0.7831\n",
            "Epoch [97/100], Step [5000/10000], Loss: 6.3830\n",
            "Epoch [97/100], Step [6000/10000], Loss: 1.5077\n",
            "Epoch [97/100], Step [7000/10000], Loss: 0.3181\n",
            "Epoch [97/100], Step [8000/10000], Loss: 0.6240\n",
            "Epoch [97/100], Step [9000/10000], Loss: 0.4512\n",
            "Epoch [97/100], Step [10000/10000], Loss: 0.7606\n",
            "Epoch [98/100], Step [1000/10000], Loss: 0.3963\n",
            "Epoch [98/100], Step [2000/10000], Loss: 1.1005\n",
            "Epoch [98/100], Step [3000/10000], Loss: 0.9201\n",
            "Epoch [98/100], Step [4000/10000], Loss: 1.4772\n",
            "Epoch [98/100], Step [5000/10000], Loss: 0.3459\n",
            "Epoch [98/100], Step [6000/10000], Loss: 2.1802\n",
            "Epoch [98/100], Step [7000/10000], Loss: 1.0293\n",
            "Epoch [98/100], Step [8000/10000], Loss: 0.8066\n",
            "Epoch [98/100], Step [9000/10000], Loss: 0.6674\n",
            "Epoch [98/100], Step [10000/10000], Loss: 0.0634\n",
            "Epoch [99/100], Step [1000/10000], Loss: 0.4325\n",
            "Epoch [99/100], Step [2000/10000], Loss: 0.9217\n",
            "Epoch [99/100], Step [3000/10000], Loss: 1.4274\n",
            "Epoch [99/100], Step [4000/10000], Loss: 0.7905\n",
            "Epoch [99/100], Step [5000/10000], Loss: 0.6169\n",
            "Epoch [99/100], Step [6000/10000], Loss: 0.8504\n",
            "Epoch [99/100], Step [7000/10000], Loss: 0.2980\n",
            "Epoch [99/100], Step [8000/10000], Loss: 0.4902\n",
            "Epoch [99/100], Step [9000/10000], Loss: 0.4922\n",
            "Epoch [99/100], Step [10000/10000], Loss: 0.8962\n",
            "Epoch [100/100], Step [1000/10000], Loss: 0.5203\n",
            "Epoch [100/100], Step [2000/10000], Loss: 1.0443\n",
            "Epoch [100/100], Step [3000/10000], Loss: 0.2942\n",
            "Epoch [100/100], Step [4000/10000], Loss: 1.2988\n",
            "Epoch [100/100], Step [5000/10000], Loss: 0.5092\n",
            "Epoch [100/100], Step [6000/10000], Loss: 0.5210\n",
            "Epoch [100/100], Step [7000/10000], Loss: 0.1974\n",
            "Epoch [100/100], Step [8000/10000], Loss: 2.8484\n",
            "Epoch [100/100], Step [9000/10000], Loss: 1.7220\n",
            "Epoch [100/100], Step [10000/10000], Loss: 0.8933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 输出测试集精度\n",
        "#总精度及各类精度相关参数定义\n",
        "correct = 0\n",
        "total = 0\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        for label, prediction in zip(labels, predicted):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "print('Accuracy of the net on the train iamges is {:.2f} %'.format(accuracy_history[-1]))\n",
        "print('Accuracy of the net on the test iamges is {:.2f} %'.format(100 * correct / total))\n",
        "print('\\n')\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU9l2rwJnRgT",
        "outputId": "c1e3b11d-5f45-4e2f-8f6e-d7fa72537295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the net on the train iamges is 76.71 %\n",
            "Accuracy of the net on the test iamges is 76.44 %\n",
            "\n",
            "\n",
            "Accuracy for class: plane is 84.0 %\n",
            "Accuracy for class: car   is 88.0 %\n",
            "Accuracy for class: bird  is 69.6 %\n",
            "Accuracy for class: cat   is 61.5 %\n",
            "Accuracy for class: deer  is 60.6 %\n",
            "Accuracy for class: dog   is 61.2 %\n",
            "Accuracy for class: frog  is 86.1 %\n",
            "Accuracy for class: horse is 81.7 %\n",
            "Accuracy for class: ship  is 85.5 %\n",
            "Accuracy for class: truck is 86.2 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  保存模型\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "metadata": {
        "id": "f1oRlNU4ndmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 可视化数据查看\n",
        "import itertools\n",
        "def print_label(input_dex):\n",
        "  plt.figure(figsize=(10, 4))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(range(num_epochs), loss_history)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Loss Curve')\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(range(num_epochs), accuracy_history)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.title('Accuracy Curve')\n",
        "\n",
        "  curve_fig = \"curve.png\"\n",
        "  plt.savefig(curve_fig)\n",
        "  curve = Image.open(curve_fig)\n",
        "  k = int(int(input_dex)/batch_size)\n",
        "  m = int(int(input_dex)%batch_size)\n",
        "  data_iter = iter(test_loader)\n",
        "  images, labels = next(itertools.islice(data_iter, k, k+1))\n",
        "  image = images[m].numpy()\n",
        "  image = np.transpose(image, (1,2,0))\n",
        "  show_img = image\n",
        "  imagebatch = image.reshape(-1,3,32,32)\n",
        "  # 转换为torch tensor\n",
        "  image_tensor = torch.from_numpy(imagebatch)\n",
        "  image_tensor = image_tensor.cuda()\n",
        "  # 调用模型进行评估\n",
        "  model.eval()\n",
        "  output = model(image_tensor)\n",
        "  precise, predicted = torch.max(output.data, 1)\n",
        "  pre = predicted.cpu().numpy()\n",
        "\n",
        "  return curve,classes[pre[0]],classes[labels[m].numpy()],show_img\n",
        "\n",
        "\n",
        "demo = gr.Interface(fn=print_label,\n",
        "                    inputs=gr.Textbox(label=\"Input 0-9999\"),\n",
        "                    outputs=[gr.outputs.Image(type=\"pil\",label=\"loss and acc\"),\n",
        "                    gr.Textbox(label=\"predict label\"),\n",
        "                    gr.Textbox(label=\"true label\"),\n",
        "                    gr.outputs.Image(type=\"pil\",label=\"Image\")]\n",
        "                    )\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "OLPAo19Dn2kL",
        "outputId": "ac2c72a6-f3aa-4406-ce65-bfc1a542a9f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-98ea5b7e7589>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcurve\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m demo = gr.Interface(fn=print_label,\n\u001b[0m\u001b[1;32m     39\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Input 0-9999\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     outputs=[gr.outputs.Image(type=\"pil\",label=\"loss and acc\"),\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gr' is not defined"
          ]
        }
      ]
    }
  ]
}