{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMnwQaooQI3ieglPRChKkwB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reachel1/Freshman_learn_AI/blob/main/test1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHO9ZcicUtvv",
        "outputId": "3de324bc-40e1-4975-ac52-95f6feadce60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jul  5 09:21:03 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "# Importing drive method from colab for accessing google drive\n",
        "from google.colab import drive\n",
        "# Mounting drive\n",
        "# This will require authentication : Follow the steps as guided\n",
        "drive.mount('/Data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP7T3VlxWiCp",
        "outputId": "f6f9895b-01cc-421f-93d8-21826d95fd91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /Data/'My Drive'/dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBSdroL7fSA8",
        "outputId": "0b17b515-6b7a-46de-b7b0-3316bd31d33c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cifar-10-python.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt # plt 用于显示图片\n",
        "import matplotlib.image as mpimg # mpimg 用于读取图片\n",
        "import numpy as np\n",
        "\n",
        "#resize功能\n",
        "from scipy import misc\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# 超参数设置\n",
        "num_epochs = 20\n",
        "num_classes = 10\n",
        "batch_size = 5\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "y9tms4NwcMFw"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "# 数据增广方法\n",
        "transform = transforms.Compose([\n",
        "    # +4填充至36x36\n",
        "    transforms.Pad(4),\n",
        "    # 随机水平翻转\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # 随机裁剪至32x32\n",
        "    transforms.RandomCrop(32),\n",
        "    # 转换至Tensor\n",
        "    transforms.ToTensor(),\n",
        "    ])"
      ],
      "metadata": {
        "id": "TxoYrdwKits0"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import os\n",
        "import pickle\n",
        "def load_cifar_data(filepath,train):\n",
        "  if os.path.isdir(filepath):\n",
        "    if train:\n",
        "      data = []\n",
        "      label = []\n",
        "      for name in sorted(os.listdir(filepath))[1:6]:\n",
        "        print(name)\n",
        "        data0 = list(load_cifar_batch(os.path.join(filepath,name)).get(b'data'))\n",
        "        label0 = list(load_cifar_batch(os.path.join(filepath,name)).get(b'labels'))\n",
        "        data.append(data0)\n",
        "        label.append(label0)\n",
        "      dic = (data,label)\n",
        "      return dic\n",
        "    else:\n",
        "      print(sorted(os.listdir(filepath))[7])\n",
        "      data = list(load_cifar_batch(os.path.join(filepath,sorted(os.listdir(filepath))[7])).get(b'data'))\n",
        "      label = list(load_cifar_batch(os.path.join(filepath,sorted(os.listdir(filepath))[7])).get(b'labels'))\n",
        "      dic = (data,label)\n",
        "      return dic\n",
        "def load_cifar_batch(filename):\n",
        "  with open(filename,'rb') as fo:\n",
        "          data0 = pickle.load(fo,encoding='bytes')\n",
        "          #data0['data'] = data0['data'].reshape(10000,3,32,32).transpose(0,2,3,1).astype(\"float\")\n",
        "          return data0\n",
        "train_data = load_cifar_data('/Data/My Drive/dataset/cifar-10-batches-py',True)\n",
        "test_data = load_cifar_data('/Data/My Drive/dataset/cifar-10-batches-py',False)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fOPqPK4pkYj",
        "outputId": "60fb2178-4c25-48dd-a163-d0e47490793e"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_batch_1\n",
            "data_batch_2\n",
            "data_batch_3\n",
            "data_batch_4\n",
            "data_batch_5\n",
            "test_batch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LOAD_CIFAR = True\n",
        "DOWNLOAD_CIFAR = False\n",
        "# 从data继承读取数据集的类\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 训练数据集\n",
        "train_data = torchvision.datasets.CIFAR10(\n",
        "    root='/Data/My Drive/dataset',\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=DOWNLOAD_CIFAR,\n",
        ")\n",
        "\n",
        "# 测试数据集\n",
        "test_data = torchvision.datasets.CIFAR10(\n",
        "    root='/Data/My Drive/dataset',\n",
        "    train=False,\n",
        "    transform=transform\n",
        ")"
      ],
      "metadata": {
        "id": "c0IVXuIfcbtn"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练数据加载器\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "# 测试数据加载器\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "UJDkPPJiBbqH"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 查看数据,取一组batch\n",
        "data_iter = iter(test_loader)\n",
        "\n",
        "images, labels = next(data_iter)\n",
        "# 取batch中的一张图像\n",
        "idx = 2\n",
        "image = images[idx].numpy()\n",
        "image = np.transpose(image, (1,2,0))\n",
        "plt.imshow(image)\n",
        "print(classes[labels[idx].numpy()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "OqUP_tXQjYk_",
        "outputId": "b3b40040-3093-4012-deee-8a91bc65444c"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ship\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAvPklEQVR4nO3dfXDV9Zn38c95zvMJAfJUggVRUBF2llWasXWtsAJ7j7dWuqNtZxa7jo5ucFbZblt2Wq3uzsTVe1rbDsU/dle2M0Vbd4qOzlZXscS7u+AuVG5qrVmgVEBIgEBykpOc59/9h2N2o6DfC074JuH9mjkzJLm48v09nFw5Ob/zOaEgCAIBAHCehX0vAABwYWIAAQC8YAABALxgAAEAvGAAAQC8YAABALxgAAEAvGAAAQC8iPpewAeVSiUdOXJEtbW1CoVCvpcDADAKgkCDg4NqbW1VOHzmxzkTbgAdOXJEbW1tvpcBADhHhw4d0qxZs8749XEbQBs2bNBjjz2mnp4eLV68WN///vd19dVXf+z/q62tlST9n82vqbKqxul7nTzR67yubDbjXCtJn5wz17m2vq7O1DsacX+EF49FTL3jH/FbxwfFDLWSFA3Z0puKRfd9XlNlOyUt+9BSK0mRkPs+7+8/ZepdU+t2br8vFo0510YN65akUNh9vxRKOVNv46ll6x1ybz48PGLqHY3a9mEiUeFcm8/Z9mEh715fYViHJIXC7tv5ybZmU2/pv3+en8m4DKAf//jHWrdunZ544gktXbpUjz/+uFasWKHu7m41NjZ+5P99/89ulVU1qqx2u5NWDA85r82ywyWpqvqjd+CY2hr3WkmKRdzvQOYBZOk93gOo4P7Ds6bavVaSYuM5gAznSqFYMPX+uDvmB8ViDKAP9TYMoEjE+ItN1FZfUeH+gz9nHUC5rHNtZUWlqbf156HVxz2NMi6nx7e//W3deeed+vKXv6zLL79cTzzxhKqqqvSP//iP4/HtAACTUNkHUC6X065du7R8+fL//ibhsJYvX67t27d/qD6bzSqVSo25AQCmvrIPoBMnTqhYLKqpqWnM55uamtTT0/Oh+s7OTiWTydEbFyAAwIXB++uA1q9fr4GBgdHboUOHfC8JAHAelP0ihBkzZigSiai3d+yVab29vWpu/vBVFIlEQolEotzLAABMcGV/BBSPx7VkyRJt3bp19HOlUklbt25Ve3t7ub8dAGCSGpfLsNetW6c1a9boD/7gD3T11Vfr8ccfVzqd1pe//OXx+HYAgEloXAbQrbfequPHj+uBBx5QT0+Pfu/3fk8vvvjihy5MAABcuMYtCWHt2rVau3btWf//2pqEqqrdXtyVTbs/h1TKDZvWURF3f5FedaXxxW6G10WGVTT1TkTd/7paGbf9JTaskqk+W3RfeyJqeyV3POa+dsPrLSXZXg1vfaFw2Phi3pBhnyficVNvw2uWlR7Om3pbzqy4cd2B3Pd52HjwY8YXolpeKJzPur+wVJKihhfcVlqfT/ect+n9KjgAwIWJAQQA8IIBBADwggEEAPCCAQQA8IIBBADwggEEAPCCAQQA8IIBBADwggEEAPBi3KJ4zlU0VFA0VHCqjUfcY0piYWOkTdhtDZJUYViHJMUi7jEY2RFbhFAk4h7JURG1vY98Ppsx1Yflvg+Dgq13EHI/hYuyxd/EY+77xRqto8B2HoYMvysWS7a4nOFh93Or7/hxU++mGdOca0PGuJxI3P3YR4zHPmI8noZEKEWN25ktut9/ooafKZKUz7v3Hg88AgIAeMEAAgB4wQACAHjBAAIAeMEAAgB4wQACAHjBAAIAeMEAAgB4wQACAHjBAAIAeMEAAgB4MWGz4MJBXuEg51QbkXv2VSxszGsz9A4XbXlt8Zh7XlsoYssOi4Xd9t17tbbToBRy7y1J4VLWubaQMWb1RaqdazM527qrqtyz4CLGfC+VbOehAvdssnTGlqe3a9cvnWvzxkzCaXVXOdcmErbfhy2xZ6HAuL9LtvMwbMiaCxlzAEslQ5aicd2Bofd44BEQAMALBhAAwAsGEADACwYQAMALBhAAwAsGEADACwYQAMALBhAAwAsGEADACwYQAMCLCRvFE4+GlIi5ZW3Ewu4xGCq6x8JIUkTuURUhY++YIs61+YItXqVYcs8pidTFTb1DgXs8kSSp5B6BUyoYI1OK7nFGQ6l+U+uaqgrn2rAhKkeSCjnbuRKNud9V+4dtcTknU+71lVHb76w5Q9JLLm879tG4+z4PjFE8xaLtHC8U3M/xnPHYx6Puxz4wRjyVirbonnLjERAAwAsGEADACwYQAMALBhAAwAsGEADACwYQAMALBhAAwAsGEADACwYQAMALBhAAwAsGEADAiwmbBRcLFRQLuQVJxcLu+Uf5rC1TLWzIggtKxt4h990fDRtCtSRFI4YsuJAt9yowZt5J7pldhZJtLUW51w8Npky9DxrOlbAxI82aTdZWV+Vc23f8uKn3/9uzx7l20RVXmHqXDOdhtuiepyZJFUHMfR2GPEJJGhm21cej7sezkLdl9UWi7sc+X7D9nMhmbWspNx4BAQC8KPsA+ta3vqVQKDTmtmDBgnJ/GwDAJDcuf4K74oor9Morr/z3NzHEiQMALgzjMhmi0aiam5vHozUAYIoYl+eA9u7dq9bWVs2dO1df+tKXdPDgwTPWZrNZpVKpMTcAwNRX9gG0dOlSbdq0SS+++KI2btyoAwcO6DOf+YwGBwdPW9/Z2alkMjl6a2trK/eSAAATUNkH0KpVq/Qnf/InWrRokVasWKF/+Zd/UX9/v37yk5+ctn79+vUaGBgYvR06dKjcSwIATEDjfnVAfX29Lr30Uu3bt++0X08kEkokEuO9DADABDPurwMaGhrS/v371dLSMt7fCgAwiZR9AH3lK19RV1eXfve73+nf//3f9bnPfU6RSERf+MIXyv2tAACTWNn/BHf48GF94QtfUF9fn2bOnKlPf/rT2rFjh2bOnGnqE8qnFcq5xXiEC+6xGSMDxqvsDFEVQdgWIxOpdN/9cWNETTwaca4N5dOm3kVrfEfRsJaoe3SLJAUh92OfTg+Yevf2um9ndV2NqXcQNkb3GF5LlxuyHZ+KmPufwI/395t6//JN95if6oT7eSJJ8+bOda6NGuKgJCk7fPqLps6kMurev5QdMfUuFtxjforu6UTvyfi96rjsA+jpp58ud0sAwBREFhwAwAsGEADACwYQAMALBhAAwAsGEADACwYQAMALBhAAwAsGEADACwYQAMALBhAAwItxfzuGsxUPAsUDt3wlSxZcIiia1lFTcs8mS8qWZRUecM9gS5Rs664wRF+Fh23ZVOGMLWssHja83UbRlgWXS7kf+9pq29t+TGtocK49cLjH1Pu3h2z1/7Vvq3PtqRP9pt5DGUOWYv7Xpt4RuffOG7P6Fs6/1Ln2f/+vlaben2iabqrPVrjfPzNpW/ZiLu1+rtQFxszNEVvmXbnxCAgA4AUDCADgBQMIAOAFAwgA4AUDCADgBQMIAOAFAwgA4AUDCADgBQMIAOAFAwgA4MWEjeI5cuiIqqqqnGoHU+5xEsV81rSOd99917n2VCJm6p0eSjnXNk53j4WRpJrqCufaSNQW85PLF0z10Xilc204Gjf1ThtigTJhW8yPAve7x8EjJ0ytDxw+aapP59z3S0Wy0dQ7VF1yrq0xdZaq4+6/4x59579MvY8c6XWu/b//999MvS+7ZK6pfmZ9nXPtyFC/qXc61edcm79svqn30MApU3258QgIAOAFAwgA4AUDCADgBQMIAOAFAwgA4AUDCADgBQMIAOAFAwgA4AUDCADgBQMIAOAFAwgA4MWEzYLb+cabSiQSTrWlknuW2chI2rSO3/Ucca61Ro1FDeN/WtI9a0qSqivcs8MSxnXHohFTfdTxOEpSOOqeYSdJw5mc+zqM+zCIuK+75+SQqXe+ZPvdr6q23lBty+rLDbnn6YVlO1kyGff7W12t7fh8asmVzrXpAVv2XiaTMdUfPOieqbZ//35T75FC4Fz7Tt+Irfew7edhufEICADgBQMIAOAFAwgA4AUDCADgBQMIAOAFAwgA4AUDCADgBQMIAOAFAwgA4AUDCADgBQMIAODFhM2C2/tur2IxtzyzIHDPvsoW3HOvJCk5bbpzbSJuyzHLGXKyjg/ZsqkiIffMrtqKalPvQjFvqg/F3H/PiURs+zAUdV97Ih0z9c7lU861J0/assYk93wvSTIcTuWKWVPvwbR7flhuxNa7bWaDc+30ac2m3un0gHPtyVPHTb2n19vOwz9YfIVz7eGj75p6D4y4Zy++fbjP1DsctuU6lhuPgAAAXpgH0GuvvaYbb7xRra2tCoVCevbZZ8d8PQgCPfDAA2ppaVFlZaWWL1+uvXv3lmu9AIApwjyA0um0Fi9erA0bNpz2648++qi+973v6YknntDrr7+u6upqrVixwhxvDgCY2szPAa1atUqrVq067deCINDjjz+ub3zjG7rpppskST/84Q/V1NSkZ599Vrfddtu5rRYAMGWU9TmgAwcOqKenR8uXLx/9XDKZ1NKlS7V9+/bT/p9sNqtUKjXmBgCY+so6gHp6eiRJTU1NYz7f1NQ0+rUP6uzsVDKZHL21tbWVc0kAgAnK+1Vw69ev18DAwOjt0KFDvpcEADgPyjqAmpvfu46/t7d3zOd7e3tHv/ZBiURCdXV1Y24AgKmvrANozpw5am5u1tatW0c/l0ql9Prrr6u9vb2c3woAMMmZr4IbGhrSvn37Rj8+cOCAdu/erYaGBs2ePVv33Xef/vZv/1aXXHKJ5syZo29+85tqbW3VzTffXM51AwAmOfMA2rlzpz772c+Ofrxu3TpJ0po1a7Rp0yZ99atfVTqd1l133aX+/n59+tOf1osvvqiKClu0xXAprGjJ7QFaVVWNc99KY9TLrLaLnWvzOVtEzfEzXJhxOif6bBEbTU2NzrWJGbNMvdP9trWUwiXn2uS0po8v+h8SiWnOtRnb4dFwwf2KzIpq25+Oi/khU30kVHSujUcSpt6xuHscS77CFt1y9e+7R9RcelGrqXcm5x5ldWC/7X6/v/stU337VVc617a12bbz4J53nGvzRVvEU6noHmM2HswD6LrrrlMQnHkjQ6GQHn74YT388MPntDAAwNTm/So4AMCFiQEEAPCCAQQA8IIBBADwggEEAPCCAQQA8IIBBADwggEEAPCCAQQA8IIBBADwwhzFc77MmNmsWNwtv6ki7j5HT5w4bFpHOj3oXlwKmXpn8u45TMmZp387izP5xJx5zrW1Sfc8NUmqm+GeMydJfSdPOdcWS7ZTMu8ekaaREffsMEkaHnbPa8vlR0y9JVswXTzuvl8qEtWm3rEg51zbaHy7lJnT3OsrYrbfh2cacgPr4jFT776DB0317+z/nXNtc8MMU++B3h3OtbGGmabeuYjfEcAjIACAFwwgAIAXDCAAgBcMIACAFwwgAIAXDCAAgBcMIACAFwwgAIAXDCAAgBcMIACAFxM2iicSjiriGBORzWac+4aMM/dkX79zbSrlHt0iSZFYwr22FDH1fufdXufaupQtRiaZrDfVRyJukUqSlM24x8JIUijkHmeUiBlP9+oq59LKwP1YSlI4aottUlByLq2udF+3JMUC91igWdNtMT9VcffzNp3qN/UuGKKSQoGpteYYoqwk6Tdv/9a59tJL59sWU3Q/x48eedfUOjGtwbaWMuMREADACwYQAMALBhAAwAsGEADACwYQAMALBhAAwAsGEADACwYQAMALBhAAwAsGEADACwYQAMCLCZsFN5zJKVZym48RQ9BTJOqeSyZJxaL7jI5Ga0y9S4F773ii1tR7xowW59qamkpT74pK2z5MJtzro7G4qXcQcs9UC4q2QLBCwT0jLVlnO/bhsG0tpaJ7Rl40sOXplbLumWrJhC3DLihknWuLRfdaScoV3HPmRowZg1W1SVP9Oz19zrVv7f9XU+9s1j2rMZ91z42TpCBiy5gsNx4BAQC8YAABALxgAAEAvGAAAQC8YAABALxgAAEAvGAAAQC8YAABALxgAAEAvGAAAQC8mLBRPHX1MxRPuEXE1FTGnPuWiu6xFpIUC7vHyDQ2tpp6h6Lu645X2OJy4ob4m4oK22kQidp+b7HE5YQitqgXGXpHQrZ1D6fdI2rCQcnUOxGz7fPAEN0zPOAeCyNJ7/5ur3PtyZjt+NRXum9n0/R6U++Kiirn2kzOGFETTZjqo1V1zrXHDx8x9W5rmelcW5uznYcpY3RPufEICADgBQMIAOCFeQC99tpruvHGG9Xa2qpQKKRnn312zNdvv/12hUKhMbeVK1eWa70AgCnCPIDS6bQWL16sDRs2nLFm5cqVOnr06OjtqaeeOqdFAgCmHvNFCKtWrdKqVas+siaRSKi5ufmsFwUAmPrG5Tmgbdu2qbGxUfPnz9c999yjvr4zX5WTzWaVSqXG3AAAU1/ZB9DKlSv1wx/+UFu3btXf/d3fqaurS6tWrVKxWDxtfWdnp5LJ5Oitra2t3EsCAExAZX8d0G233Tb67yuvvFKLFi3SxRdfrG3btmnZsmUfql+/fr3WrVs3+nEqlWIIAcAFYNwvw547d65mzJihffv2nfbriURCdXV1Y24AgKlv3AfQ4cOH1dfXp5aWlvH+VgCAScT8J7ihoaExj2YOHDig3bt3q6GhQQ0NDXrooYe0evVqNTc3a//+/frqV7+qefPmacWKFWVdOABgcjMPoJ07d+qzn/3s6MfvP3+zZs0abdy4UXv27NE//dM/qb+/X62trbrhhhv0N3/zN0okbNlKiUSVcxZcZbV77ll9XaNpHaWCewZXNB439a6sqXWuDUIRU+9wxP3QlgJjb+sDZ0N5YGwdyD37qlCw5QAWisPOtam+E6be1jtezJAFNzRw3NT76BH3bLKmBtufyOurZzjXDhtzzEqGTMKCcY8HRff9LUmfmOX+vPX8S+aaev/e5e71//XbQ6beb/zqN6b6cjMPoOuuu05BcOaD89JLL53TggAAFway4AAAXjCAAABeMIAAAF4wgAAAXjCAAABeMIAAAF4wgAAAXjCAAABeMIAAAF4wgAAAXpT9/YDKJZGoUryiyqk2Ysg9O3b8zO/OejqpgX7n2lLJNs/nXTrfuba+wT1TS5IiMfd8t5BsWXCFoi2zK5fLOtcO59Km3pmse15bIWd7t91QMe9cG2Tdt1GSquMxU319fYNzbWV8pql3NOSee1Zf43affF+y1r0+Z9yHw4b7Wy7rfiwlKRwqmOqnJd0z8qoStp8Thw+941wbsUXY6Yr5l9j+Q5nxCAgA4AUDCADgBQMIAOAFAwgA4AUDCADgBQMIAOAFAwgA4AUDCADgBQMIAOAFAwgA4MWEjeI52XdcsUSlU+3+Uyec+xaLtriP+mnTnGtbWppMvXMF93iQfC5j6l0Kis61qWFb/M3IiHv8jSQVC+77PBK2ZYnEY+6/Q1njbyqq3c4/SaqM2e5KmeEhU31J7vFH1TU1pt6RUMi5Nh6xxTZFIu7HJ2Y8PpmCe1xOyLjukGF/S1I+n3OuPdx3ytR7OD3gXBuNJky9m1tmmerLjUdAAAAvGEAAAC8YQAAALxhAAAAvGEAAAC8YQAAALxhAAAAvGEAAAC8YQAAALxhAAAAvGEAAAC8mbBbcu0cOKRpzyzWqqq5y7rvg8itN62iY0ehcW1Xlnh0mSZkR9wy2U6dOmnrn8+75a8OBe46VJFVVVZjqk3Xu+VTVCVuWVaUhPyxqyDyTpGLRPauvULDtw3zePatPkjJhQ+6ZbNsZDrvnpBWLtqy+vKE8Gombegcl93zETNaWpdh33D1fUpJO9LnXDw4Omnqf6u93rq2uqjb1TtRON9WXG4+AAABeMIAAAF4wgAAAXjCAAABeMIAAAF4wgAAAXjCAAABeMIAAAF4wgAAAXjCAAABeTNgongULr1S8wi1iJxpxjxKJVthiZAaHhpxrh4ZSpt6JhHuMTD5vi3opFdxjZFqbZpp6JypskSmRsHseS1Byj5yRpHRmxLk2k7JFoPQb4o/6Th439R4xxDBJ0mWXzXeujdXXm3pbgnsiYVvMT6bgfjyzadvxOdxzyLn2+Anb8cnlbPe34bT78RzoHzD1jkfcf0xbfl5J0tZXXzXVlxuPgAAAXpgGUGdnp6666irV1taqsbFRN998s7q7u8fUZDIZdXR0aPr06aqpqdHq1avV29tb1kUDACY/0wDq6upSR0eHduzYoZdffln5fF433HCD0v/j4ef999+v559/Xs8884y6urp05MgR3XLLLWVfOABgcjM9B/Tiiy+O+XjTpk1qbGzUrl27dO2112pgYED/8A//oM2bN+v666+XJD355JO67LLLtGPHDn3qU58q38oBAJPaOT0HNDDw3pNpDQ0NkqRdu3Ypn89r+fLlozULFizQ7NmztX379tP2yGazSqVSY24AgKnvrAdQqVTSfffdp2uuuUYLFy6UJPX09Cgej6v+A1fhNDU1qaen57R9Ojs7lUwmR29tbW1nuyQAwCRy1gOoo6NDb775pp5++ulzWsD69es1MDAwejt0yP3SSgDA5HVWrwNau3atXnjhBb322muaNWvW6Oebm5uVy+XU398/5lFQb2+vmpubT9srkUgoYXwbZgDA5Gd6BBQEgdauXastW7bo1Vdf1Zw5c8Z8fcmSJYrFYtq6devo57q7u3Xw4EG1t7eXZ8UAgCnB9Aioo6NDmzdv1nPPPafa2trR53WSyaQqKyuVTCZ1xx13aN26dWpoaFBdXZ3uvfdetbe3cwUcAGAM0wDauHGjJOm6664b8/knn3xSt99+uyTpO9/5jsLhsFavXq1sNqsVK1boBz/4QVkWCwCYOkwDKAg+PtOroqJCGzZs0IYNG856UZIUrUgoWuH23FA06p6pVgxKpnVEQu45ZtGI7ZoOS6xWhTF/bSTtnmU1MmDL4BqxlSsad98v4ZhtHwZF96yx7t+8Zep98He/c64tFG3ZYUFQNNW3tpz+OdTTaUgmTb1HhofHpVaS+k/1O9f2neoz9R7JuecAFg3niSQNG7dzwPDykbDcf6ZIUlXU/cd0z9Gjpt5nujr5fCELDgDgBQMIAOAFAwgA4AUDCADgBQMIAOAFAwgA4AUDCADgBQMIAOAFAwgA4AUDCADgxVm9HcP5UCzmVCy6La/SEFNTsiXxKCz3vJxSyRav0nvsmHNtOjVg6p0dMcSUFGwxJdZYk7nzPulcO7Nxhm0thgMaM0Q2SVIyWedcmzBGJUUipnJlshnn2re7u029h9JD47IOScobzq2SQ9TX/5QedM+EGjHcHyRpeDhtqs/l3KOYEoZoHUlKHTvhXNvf32/qXSzZ9nm58QgIAOAFAwgA4AUDCADgBQMIAOAFAwgA4AUDCADgBQMIAOAFAwgA4AUDCADgBQMIAOAFAwgA4MWEzYLLpAdUKrjlK4WL7jlMgWzZR+GI+y4qFvKm3nv3/pdz7dBAv6l3POa+7liiwtQ7agwyKxXcM/LCBWNYX9H9eE5vaDC1DrvHAGp4xD1PTZJGjPWHDh12rrWsW5JChl9Dg7Dtd9bhnHt23IAxxyzd556PGDPmrxWM9+VC0f0cT/enbL1H3HPpioZ1vIcsOADABYgBBADwggEEAPCCAQQA8IIBBADwggEEAPCCAQQA8IIBBADwggEEAPCCAQQA8GLCRvHkR9JSyS1WoiLqnj0SCttiZMIR9xkdNsTfSFJdXY1zbUXMtu6a6irn2khFwtS7qsIW3VPIu8ea7H37bVPvgZMn3WvTg6bexcA91iQWtx2fqOG8kqREPO5cGwrb4lWGMyPOtcdP9tl6Z92jeCLG++a0unrn2lzGfR2SPVqpkHc/V0rmuBxDtlLIlsMUsuQwjQMeAQEAvGAAAQC8YAABALxgAAEAvGAAAQC8YAABALxgAAEAvGAAAQC8YAABALxgAAEAvGAAAQC8mLBZcMVCSeFIyak2FHLPkCoVbDlM2ax7flix4J55JkmVUffdH47FTL1H0mnn2uzJI6beh4ZtOVmlQsG5NhTYcsxihv0Sidoy7GIV7udV2HhPyuXc94kkDZ1yz2vLZGzHJ5MZdq61JY1JFWH333HzmZypd17ux2fEkHcnSSMjtvpSye1nlSSFwra9WDBk5AVF22OKeMx6RMuLR0AAAC9MA6izs1NXXXWVamtr1djYqJtvvlnd3d1jaq677jqFQqExt7vvvrusiwYATH6mAdTV1aWOjg7t2LFDL7/8svL5vG644QalP/DnnjvvvFNHjx4dvT366KNlXTQAYPIz/eX6xRdfHPPxpk2b1NjYqF27dunaa68d/XxVVZWam5vLs0IAwJR0Ts8BDQwMSJIaGhrGfP5HP/qRZsyYoYULF2r9+vUaHj7zk5zZbFapVGrMDQAw9Z31VXClUkn33XefrrnmGi1cuHD081/84hd10UUXqbW1VXv27NHXvvY1dXd366c//elp+3R2duqhhx4622UAACapsx5AHR0devPNN/WLX/xizOfvuuuu0X9feeWVamlp0bJly7R//35dfPHFH+qzfv16rVu3bvTjVCqltra2s10WAGCSOKsBtHbtWr3wwgt67bXXNGvWrI+sXbp0qSRp3759px1AiURCiUTibJYBAJjETAMoCALde++92rJli7Zt26Y5c+Z87P/ZvXu3JKmlpeWsFggAmJpMA6ijo0ObN2/Wc889p9raWvX09EiSksmkKisrtX//fm3evFl//Md/rOnTp2vPnj26//77de2112rRokXjsgEAgMnJNIA2btwo6b0Xm/5PTz75pG6//XbF43G98sorevzxx5VOp9XW1qbVq1frG9/4RtkWDACYGsx/gvsobW1t6urqOqcFve/wO79VNBZ3qj12xK1OkrKZrGkdxYJ7fT5vzLLKu+eBfdy+/6CwIW8qFrPl40Wjtqv3IxH3LKtozL1WkkKGKKtC0ZbVl0m7H89s1j17T5IGU7asscAQHVdda8u8ixjy2oKCe+aZJGXT7jlzBWOW4kDW/fhYs92KJdt9ImRIySsFtn1oEY3aMiNDJVsmYbmRBQcA8IIBBADwggEEAPCCAQQA8IIBBADwggEEAPCCAQQA8IIBBADwggEEAPCCAQQA8OKs3w9ovL37298oHHFbXrHoHpsRMkTUSFI04R5tEYrYeocMOTJxx1ii91VVVY3LOqT33ozQolBwj/sYGrLFseRy7r1LgW07wyH386pkjPmJJ9yPjyQ1trY616aHBky9U/2nnGsLOdt2BoZjb4mzkaThnCXmxxY5Y42+sizdup0xw8+siGz3zeHhQVN9ufEICADgBQMIAOAFAwgA4AUDCADgBQMIAOAFAwgA4AUDCADgBQMIAOAFAwgA4AUDCADgBQMIAODFhM2CCxczCiviVFuy5E0Zs+CKYfddFA5su9MSwZYtZk29C3n3nCxrRpole88qGrXtw1jcPSMvEnXP9ZOkqCEPrFiwZXBVxG3bmahMONee6rOdK+nBIefaWNjtPvm+SMj9d9xc1niOB+77PJAt282ajxgOu29nyJgzVxF13+dDqX5T7+G0LTew3HgEBADwggEEAPCCAQQA8IIBBADwggEEAPCCAQQA8IIBBADwggEEAPCCAQQA8IIBBADwYsJG8QRBQYFjZEUQuEfxBCVbxEaQN8SxGCNqLCsJGaI+JKkYcY/viMTc42wkKZFwj4WRpIhh7WHDuiWZAlaCku34FPPu0TDFkRFT71zMtg9HRtLOtekh92gdyRhlFbcdn8yweySU6/19tN5wl7B1tkfxWPpHjfflIOd+Hp7q6zX1zuds52258QgIAOAFAwgA4AUDCADgBQMIAOAFAwgA4AUDCADgBQMIAOAFAwgA4AUDCADgBQMIAOAFAwgA4MWEzYIrBoF7FpwhtykStW1yOBpzr40YexsyoSJhWwaXKVMtYvs9xJpLF5RKzrUFQy6ZJBUN+W75gi0LLpJxz8nKDw2aeheNx7M6m3GutWS7SVLYcP/Jjriv473FWFPYLK3Hr7f1PIzG3H9ORIx5hyd7jznX5rPumYGSZIy8KzseAQEAvDANoI0bN2rRokWqq6tTXV2d2tvb9bOf/Wz065lMRh0dHZo+fbpqamq0evVq9fba0lkBABcG0wCaNWuWHnnkEe3atUs7d+7U9ddfr5tuukm//vWvJUn333+/nn/+eT3zzDPq6urSkSNHdMstt4zLwgEAk1sosL4Jxwc0NDToscce0+c//3nNnDlTmzdv1uc//3lJ0ttvv63LLrtM27dv16c+9SmnfqlUSslkUi0XL3B+HqNkeI6B54BO29zUezyfA7KejJP1OaCKadNM9TNnf9K59vjhI6beOcN7GQUF92MpaVyfA8qHx6+39cei5TmgSuPPoFNHDzrXDqSOm3pbngPKZ233H0kaGBhQXV3dGb9+1s8BFYtFPf3000qn02pvb9euXbuUz+e1fPny0ZoFCxZo9uzZ2r59+xn7ZLNZpVKpMTcAwNRnHkC/+tWvVFNTo0QiobvvvltbtmzR5Zdfrp6eHsXjcdXX14+pb2pqUk9Pzxn7dXZ2KplMjt7a2trMGwEAmHzMA2j+/PnavXu3Xn/9dd1zzz1as2aN3nrrrbNewPr16zUwMDB6O3To0Fn3AgBMHubXAcXjcc2bN0+StGTJEv3nf/6nvvvd7+rWW29VLpdTf3//mEdBvb29am5uPmO/RCKhRCJhXzkAYFI759cBlUolZbNZLVmyRLFYTFu3bh39Wnd3tw4ePKj29vZz/TYAgCnG9Aho/fr1WrVqlWbPnq3BwUFt3rxZ27Zt00svvaRkMqk77rhD69atU0NDg+rq6nTvvfeqvb3d+Qo4AMCFwzSAjh07pj/90z/V0aNHlUwmtWjRIr300kv6oz/6I0nSd77zHYXDYa1evVrZbFYrVqzQD37wg7NaWCSWcL6UOGa4LNhy6bMkBSH3emuqRchypafxctYgMESJFG2XVxYNl1VLUslw+XMhnzf1zuVyzrUjhsuqJak4MuxcWzBcyixJ1cZLwiuT093XkrPtw3zGfR9aYnusQtbehvOwaLxiOzC+IKDa8FKGdOqUqXcq1e9ebNzOcNgyAuyXYX+cc34dULm9/zqgWQsWOw8gy1CZUAPIUBs2rEMyZjyFbCeW5XVXEgPodKqnN5rqW+fNd64d7Dtp6p1Nu2/nRBpABU2cAVRXVelcOzI4YOp94qj7hVlBYLv/RAyvXcxmsqbe0ji+DggAgHPBAAIAeMEAAgB4wQACAHjBAAIAeMEAAgB4wQACAHjBAAIAeMEAAgB4YU7DHm/vBzOULPEwljAHY/BDEHJ/tfV4JiFYEhmk8U1CCIyxQJZjaTrukkqGd0S1vDOrJAWB4Z1cjeeVNU2iWHCPVhrPfWg7sWysSQglQxKC9Y1ZrUkIpnPcsr9lO7es5+F4B+F8XP8JF8Vz+PBh3pQOAKaAQ4cOadasWWf8+oQbQKVSSUeOHFFtbe2Y34hSqZTa2tp06NChj8wWmuzYzqnjQthGie2casqxnUEQaHBwUK2trR+Zvznh/gQXDoc/cmLW1dVN6YP/PrZz6rgQtlFiO6eac93OZDL5sTVchAAA8IIBBADwYtIMoEQioQcffFCJRML3UsYV2zl1XAjbKLGdU8353M4JdxECAODCMGkeAQEAphYGEADACwYQAMALBhAAwItJM4A2bNigT37yk6qoqNDSpUv1H//xH76XVFbf+ta3FAqFxtwWLFjge1nn5LXXXtONN96o1tZWhUIhPfvss2O+HgSBHnjgAbW0tKiyslLLly/X3r17/Sz2HHzcdt5+++0fOrYrV670s9iz1NnZqauuukq1tbVqbGzUzTffrO7u7jE1mUxGHR0dmj59umpqarR69Wr19vZ6WvHZcdnO66677kPH8+677/a04rOzceNGLVq0aPTFpu3t7frZz342+vXzdSwnxQD68Y9/rHXr1unBBx/UL3/5Sy1evFgrVqzQsWPHfC+trK644godPXp09PaLX/zC95LOSTqd1uLFi7Vhw4bTfv3RRx/V9773PT3xxBN6/fXXVV1drRUrViiTyZznlZ6bj9tOSVq5cuWYY/vUU0+dxxWeu66uLnV0dGjHjh16+eWXlc/ndcMNNyidTo/W3H///Xr++ef1zDPPqKurS0eOHNEtt9zicdV2LtspSXfeeeeY4/noo496WvHZmTVrlh555BHt2rVLO3fu1PXXX6+bbrpJv/71ryWdx2MZTAJXX3110NHRMfpxsVgMWltbg87OTo+rKq8HH3wwWLx4se9ljBtJwZYtW0Y/LpVKQXNzc/DYY4+Nfq6/vz9IJBLBU0895WGF5fHB7QyCIFizZk1w0003eVnPeDl27FggKejq6gqC4L1jF4vFgmeeeWa05je/+U0gKdi+fbuvZZ6zD25nEATBH/7hHwZ/8Rd/4W9R42TatGnB3//935/XYznhHwHlcjnt2rVLy5cvH/1cOBzW8uXLtX37do8rK7+9e/eqtbVVc+fO1Ze+9CUdPHjQ95LGzYEDB9TT0zPmuCaTSS1dunTKHVdJ2rZtmxobGzV//nzdc8896uvr872kczIwMCBJamhokCTt2rVL+Xx+zPFcsGCBZs+ePamP5we3830/+tGPNGPGDC1cuFDr16/X8PCwj+WVRbFY1NNPP610Oq329vbzeiwnXBjpB504cULFYlFNTU1jPt/U1KS3337b06rKb+nSpdq0aZPmz5+vo0eP6qGHHtJnPvMZvfnmm6qtrfW9vLLr6emRpNMe1/e/NlWsXLlSt9xyi+bMmaP9+/frr//6r7Vq1Spt375dkUjE9/LMSqWS7rvvPl1zzTVauHChpPeOZzweV319/ZjayXw8T7edkvTFL35RF110kVpbW7Vnzx597WtfU3d3t3760596XK3dr371K7W3tyuTyaimpkZbtmzR5Zdfrt27d5+3YznhB9CFYtWqVaP/XrRokZYuXaqLLrpIP/nJT3THHXd4XBnO1W233Tb67yuvvFKLFi3SxRdfrG3btmnZsmUeV3Z2Ojo69Oabb0765yg/zpm286677hr995VXXqmWlhYtW7ZM+/fv18UXX3y+l3nW5s+fr927d2tgYED//M//rDVr1qirq+u8rmHC/wluxowZikQiH7oCo7e3V83NzZ5WNf7q6+t16aWXat++fb6XMi7eP3YX2nGVpLlz52rGjBmT8tiuXbtWL7zwgn7+85+PeduU5uZm5XI59ff3j6mfrMfzTNt5OkuXLpWkSXc84/G45s2bpyVLlqizs1OLFy/Wd7/73fN6LCf8AIrH41qyZIm2bt06+rlSqaStW7eqvb3d48rG19DQkPbv36+WlhbfSxkXc+bMUXNz85jjmkql9Prrr0/p4yq9966/fX19k+rYBkGgtWvXasuWLXr11Vc1Z86cMV9fsmSJYrHYmOPZ3d2tgwcPTqrj+XHbeTq7d++WpEl1PE+nVCopm82e32NZ1ksaxsnTTz8dJBKJYNOmTcFbb70V3HXXXUF9fX3Q09Pje2ll85d/+ZfBtm3bggMHDgT/9m//FixfvjyYMWNGcOzYMd9LO2uDg4PBG2+8EbzxxhuBpODb3/528MYbbwTvvPNOEARB8MgjjwT19fXBc889F+zZsye46aabgjlz5gQjIyOeV27zUds5ODgYfOUrXwm2b98eHDhwIHjllVeC3//93w8uueSSIJPJ+F66s3vuuSdIJpPBtm3bgqNHj47ehoeHR2vuvvvuYPbs2cGrr74a7Ny5M2hvbw/a29s9rtru47Zz3759wcMPPxzs3LkzOHDgQPDcc88Fc+fODa699lrPK7f5+te/HnR1dQUHDhwI9uzZE3z9618PQqFQ8K//+q9BEJy/YzkpBlAQBMH3v//9YPbs2UE8Hg+uvvrqYMeOHb6XVFa33npr0NLSEsTj8eATn/hEcOuttwb79u3zvaxz8vOf/zyQ9KHbmjVrgiB471Lsb37zm0FTU1OQSCSCZcuWBd3d3X4XfRY+ajuHh4eDG264IZg5c2YQi8WCiy66KLjzzjsn3S9Pp9s+ScGTTz45WjMyMhL8+Z//eTBt2rSgqqoq+NznPhccPXrU36LPwsdt58GDB4Nrr702aGhoCBKJRDBv3rzgr/7qr4KBgQG/Czf6sz/7s+Ciiy4K4vF4MHPmzGDZsmWjwycIzt+x5O0YAABeTPjngAAAUxMDCADgBQMIAOAFAwgA4AUDCADgBQMIAOAFAwgA4AUDCADgBQMIAOAFAwgA4AUDCADgBQMIAODF/wcFIUcUPcJyQgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 搭建卷积神经网络模型\n",
        "# 三个卷积层\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "            # 32*32*3\n",
        "        self.conv1 = nn.Sequential(\n",
        "            # 卷积层计算\n",
        "            nn.Conv2d(3, 5, kernel_size=5, stride=1, padding=2),\n",
        "            #  批归一化\n",
        "            nn.BatchNorm2d(5),\n",
        "            #ReLU激活函数\n",
        "            nn.ReLU(),\n",
        "            # 池化层：最大池化\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1))\n",
        "            # 31*31*5\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(5, 8, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1))# 搭建卷积神经网络模型\n",
        "            # 30*30*8\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(8, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))# 搭建卷积神经网络模型\n",
        "            # 15*15*16\n",
        "\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(16, 24, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1))# 搭建卷积神经网络模型\n",
        "            # 14*14*24\n",
        "\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(24, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))# 搭建卷积神经网络模型\n",
        "            # 7*7*32\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(7*7*32, 120),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84, num_classes),\n",
        "        )\n",
        "\n",
        "    # 定义前向传播顺序\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.conv5(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "5aS7E2BBkpLo"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 实例化一个模型\n",
        "model = ConvNet(num_classes)"
      ],
      "metadata": {
        "id": "VK07kYOHk2Br"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义损失函数和优化器\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "2qUHkTdhksvr"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置cuda-gpu\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTuV8mucco2z",
        "outputId": "25b0bea5-0df2-4cc2-f25d-e027cbf2f271"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Thu Jul  6 12:16:50 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   74C    P0    29W /  70W |   1131MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 开始训练\n",
        "model = model.cuda()\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        # 前向传播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 反向传播和优化\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbRva835lY5b",
        "outputId": "0645dd04-6d7e-4aee-b958-6ca9420c3e2f"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [100/10000], Loss: 1.9248\n",
            "Epoch [1/20], Step [200/10000], Loss: 2.0185\n",
            "Epoch [1/20], Step [300/10000], Loss: 2.0043\n",
            "Epoch [1/20], Step [400/10000], Loss: 2.1152\n",
            "Epoch [1/20], Step [500/10000], Loss: 1.6891\n",
            "Epoch [1/20], Step [600/10000], Loss: 2.3145\n",
            "Epoch [1/20], Step [700/10000], Loss: 1.9730\n",
            "Epoch [1/20], Step [800/10000], Loss: 1.9035\n",
            "Epoch [1/20], Step [900/10000], Loss: 2.0206\n",
            "Epoch [1/20], Step [1000/10000], Loss: 2.4205\n",
            "Epoch [1/20], Step [1100/10000], Loss: 2.5579\n",
            "Epoch [1/20], Step [1200/10000], Loss: 1.7717\n",
            "Epoch [1/20], Step [1300/10000], Loss: 1.8270\n",
            "Epoch [1/20], Step [1400/10000], Loss: 2.5746\n",
            "Epoch [1/20], Step [1500/10000], Loss: 1.7352\n",
            "Epoch [1/20], Step [1600/10000], Loss: 1.8404\n",
            "Epoch [1/20], Step [1700/10000], Loss: 1.9446\n",
            "Epoch [1/20], Step [1800/10000], Loss: 2.2221\n",
            "Epoch [1/20], Step [1900/10000], Loss: 3.6812\n",
            "Epoch [1/20], Step [2000/10000], Loss: 2.0008\n",
            "Epoch [1/20], Step [2100/10000], Loss: 1.9430\n",
            "Epoch [1/20], Step [2200/10000], Loss: 2.3748\n",
            "Epoch [1/20], Step [2300/10000], Loss: 1.3062\n",
            "Epoch [1/20], Step [2400/10000], Loss: 2.4288\n",
            "Epoch [1/20], Step [2500/10000], Loss: 2.0168\n",
            "Epoch [1/20], Step [2600/10000], Loss: 1.7918\n",
            "Epoch [1/20], Step [2700/10000], Loss: 1.5981\n",
            "Epoch [1/20], Step [2800/10000], Loss: 1.8620\n",
            "Epoch [1/20], Step [2900/10000], Loss: 2.1515\n",
            "Epoch [1/20], Step [3000/10000], Loss: 1.9373\n",
            "Epoch [1/20], Step [3100/10000], Loss: 2.0426\n",
            "Epoch [1/20], Step [3200/10000], Loss: 2.0220\n",
            "Epoch [1/20], Step [3300/10000], Loss: 1.7371\n",
            "Epoch [1/20], Step [3400/10000], Loss: 1.8136\n",
            "Epoch [1/20], Step [3500/10000], Loss: 2.0134\n",
            "Epoch [1/20], Step [3600/10000], Loss: 1.8566\n",
            "Epoch [1/20], Step [3700/10000], Loss: 1.8490\n",
            "Epoch [1/20], Step [3800/10000], Loss: 2.7829\n",
            "Epoch [1/20], Step [3900/10000], Loss: 1.7837\n",
            "Epoch [1/20], Step [4000/10000], Loss: 2.2446\n",
            "Epoch [1/20], Step [4100/10000], Loss: 1.9472\n",
            "Epoch [1/20], Step [4200/10000], Loss: 1.5980\n",
            "Epoch [1/20], Step [4300/10000], Loss: 1.6852\n",
            "Epoch [1/20], Step [4400/10000], Loss: 2.5782\n",
            "Epoch [1/20], Step [4500/10000], Loss: 2.2040\n",
            "Epoch [1/20], Step [4600/10000], Loss: 1.8583\n",
            "Epoch [1/20], Step [4700/10000], Loss: 1.3579\n",
            "Epoch [1/20], Step [4800/10000], Loss: 1.9960\n",
            "Epoch [1/20], Step [4900/10000], Loss: 2.1296\n",
            "Epoch [1/20], Step [5000/10000], Loss: 1.8274\n",
            "Epoch [1/20], Step [5100/10000], Loss: 1.7970\n",
            "Epoch [1/20], Step [5200/10000], Loss: 2.0458\n",
            "Epoch [1/20], Step [5300/10000], Loss: 2.5203\n",
            "Epoch [1/20], Step [5400/10000], Loss: 2.2503\n",
            "Epoch [1/20], Step [5500/10000], Loss: 2.2459\n",
            "Epoch [1/20], Step [5600/10000], Loss: 1.5005\n",
            "Epoch [1/20], Step [5700/10000], Loss: 2.1851\n",
            "Epoch [1/20], Step [5800/10000], Loss: 2.7417\n",
            "Epoch [1/20], Step [5900/10000], Loss: 2.3769\n",
            "Epoch [1/20], Step [6000/10000], Loss: 1.6272\n",
            "Epoch [1/20], Step [6100/10000], Loss: 1.9208\n",
            "Epoch [1/20], Step [6200/10000], Loss: 2.2147\n",
            "Epoch [1/20], Step [6300/10000], Loss: 1.9123\n",
            "Epoch [1/20], Step [6400/10000], Loss: 1.7808\n",
            "Epoch [1/20], Step [6500/10000], Loss: 2.2454\n",
            "Epoch [1/20], Step [6600/10000], Loss: 2.1592\n",
            "Epoch [1/20], Step [6700/10000], Loss: 1.4617\n",
            "Epoch [1/20], Step [6800/10000], Loss: 1.7941\n",
            "Epoch [1/20], Step [6900/10000], Loss: 2.2789\n",
            "Epoch [1/20], Step [7000/10000], Loss: 2.2802\n",
            "Epoch [1/20], Step [7100/10000], Loss: 1.8210\n",
            "Epoch [1/20], Step [7200/10000], Loss: 1.3941\n",
            "Epoch [1/20], Step [7300/10000], Loss: 2.2695\n",
            "Epoch [1/20], Step [7400/10000], Loss: 1.3925\n",
            "Epoch [1/20], Step [7500/10000], Loss: 1.4500\n",
            "Epoch [1/20], Step [7600/10000], Loss: 1.5536\n",
            "Epoch [1/20], Step [7700/10000], Loss: 1.7165\n",
            "Epoch [1/20], Step [7800/10000], Loss: 1.5179\n",
            "Epoch [1/20], Step [7900/10000], Loss: 1.9496\n",
            "Epoch [1/20], Step [8000/10000], Loss: 2.7443\n",
            "Epoch [1/20], Step [8100/10000], Loss: 1.4736\n",
            "Epoch [1/20], Step [8200/10000], Loss: 1.9688\n",
            "Epoch [1/20], Step [8300/10000], Loss: 1.9132\n",
            "Epoch [1/20], Step [8400/10000], Loss: 1.3413\n",
            "Epoch [1/20], Step [8500/10000], Loss: 1.2595\n",
            "Epoch [1/20], Step [8600/10000], Loss: 2.1793\n",
            "Epoch [1/20], Step [8700/10000], Loss: 2.0609\n",
            "Epoch [1/20], Step [8800/10000], Loss: 1.7968\n",
            "Epoch [1/20], Step [8900/10000], Loss: 1.9294\n",
            "Epoch [1/20], Step [9000/10000], Loss: 1.5950\n",
            "Epoch [1/20], Step [9100/10000], Loss: 1.3057\n",
            "Epoch [1/20], Step [9200/10000], Loss: 1.8027\n",
            "Epoch [1/20], Step [9300/10000], Loss: 1.6329\n",
            "Epoch [1/20], Step [9400/10000], Loss: 2.9234\n",
            "Epoch [1/20], Step [9500/10000], Loss: 1.4189\n",
            "Epoch [1/20], Step [9600/10000], Loss: 1.7115\n",
            "Epoch [1/20], Step [9700/10000], Loss: 1.4852\n",
            "Epoch [1/20], Step [9800/10000], Loss: 2.3198\n",
            "Epoch [1/20], Step [9900/10000], Loss: 1.5792\n",
            "Epoch [1/20], Step [10000/10000], Loss: 1.5436\n",
            "Epoch [2/20], Step [100/10000], Loss: 1.4100\n",
            "Epoch [2/20], Step [200/10000], Loss: 1.5594\n",
            "Epoch [2/20], Step [300/10000], Loss: 1.9417\n",
            "Epoch [2/20], Step [400/10000], Loss: 1.2730\n",
            "Epoch [2/20], Step [500/10000], Loss: 2.0430\n",
            "Epoch [2/20], Step [600/10000], Loss: 1.8319\n",
            "Epoch [2/20], Step [700/10000], Loss: 0.9534\n",
            "Epoch [2/20], Step [800/10000], Loss: 1.9753\n",
            "Epoch [2/20], Step [900/10000], Loss: 1.6364\n",
            "Epoch [2/20], Step [1000/10000], Loss: 1.2085\n",
            "Epoch [2/20], Step [1100/10000], Loss: 1.1562\n",
            "Epoch [2/20], Step [1200/10000], Loss: 2.5817\n",
            "Epoch [2/20], Step [1300/10000], Loss: 1.7142\n",
            "Epoch [2/20], Step [1400/10000], Loss: 1.4352\n",
            "Epoch [2/20], Step [1500/10000], Loss: 3.1603\n",
            "Epoch [2/20], Step [1600/10000], Loss: 1.9721\n",
            "Epoch [2/20], Step [1700/10000], Loss: 1.5523\n",
            "Epoch [2/20], Step [1800/10000], Loss: 1.4703\n",
            "Epoch [2/20], Step [1900/10000], Loss: 1.7878\n",
            "Epoch [2/20], Step [2000/10000], Loss: 2.0848\n",
            "Epoch [2/20], Step [2100/10000], Loss: 2.0666\n",
            "Epoch [2/20], Step [2200/10000], Loss: 1.8568\n",
            "Epoch [2/20], Step [2300/10000], Loss: 1.2279\n",
            "Epoch [2/20], Step [2400/10000], Loss: 1.5433\n",
            "Epoch [2/20], Step [2500/10000], Loss: 2.2246\n",
            "Epoch [2/20], Step [2600/10000], Loss: 1.4578\n",
            "Epoch [2/20], Step [2700/10000], Loss: 1.7961\n",
            "Epoch [2/20], Step [2800/10000], Loss: 1.5111\n",
            "Epoch [2/20], Step [2900/10000], Loss: 2.3273\n",
            "Epoch [2/20], Step [3000/10000], Loss: 2.1806\n",
            "Epoch [2/20], Step [3100/10000], Loss: 2.0649\n",
            "Epoch [2/20], Step [3200/10000], Loss: 1.1463\n",
            "Epoch [2/20], Step [3300/10000], Loss: 1.0845\n",
            "Epoch [2/20], Step [3400/10000], Loss: 1.4982\n",
            "Epoch [2/20], Step [3500/10000], Loss: 1.7807\n",
            "Epoch [2/20], Step [3600/10000], Loss: 2.0601\n",
            "Epoch [2/20], Step [3700/10000], Loss: 1.4542\n",
            "Epoch [2/20], Step [3800/10000], Loss: 1.4457\n",
            "Epoch [2/20], Step [3900/10000], Loss: 1.4227\n",
            "Epoch [2/20], Step [4000/10000], Loss: 1.7251\n",
            "Epoch [2/20], Step [4100/10000], Loss: 1.5221\n",
            "Epoch [2/20], Step [4200/10000], Loss: 1.8886\n",
            "Epoch [2/20], Step [4300/10000], Loss: 1.9772\n",
            "Epoch [2/20], Step [4400/10000], Loss: 2.5400\n",
            "Epoch [2/20], Step [4500/10000], Loss: 1.7712\n",
            "Epoch [2/20], Step [4600/10000], Loss: 1.4551\n",
            "Epoch [2/20], Step [4700/10000], Loss: 2.4341\n",
            "Epoch [2/20], Step [4800/10000], Loss: 2.0274\n",
            "Epoch [2/20], Step [4900/10000], Loss: 2.1662\n",
            "Epoch [2/20], Step [5000/10000], Loss: 1.7325\n",
            "Epoch [2/20], Step [5100/10000], Loss: 1.3015\n",
            "Epoch [2/20], Step [5200/10000], Loss: 1.5619\n",
            "Epoch [2/20], Step [5300/10000], Loss: 1.6572\n",
            "Epoch [2/20], Step [5400/10000], Loss: 1.6210\n",
            "Epoch [2/20], Step [5500/10000], Loss: 1.7518\n",
            "Epoch [2/20], Step [5600/10000], Loss: 1.4764\n",
            "Epoch [2/20], Step [5700/10000], Loss: 1.9247\n",
            "Epoch [2/20], Step [5800/10000], Loss: 1.7104\n",
            "Epoch [2/20], Step [5900/10000], Loss: 1.8264\n",
            "Epoch [2/20], Step [6000/10000], Loss: 1.6109\n",
            "Epoch [2/20], Step [6100/10000], Loss: 1.7682\n",
            "Epoch [2/20], Step [6200/10000], Loss: 1.4882\n",
            "Epoch [2/20], Step [6300/10000], Loss: 1.8712\n",
            "Epoch [2/20], Step [6400/10000], Loss: 1.3868\n",
            "Epoch [2/20], Step [6500/10000], Loss: 1.4082\n",
            "Epoch [2/20], Step [6600/10000], Loss: 1.4787\n",
            "Epoch [2/20], Step [6700/10000], Loss: 1.7082\n",
            "Epoch [2/20], Step [6800/10000], Loss: 1.3427\n",
            "Epoch [2/20], Step [6900/10000], Loss: 1.9739\n",
            "Epoch [2/20], Step [7000/10000], Loss: 2.0171\n",
            "Epoch [2/20], Step [7100/10000], Loss: 1.8311\n",
            "Epoch [2/20], Step [7200/10000], Loss: 1.7666\n",
            "Epoch [2/20], Step [7300/10000], Loss: 1.6227\n",
            "Epoch [2/20], Step [7400/10000], Loss: 1.1437\n",
            "Epoch [2/20], Step [7500/10000], Loss: 1.8039\n",
            "Epoch [2/20], Step [7600/10000], Loss: 1.3519\n",
            "Epoch [2/20], Step [7700/10000], Loss: 1.5611\n",
            "Epoch [2/20], Step [7800/10000], Loss: 2.4679\n",
            "Epoch [2/20], Step [7900/10000], Loss: 1.8180\n",
            "Epoch [2/20], Step [8000/10000], Loss: 1.1264\n",
            "Epoch [2/20], Step [8100/10000], Loss: 1.9486\n",
            "Epoch [2/20], Step [8200/10000], Loss: 2.4764\n",
            "Epoch [2/20], Step [8300/10000], Loss: 1.4304\n",
            "Epoch [2/20], Step [8400/10000], Loss: 2.0998\n",
            "Epoch [2/20], Step [8500/10000], Loss: 1.6885\n",
            "Epoch [2/20], Step [8600/10000], Loss: 1.6136\n",
            "Epoch [2/20], Step [8700/10000], Loss: 2.0289\n",
            "Epoch [2/20], Step [8800/10000], Loss: 2.2027\n",
            "Epoch [2/20], Step [8900/10000], Loss: 1.5665\n",
            "Epoch [2/20], Step [9000/10000], Loss: 2.1919\n",
            "Epoch [2/20], Step [9100/10000], Loss: 1.7942\n",
            "Epoch [2/20], Step [9200/10000], Loss: 2.0654\n",
            "Epoch [2/20], Step [9300/10000], Loss: 2.1534\n",
            "Epoch [2/20], Step [9400/10000], Loss: 2.0636\n",
            "Epoch [2/20], Step [9500/10000], Loss: 1.4456\n",
            "Epoch [2/20], Step [9600/10000], Loss: 1.4620\n",
            "Epoch [2/20], Step [9700/10000], Loss: 0.8992\n",
            "Epoch [2/20], Step [9800/10000], Loss: 0.9402\n",
            "Epoch [2/20], Step [9900/10000], Loss: 1.4007\n",
            "Epoch [2/20], Step [10000/10000], Loss: 1.2297\n",
            "Epoch [3/20], Step [100/10000], Loss: 1.9254\n",
            "Epoch [3/20], Step [200/10000], Loss: 1.1310\n",
            "Epoch [3/20], Step [300/10000], Loss: 1.0016\n",
            "Epoch [3/20], Step [400/10000], Loss: 2.1883\n",
            "Epoch [3/20], Step [500/10000], Loss: 1.3623\n",
            "Epoch [3/20], Step [600/10000], Loss: 2.1607\n",
            "Epoch [3/20], Step [700/10000], Loss: 0.9648\n",
            "Epoch [3/20], Step [800/10000], Loss: 1.4011\n",
            "Epoch [3/20], Step [900/10000], Loss: 0.8457\n",
            "Epoch [3/20], Step [1000/10000], Loss: 1.5965\n",
            "Epoch [3/20], Step [1100/10000], Loss: 1.4492\n",
            "Epoch [3/20], Step [1200/10000], Loss: 1.7510\n",
            "Epoch [3/20], Step [1300/10000], Loss: 1.1238\n",
            "Epoch [3/20], Step [1400/10000], Loss: 1.5167\n",
            "Epoch [3/20], Step [1500/10000], Loss: 1.1832\n",
            "Epoch [3/20], Step [1600/10000], Loss: 0.9921\n",
            "Epoch [3/20], Step [1700/10000], Loss: 1.5363\n",
            "Epoch [3/20], Step [1800/10000], Loss: 1.5259\n",
            "Epoch [3/20], Step [1900/10000], Loss: 2.1832\n",
            "Epoch [3/20], Step [2000/10000], Loss: 1.4514\n",
            "Epoch [3/20], Step [2100/10000], Loss: 0.9802\n",
            "Epoch [3/20], Step [2200/10000], Loss: 1.9858\n",
            "Epoch [3/20], Step [2300/10000], Loss: 1.5941\n",
            "Epoch [3/20], Step [2400/10000], Loss: 1.6507\n",
            "Epoch [3/20], Step [2500/10000], Loss: 1.5606\n",
            "Epoch [3/20], Step [2600/10000], Loss: 1.5660\n",
            "Epoch [3/20], Step [2700/10000], Loss: 2.1161\n",
            "Epoch [3/20], Step [2800/10000], Loss: 0.9084\n",
            "Epoch [3/20], Step [2900/10000], Loss: 1.0961\n",
            "Epoch [3/20], Step [3000/10000], Loss: 1.4647\n",
            "Epoch [3/20], Step [3100/10000], Loss: 2.8256\n",
            "Epoch [3/20], Step [3200/10000], Loss: 1.3300\n",
            "Epoch [3/20], Step [3300/10000], Loss: 1.4554\n",
            "Epoch [3/20], Step [3400/10000], Loss: 1.3446\n",
            "Epoch [3/20], Step [3500/10000], Loss: 1.5325\n",
            "Epoch [3/20], Step [3600/10000], Loss: 2.0837\n",
            "Epoch [3/20], Step [3700/10000], Loss: 1.0815\n",
            "Epoch [3/20], Step [3800/10000], Loss: 1.4369\n",
            "Epoch [3/20], Step [3900/10000], Loss: 1.4006\n",
            "Epoch [3/20], Step [4000/10000], Loss: 0.9869\n",
            "Epoch [3/20], Step [4100/10000], Loss: 1.2788\n",
            "Epoch [3/20], Step [4200/10000], Loss: 1.8898\n",
            "Epoch [3/20], Step [4300/10000], Loss: 1.6400\n",
            "Epoch [3/20], Step [4400/10000], Loss: 1.4019\n",
            "Epoch [3/20], Step [4500/10000], Loss: 1.6813\n",
            "Epoch [3/20], Step [4600/10000], Loss: 1.7262\n",
            "Epoch [3/20], Step [4700/10000], Loss: 1.5283\n",
            "Epoch [3/20], Step [4800/10000], Loss: 1.7488\n",
            "Epoch [3/20], Step [4900/10000], Loss: 1.9727\n",
            "Epoch [3/20], Step [5000/10000], Loss: 1.5255\n",
            "Epoch [3/20], Step [5100/10000], Loss: 1.1610\n",
            "Epoch [3/20], Step [5200/10000], Loss: 1.3679\n",
            "Epoch [3/20], Step [5300/10000], Loss: 0.9703\n",
            "Epoch [3/20], Step [5400/10000], Loss: 1.5284\n",
            "Epoch [3/20], Step [5500/10000], Loss: 2.4618\n",
            "Epoch [3/20], Step [5600/10000], Loss: 2.0621\n",
            "Epoch [3/20], Step [5700/10000], Loss: 1.3322\n",
            "Epoch [3/20], Step [5800/10000], Loss: 2.5492\n",
            "Epoch [3/20], Step [5900/10000], Loss: 1.2268\n",
            "Epoch [3/20], Step [6000/10000], Loss: 1.9568\n",
            "Epoch [3/20], Step [6100/10000], Loss: 1.0371\n",
            "Epoch [3/20], Step [6200/10000], Loss: 2.6473\n",
            "Epoch [3/20], Step [6300/10000], Loss: 1.2279\n",
            "Epoch [3/20], Step [6400/10000], Loss: 1.5051\n",
            "Epoch [3/20], Step [6500/10000], Loss: 0.8646\n",
            "Epoch [3/20], Step [6600/10000], Loss: 1.5386\n",
            "Epoch [3/20], Step [6700/10000], Loss: 1.4640\n",
            "Epoch [3/20], Step [6800/10000], Loss: 1.4602\n",
            "Epoch [3/20], Step [6900/10000], Loss: 1.5843\n",
            "Epoch [3/20], Step [7000/10000], Loss: 1.8279\n",
            "Epoch [3/20], Step [7100/10000], Loss: 1.8780\n",
            "Epoch [3/20], Step [7200/10000], Loss: 1.6528\n",
            "Epoch [3/20], Step [7300/10000], Loss: 1.2280\n",
            "Epoch [3/20], Step [7400/10000], Loss: 1.4821\n",
            "Epoch [3/20], Step [7500/10000], Loss: 1.4457\n",
            "Epoch [3/20], Step [7600/10000], Loss: 1.4951\n",
            "Epoch [3/20], Step [7700/10000], Loss: 2.0004\n",
            "Epoch [3/20], Step [7800/10000], Loss: 1.6026\n",
            "Epoch [3/20], Step [7900/10000], Loss: 1.9769\n",
            "Epoch [3/20], Step [8000/10000], Loss: 1.5879\n",
            "Epoch [3/20], Step [8100/10000], Loss: 0.9571\n",
            "Epoch [3/20], Step [8200/10000], Loss: 2.2219\n",
            "Epoch [3/20], Step [8300/10000], Loss: 1.8491\n",
            "Epoch [3/20], Step [8400/10000], Loss: 0.8343\n",
            "Epoch [3/20], Step [8500/10000], Loss: 1.1621\n",
            "Epoch [3/20], Step [8600/10000], Loss: 1.6523\n",
            "Epoch [3/20], Step [8700/10000], Loss: 1.0251\n",
            "Epoch [3/20], Step [8800/10000], Loss: 2.3084\n",
            "Epoch [3/20], Step [8900/10000], Loss: 2.0269\n",
            "Epoch [3/20], Step [9000/10000], Loss: 2.7909\n",
            "Epoch [3/20], Step [9100/10000], Loss: 1.3616\n",
            "Epoch [3/20], Step [9200/10000], Loss: 1.3961\n",
            "Epoch [3/20], Step [9300/10000], Loss: 1.0984\n",
            "Epoch [3/20], Step [9400/10000], Loss: 1.9330\n",
            "Epoch [3/20], Step [9500/10000], Loss: 1.2153\n",
            "Epoch [3/20], Step [9600/10000], Loss: 1.6987\n",
            "Epoch [3/20], Step [9700/10000], Loss: 0.9289\n",
            "Epoch [3/20], Step [9800/10000], Loss: 0.9665\n",
            "Epoch [3/20], Step [9900/10000], Loss: 1.6552\n",
            "Epoch [3/20], Step [10000/10000], Loss: 1.5648\n",
            "Epoch [4/20], Step [100/10000], Loss: 1.5727\n",
            "Epoch [4/20], Step [200/10000], Loss: 0.6880\n",
            "Epoch [4/20], Step [300/10000], Loss: 1.7650\n",
            "Epoch [4/20], Step [400/10000], Loss: 1.8160\n",
            "Epoch [4/20], Step [500/10000], Loss: 1.7551\n",
            "Epoch [4/20], Step [600/10000], Loss: 0.8866\n",
            "Epoch [4/20], Step [700/10000], Loss: 1.0437\n",
            "Epoch [4/20], Step [800/10000], Loss: 1.5250\n",
            "Epoch [4/20], Step [900/10000], Loss: 2.0560\n",
            "Epoch [4/20], Step [1000/10000], Loss: 1.7989\n",
            "Epoch [4/20], Step [1100/10000], Loss: 0.8662\n",
            "Epoch [4/20], Step [1200/10000], Loss: 1.0930\n",
            "Epoch [4/20], Step [1300/10000], Loss: 2.3683\n",
            "Epoch [4/20], Step [1400/10000], Loss: 1.0108\n",
            "Epoch [4/20], Step [1500/10000], Loss: 1.3267\n",
            "Epoch [4/20], Step [1600/10000], Loss: 1.1378\n",
            "Epoch [4/20], Step [1700/10000], Loss: 1.6118\n",
            "Epoch [4/20], Step [1800/10000], Loss: 2.1140\n",
            "Epoch [4/20], Step [1900/10000], Loss: 0.9321\n",
            "Epoch [4/20], Step [2000/10000], Loss: 2.0770\n",
            "Epoch [4/20], Step [2100/10000], Loss: 2.2796\n",
            "Epoch [4/20], Step [2200/10000], Loss: 1.4865\n",
            "Epoch [4/20], Step [2300/10000], Loss: 1.4255\n",
            "Epoch [4/20], Step [2400/10000], Loss: 1.9653\n",
            "Epoch [4/20], Step [2500/10000], Loss: 1.5613\n",
            "Epoch [4/20], Step [2600/10000], Loss: 2.9561\n",
            "Epoch [4/20], Step [2700/10000], Loss: 1.2545\n",
            "Epoch [4/20], Step [2800/10000], Loss: 0.7058\n",
            "Epoch [4/20], Step [2900/10000], Loss: 1.3865\n",
            "Epoch [4/20], Step [3000/10000], Loss: 1.2762\n",
            "Epoch [4/20], Step [3100/10000], Loss: 1.3905\n",
            "Epoch [4/20], Step [3200/10000], Loss: 1.0803\n",
            "Epoch [4/20], Step [3300/10000], Loss: 1.4443\n",
            "Epoch [4/20], Step [3400/10000], Loss: 1.3457\n",
            "Epoch [4/20], Step [3500/10000], Loss: 1.3965\n",
            "Epoch [4/20], Step [3600/10000], Loss: 1.2845\n",
            "Epoch [4/20], Step [3700/10000], Loss: 0.8828\n",
            "Epoch [4/20], Step [3800/10000], Loss: 2.1354\n",
            "Epoch [4/20], Step [3900/10000], Loss: 1.3161\n",
            "Epoch [4/20], Step [4000/10000], Loss: 1.7875\n",
            "Epoch [4/20], Step [4100/10000], Loss: 1.4461\n",
            "Epoch [4/20], Step [4200/10000], Loss: 1.7291\n",
            "Epoch [4/20], Step [4300/10000], Loss: 1.4971\n",
            "Epoch [4/20], Step [4400/10000], Loss: 0.9179\n",
            "Epoch [4/20], Step [4500/10000], Loss: 1.2713\n",
            "Epoch [4/20], Step [4600/10000], Loss: 1.0060\n",
            "Epoch [4/20], Step [4700/10000], Loss: 1.7592\n",
            "Epoch [4/20], Step [4800/10000], Loss: 2.4001\n",
            "Epoch [4/20], Step [4900/10000], Loss: 1.8472\n",
            "Epoch [4/20], Step [5000/10000], Loss: 1.0263\n",
            "Epoch [4/20], Step [5100/10000], Loss: 0.9846\n",
            "Epoch [4/20], Step [5200/10000], Loss: 1.6439\n",
            "Epoch [4/20], Step [5300/10000], Loss: 0.8925\n",
            "Epoch [4/20], Step [5400/10000], Loss: 1.4526\n",
            "Epoch [4/20], Step [5500/10000], Loss: 1.5359\n",
            "Epoch [4/20], Step [5600/10000], Loss: 1.8775\n",
            "Epoch [4/20], Step [5700/10000], Loss: 1.3282\n",
            "Epoch [4/20], Step [5800/10000], Loss: 0.9639\n",
            "Epoch [4/20], Step [5900/10000], Loss: 0.9577\n",
            "Epoch [4/20], Step [6000/10000], Loss: 1.1339\n",
            "Epoch [4/20], Step [6100/10000], Loss: 1.3976\n",
            "Epoch [4/20], Step [6200/10000], Loss: 1.6011\n",
            "Epoch [4/20], Step [6300/10000], Loss: 1.1946\n",
            "Epoch [4/20], Step [6400/10000], Loss: 1.2980\n",
            "Epoch [4/20], Step [6500/10000], Loss: 0.7715\n",
            "Epoch [4/20], Step [6600/10000], Loss: 1.0969\n",
            "Epoch [4/20], Step [6700/10000], Loss: 1.5672\n",
            "Epoch [4/20], Step [6800/10000], Loss: 1.5915\n",
            "Epoch [4/20], Step [6900/10000], Loss: 1.7616\n",
            "Epoch [4/20], Step [7000/10000], Loss: 1.5316\n",
            "Epoch [4/20], Step [7100/10000], Loss: 1.3640\n",
            "Epoch [4/20], Step [7200/10000], Loss: 2.1803\n",
            "Epoch [4/20], Step [7300/10000], Loss: 1.7017\n",
            "Epoch [4/20], Step [7400/10000], Loss: 2.4726\n",
            "Epoch [4/20], Step [7500/10000], Loss: 1.2419\n",
            "Epoch [4/20], Step [7600/10000], Loss: 1.1817\n",
            "Epoch [4/20], Step [7700/10000], Loss: 1.4487\n",
            "Epoch [4/20], Step [7800/10000], Loss: 1.1541\n",
            "Epoch [4/20], Step [7900/10000], Loss: 0.7385\n",
            "Epoch [4/20], Step [8000/10000], Loss: 0.9032\n",
            "Epoch [4/20], Step [8100/10000], Loss: 1.6265\n",
            "Epoch [4/20], Step [8200/10000], Loss: 1.4271\n",
            "Epoch [4/20], Step [8300/10000], Loss: 1.8698\n",
            "Epoch [4/20], Step [8400/10000], Loss: 1.5412\n",
            "Epoch [4/20], Step [8500/10000], Loss: 1.2432\n",
            "Epoch [4/20], Step [8600/10000], Loss: 1.1721\n",
            "Epoch [4/20], Step [8700/10000], Loss: 1.9575\n",
            "Epoch [4/20], Step [8800/10000], Loss: 1.0985\n",
            "Epoch [4/20], Step [8900/10000], Loss: 0.8713\n",
            "Epoch [4/20], Step [9000/10000], Loss: 1.7996\n",
            "Epoch [4/20], Step [9100/10000], Loss: 0.8037\n",
            "Epoch [4/20], Step [9200/10000], Loss: 0.9837\n",
            "Epoch [4/20], Step [9300/10000], Loss: 0.9331\n",
            "Epoch [4/20], Step [9400/10000], Loss: 2.0268\n",
            "Epoch [4/20], Step [9500/10000], Loss: 1.0356\n",
            "Epoch [4/20], Step [9600/10000], Loss: 1.3530\n",
            "Epoch [4/20], Step [9700/10000], Loss: 1.2886\n",
            "Epoch [4/20], Step [9800/10000], Loss: 1.0200\n",
            "Epoch [4/20], Step [9900/10000], Loss: 0.7157\n",
            "Epoch [4/20], Step [10000/10000], Loss: 1.7398\n",
            "Epoch [5/20], Step [100/10000], Loss: 1.3618\n",
            "Epoch [5/20], Step [200/10000], Loss: 2.0243\n",
            "Epoch [5/20], Step [300/10000], Loss: 1.6208\n",
            "Epoch [5/20], Step [400/10000], Loss: 2.2185\n",
            "Epoch [5/20], Step [500/10000], Loss: 1.6310\n",
            "Epoch [5/20], Step [600/10000], Loss: 1.6282\n",
            "Epoch [5/20], Step [700/10000], Loss: 1.8697\n",
            "Epoch [5/20], Step [800/10000], Loss: 1.1206\n",
            "Epoch [5/20], Step [900/10000], Loss: 1.8205\n",
            "Epoch [5/20], Step [1000/10000], Loss: 0.8200\n",
            "Epoch [5/20], Step [1100/10000], Loss: 1.3998\n",
            "Epoch [5/20], Step [1200/10000], Loss: 0.8149\n",
            "Epoch [5/20], Step [1300/10000], Loss: 0.8706\n",
            "Epoch [5/20], Step [1400/10000], Loss: 1.5398\n",
            "Epoch [5/20], Step [1500/10000], Loss: 2.0014\n",
            "Epoch [5/20], Step [1600/10000], Loss: 1.7886\n",
            "Epoch [5/20], Step [1700/10000], Loss: 1.1211\n",
            "Epoch [5/20], Step [1800/10000], Loss: 1.7363\n",
            "Epoch [5/20], Step [1900/10000], Loss: 1.0378\n",
            "Epoch [5/20], Step [2000/10000], Loss: 1.3627\n",
            "Epoch [5/20], Step [2100/10000], Loss: 0.6981\n",
            "Epoch [5/20], Step [2200/10000], Loss: 1.7975\n",
            "Epoch [5/20], Step [2300/10000], Loss: 1.3296\n",
            "Epoch [5/20], Step [2400/10000], Loss: 1.2537\n",
            "Epoch [5/20], Step [2500/10000], Loss: 1.4488\n",
            "Epoch [5/20], Step [2600/10000], Loss: 1.6089\n",
            "Epoch [5/20], Step [2700/10000], Loss: 1.1385\n",
            "Epoch [5/20], Step [2800/10000], Loss: 1.3437\n",
            "Epoch [5/20], Step [2900/10000], Loss: 1.2045\n",
            "Epoch [5/20], Step [3000/10000], Loss: 1.9335\n",
            "Epoch [5/20], Step [3100/10000], Loss: 2.0212\n",
            "Epoch [5/20], Step [3200/10000], Loss: 1.4116\n",
            "Epoch [5/20], Step [3300/10000], Loss: 1.5329\n",
            "Epoch [5/20], Step [3400/10000], Loss: 1.2287\n",
            "Epoch [5/20], Step [3500/10000], Loss: 1.9206\n",
            "Epoch [5/20], Step [3600/10000], Loss: 0.9238\n",
            "Epoch [5/20], Step [3700/10000], Loss: 0.9083\n",
            "Epoch [5/20], Step [3800/10000], Loss: 2.8192\n",
            "Epoch [5/20], Step [3900/10000], Loss: 1.0580\n",
            "Epoch [5/20], Step [4000/10000], Loss: 1.9731\n",
            "Epoch [5/20], Step [4100/10000], Loss: 1.2448\n",
            "Epoch [5/20], Step [4200/10000], Loss: 1.2633\n",
            "Epoch [5/20], Step [4300/10000], Loss: 2.6614\n",
            "Epoch [5/20], Step [4400/10000], Loss: 1.6363\n",
            "Epoch [5/20], Step [4500/10000], Loss: 2.1985\n",
            "Epoch [5/20], Step [4600/10000], Loss: 2.4576\n",
            "Epoch [5/20], Step [4700/10000], Loss: 2.0076\n",
            "Epoch [5/20], Step [4800/10000], Loss: 2.6519\n",
            "Epoch [5/20], Step [4900/10000], Loss: 2.2630\n",
            "Epoch [5/20], Step [5000/10000], Loss: 3.1799\n",
            "Epoch [5/20], Step [5100/10000], Loss: 0.5679\n",
            "Epoch [5/20], Step [5200/10000], Loss: 1.4561\n",
            "Epoch [5/20], Step [5300/10000], Loss: 1.5731\n",
            "Epoch [5/20], Step [5400/10000], Loss: 1.1100\n",
            "Epoch [5/20], Step [5500/10000], Loss: 1.4132\n",
            "Epoch [5/20], Step [5600/10000], Loss: 1.6370\n",
            "Epoch [5/20], Step [5700/10000], Loss: 1.8321\n",
            "Epoch [5/20], Step [5800/10000], Loss: 1.9433\n",
            "Epoch [5/20], Step [5900/10000], Loss: 0.9953\n",
            "Epoch [5/20], Step [6000/10000], Loss: 0.8410\n",
            "Epoch [5/20], Step [6100/10000], Loss: 1.3548\n",
            "Epoch [5/20], Step [6200/10000], Loss: 0.9989\n",
            "Epoch [5/20], Step [6300/10000], Loss: 1.0570\n",
            "Epoch [5/20], Step [6400/10000], Loss: 0.7225\n",
            "Epoch [5/20], Step [6500/10000], Loss: 1.1728\n",
            "Epoch [5/20], Step [6600/10000], Loss: 1.8000\n",
            "Epoch [5/20], Step [6700/10000], Loss: 1.1680\n",
            "Epoch [5/20], Step [6800/10000], Loss: 1.2776\n",
            "Epoch [5/20], Step [6900/10000], Loss: 0.8694\n",
            "Epoch [5/20], Step [7000/10000], Loss: 1.4023\n",
            "Epoch [5/20], Step [7100/10000], Loss: 1.7378\n",
            "Epoch [5/20], Step [7200/10000], Loss: 1.2943\n",
            "Epoch [5/20], Step [7300/10000], Loss: 0.9075\n",
            "Epoch [5/20], Step [7400/10000], Loss: 1.2408\n",
            "Epoch [5/20], Step [7500/10000], Loss: 1.1978\n",
            "Epoch [5/20], Step [7600/10000], Loss: 0.7037\n",
            "Epoch [5/20], Step [7700/10000], Loss: 2.8909\n",
            "Epoch [5/20], Step [7800/10000], Loss: 1.7770\n",
            "Epoch [5/20], Step [7900/10000], Loss: 1.5359\n",
            "Epoch [5/20], Step [8000/10000], Loss: 1.3218\n",
            "Epoch [5/20], Step [8100/10000], Loss: 1.1030\n",
            "Epoch [5/20], Step [8200/10000], Loss: 1.0556\n",
            "Epoch [5/20], Step [8300/10000], Loss: 0.8450\n",
            "Epoch [5/20], Step [8400/10000], Loss: 0.7650\n",
            "Epoch [5/20], Step [8500/10000], Loss: 1.0493\n",
            "Epoch [5/20], Step [8600/10000], Loss: 2.0310\n",
            "Epoch [5/20], Step [8700/10000], Loss: 1.4123\n",
            "Epoch [5/20], Step [8800/10000], Loss: 2.1321\n",
            "Epoch [5/20], Step [8900/10000], Loss: 2.0968\n",
            "Epoch [5/20], Step [9000/10000], Loss: 1.9810\n",
            "Epoch [5/20], Step [9100/10000], Loss: 2.4733\n",
            "Epoch [5/20], Step [9200/10000], Loss: 1.7109\n",
            "Epoch [5/20], Step [9300/10000], Loss: 1.9156\n",
            "Epoch [5/20], Step [9400/10000], Loss: 1.7741\n",
            "Epoch [5/20], Step [9500/10000], Loss: 1.3300\n",
            "Epoch [5/20], Step [9600/10000], Loss: 2.1326\n",
            "Epoch [5/20], Step [9700/10000], Loss: 0.2617\n",
            "Epoch [5/20], Step [9800/10000], Loss: 1.0011\n",
            "Epoch [5/20], Step [9900/10000], Loss: 1.4621\n",
            "Epoch [5/20], Step [10000/10000], Loss: 1.1149\n",
            "Epoch [6/20], Step [100/10000], Loss: 0.9571\n",
            "Epoch [6/20], Step [200/10000], Loss: 0.9046\n",
            "Epoch [6/20], Step [300/10000], Loss: 0.8844\n",
            "Epoch [6/20], Step [400/10000], Loss: 1.5138\n",
            "Epoch [6/20], Step [500/10000], Loss: 0.5344\n",
            "Epoch [6/20], Step [600/10000], Loss: 1.2216\n",
            "Epoch [6/20], Step [700/10000], Loss: 2.0962\n",
            "Epoch [6/20], Step [800/10000], Loss: 2.8220\n",
            "Epoch [6/20], Step [900/10000], Loss: 1.5238\n",
            "Epoch [6/20], Step [1000/10000], Loss: 1.3450\n",
            "Epoch [6/20], Step [1100/10000], Loss: 0.8977\n",
            "Epoch [6/20], Step [1200/10000], Loss: 1.0649\n",
            "Epoch [6/20], Step [1300/10000], Loss: 1.2067\n",
            "Epoch [6/20], Step [1400/10000], Loss: 1.6022\n",
            "Epoch [6/20], Step [1500/10000], Loss: 1.0071\n",
            "Epoch [6/20], Step [1600/10000], Loss: 1.2582\n",
            "Epoch [6/20], Step [1700/10000], Loss: 1.6936\n",
            "Epoch [6/20], Step [1800/10000], Loss: 1.6710\n",
            "Epoch [6/20], Step [1900/10000], Loss: 1.0825\n",
            "Epoch [6/20], Step [2000/10000], Loss: 1.0638\n",
            "Epoch [6/20], Step [2100/10000], Loss: 1.8752\n",
            "Epoch [6/20], Step [2200/10000], Loss: 1.8338\n",
            "Epoch [6/20], Step [2300/10000], Loss: 0.8444\n",
            "Epoch [6/20], Step [2400/10000], Loss: 1.3962\n",
            "Epoch [6/20], Step [2500/10000], Loss: 1.4561\n",
            "Epoch [6/20], Step [2600/10000], Loss: 2.1017\n",
            "Epoch [6/20], Step [2700/10000], Loss: 1.3964\n",
            "Epoch [6/20], Step [2800/10000], Loss: 1.0667\n",
            "Epoch [6/20], Step [2900/10000], Loss: 0.8329\n",
            "Epoch [6/20], Step [3000/10000], Loss: 1.6439\n",
            "Epoch [6/20], Step [3100/10000], Loss: 1.0209\n",
            "Epoch [6/20], Step [3200/10000], Loss: 1.5256\n",
            "Epoch [6/20], Step [3300/10000], Loss: 1.1780\n",
            "Epoch [6/20], Step [3400/10000], Loss: 1.4491\n",
            "Epoch [6/20], Step [3500/10000], Loss: 2.0905\n",
            "Epoch [6/20], Step [3600/10000], Loss: 1.3893\n",
            "Epoch [6/20], Step [3700/10000], Loss: 0.9116\n",
            "Epoch [6/20], Step [3800/10000], Loss: 1.2602\n",
            "Epoch [6/20], Step [3900/10000], Loss: 1.1700\n",
            "Epoch [6/20], Step [4000/10000], Loss: 0.7295\n",
            "Epoch [6/20], Step [4100/10000], Loss: 2.0750\n",
            "Epoch [6/20], Step [4200/10000], Loss: 0.9158\n",
            "Epoch [6/20], Step [4300/10000], Loss: 1.0112\n",
            "Epoch [6/20], Step [4400/10000], Loss: 1.2155\n",
            "Epoch [6/20], Step [4500/10000], Loss: 2.0186\n",
            "Epoch [6/20], Step [4600/10000], Loss: 1.3563\n",
            "Epoch [6/20], Step [4700/10000], Loss: 1.8607\n",
            "Epoch [6/20], Step [4800/10000], Loss: 1.2786\n",
            "Epoch [6/20], Step [4900/10000], Loss: 1.9315\n",
            "Epoch [6/20], Step [5000/10000], Loss: 1.6150\n",
            "Epoch [6/20], Step [5100/10000], Loss: 1.5939\n",
            "Epoch [6/20], Step [5200/10000], Loss: 1.9155\n",
            "Epoch [6/20], Step [5300/10000], Loss: 1.7144\n",
            "Epoch [6/20], Step [5400/10000], Loss: 1.4891\n",
            "Epoch [6/20], Step [5500/10000], Loss: 1.6021\n",
            "Epoch [6/20], Step [5600/10000], Loss: 0.6808\n",
            "Epoch [6/20], Step [5700/10000], Loss: 1.7493\n",
            "Epoch [6/20], Step [5800/10000], Loss: 1.0653\n",
            "Epoch [6/20], Step [5900/10000], Loss: 1.7159\n",
            "Epoch [6/20], Step [6000/10000], Loss: 1.7869\n",
            "Epoch [6/20], Step [6100/10000], Loss: 1.1772\n",
            "Epoch [6/20], Step [6200/10000], Loss: 0.8739\n",
            "Epoch [6/20], Step [6300/10000], Loss: 0.7441\n",
            "Epoch [6/20], Step [6400/10000], Loss: 1.9775\n",
            "Epoch [6/20], Step [6500/10000], Loss: 1.1583\n",
            "Epoch [6/20], Step [6600/10000], Loss: 0.4379\n",
            "Epoch [6/20], Step [6700/10000], Loss: 1.9827\n",
            "Epoch [6/20], Step [6800/10000], Loss: 1.1285\n",
            "Epoch [6/20], Step [6900/10000], Loss: 1.8145\n",
            "Epoch [6/20], Step [7000/10000], Loss: 0.6166\n",
            "Epoch [6/20], Step [7100/10000], Loss: 1.1155\n",
            "Epoch [6/20], Step [7200/10000], Loss: 1.0505\n",
            "Epoch [6/20], Step [7300/10000], Loss: 1.3533\n",
            "Epoch [6/20], Step [7400/10000], Loss: 0.7684\n",
            "Epoch [6/20], Step [7500/10000], Loss: 0.8637\n",
            "Epoch [6/20], Step [7600/10000], Loss: 1.2520\n",
            "Epoch [6/20], Step [7700/10000], Loss: 0.3866\n",
            "Epoch [6/20], Step [7800/10000], Loss: 1.1041\n",
            "Epoch [6/20], Step [7900/10000], Loss: 0.7454\n",
            "Epoch [6/20], Step [8000/10000], Loss: 0.9411\n",
            "Epoch [6/20], Step [8100/10000], Loss: 0.7596\n",
            "Epoch [6/20], Step [8200/10000], Loss: 1.7489\n",
            "Epoch [6/20], Step [8300/10000], Loss: 1.2784\n",
            "Epoch [6/20], Step [8400/10000], Loss: 0.9080\n",
            "Epoch [6/20], Step [8500/10000], Loss: 1.2783\n",
            "Epoch [6/20], Step [8600/10000], Loss: 0.9940\n",
            "Epoch [6/20], Step [8700/10000], Loss: 1.3709\n",
            "Epoch [6/20], Step [8800/10000], Loss: 2.0176\n",
            "Epoch [6/20], Step [8900/10000], Loss: 1.0928\n",
            "Epoch [6/20], Step [9000/10000], Loss: 1.7328\n",
            "Epoch [6/20], Step [9100/10000], Loss: 0.9116\n",
            "Epoch [6/20], Step [9200/10000], Loss: 0.6690\n",
            "Epoch [6/20], Step [9300/10000], Loss: 2.3663\n",
            "Epoch [6/20], Step [9400/10000], Loss: 1.0688\n",
            "Epoch [6/20], Step [9500/10000], Loss: 2.2486\n",
            "Epoch [6/20], Step [9600/10000], Loss: 0.8209\n",
            "Epoch [6/20], Step [9700/10000], Loss: 0.4616\n",
            "Epoch [6/20], Step [9800/10000], Loss: 0.7622\n",
            "Epoch [6/20], Step [9900/10000], Loss: 1.4640\n",
            "Epoch [6/20], Step [10000/10000], Loss: 2.7450\n",
            "Epoch [7/20], Step [100/10000], Loss: 0.6353\n",
            "Epoch [7/20], Step [200/10000], Loss: 2.2147\n",
            "Epoch [7/20], Step [300/10000], Loss: 1.1588\n",
            "Epoch [7/20], Step [400/10000], Loss: 1.8464\n",
            "Epoch [7/20], Step [500/10000], Loss: 0.2367\n",
            "Epoch [7/20], Step [600/10000], Loss: 1.1443\n",
            "Epoch [7/20], Step [700/10000], Loss: 0.3733\n",
            "Epoch [7/20], Step [800/10000], Loss: 1.1230\n",
            "Epoch [7/20], Step [900/10000], Loss: 1.7749\n",
            "Epoch [7/20], Step [1000/10000], Loss: 1.7009\n",
            "Epoch [7/20], Step [1100/10000], Loss: 1.0856\n",
            "Epoch [7/20], Step [1200/10000], Loss: 1.0786\n",
            "Epoch [7/20], Step [1300/10000], Loss: 0.9013\n",
            "Epoch [7/20], Step [1400/10000], Loss: 1.7785\n",
            "Epoch [7/20], Step [1500/10000], Loss: 0.9196\n",
            "Epoch [7/20], Step [1600/10000], Loss: 1.4469\n",
            "Epoch [7/20], Step [1700/10000], Loss: 0.5280\n",
            "Epoch [7/20], Step [1800/10000], Loss: 1.0302\n",
            "Epoch [7/20], Step [1900/10000], Loss: 1.2379\n",
            "Epoch [7/20], Step [2000/10000], Loss: 0.9642\n",
            "Epoch [7/20], Step [2100/10000], Loss: 1.5200\n",
            "Epoch [7/20], Step [2200/10000], Loss: 1.3447\n",
            "Epoch [7/20], Step [2300/10000], Loss: 0.7781\n",
            "Epoch [7/20], Step [2400/10000], Loss: 1.4193\n",
            "Epoch [7/20], Step [2500/10000], Loss: 0.5095\n",
            "Epoch [7/20], Step [2600/10000], Loss: 1.4715\n",
            "Epoch [7/20], Step [2700/10000], Loss: 2.0156\n",
            "Epoch [7/20], Step [2800/10000], Loss: 0.7864\n",
            "Epoch [7/20], Step [2900/10000], Loss: 1.6805\n",
            "Epoch [7/20], Step [3000/10000], Loss: 1.5449\n",
            "Epoch [7/20], Step [3100/10000], Loss: 1.4786\n",
            "Epoch [7/20], Step [3200/10000], Loss: 0.9545\n",
            "Epoch [7/20], Step [3300/10000], Loss: 1.1418\n",
            "Epoch [7/20], Step [3400/10000], Loss: 1.6531\n",
            "Epoch [7/20], Step [3500/10000], Loss: 1.3229\n",
            "Epoch [7/20], Step [3600/10000], Loss: 1.1446\n",
            "Epoch [7/20], Step [3700/10000], Loss: 1.7171\n",
            "Epoch [7/20], Step [3800/10000], Loss: 1.1590\n",
            "Epoch [7/20], Step [3900/10000], Loss: 1.1927\n",
            "Epoch [7/20], Step [4000/10000], Loss: 1.4129\n",
            "Epoch [7/20], Step [4100/10000], Loss: 1.4578\n",
            "Epoch [7/20], Step [4200/10000], Loss: 2.2667\n",
            "Epoch [7/20], Step [4300/10000], Loss: 1.2693\n",
            "Epoch [7/20], Step [4400/10000], Loss: 0.7590\n",
            "Epoch [7/20], Step [4500/10000], Loss: 1.5616\n",
            "Epoch [7/20], Step [4600/10000], Loss: 1.5782\n",
            "Epoch [7/20], Step [4700/10000], Loss: 0.7681\n",
            "Epoch [7/20], Step [4800/10000], Loss: 2.3020\n",
            "Epoch [7/20], Step [4900/10000], Loss: 0.8375\n",
            "Epoch [7/20], Step [5000/10000], Loss: 1.2140\n",
            "Epoch [7/20], Step [5100/10000], Loss: 1.3866\n",
            "Epoch [7/20], Step [5200/10000], Loss: 2.2448\n",
            "Epoch [7/20], Step [5300/10000], Loss: 0.7013\n",
            "Epoch [7/20], Step [5400/10000], Loss: 1.5022\n",
            "Epoch [7/20], Step [5500/10000], Loss: 0.8491\n",
            "Epoch [7/20], Step [5600/10000], Loss: 1.3543\n",
            "Epoch [7/20], Step [5700/10000], Loss: 1.0388\n",
            "Epoch [7/20], Step [5800/10000], Loss: 1.0743\n",
            "Epoch [7/20], Step [5900/10000], Loss: 1.1907\n",
            "Epoch [7/20], Step [6000/10000], Loss: 0.6350\n",
            "Epoch [7/20], Step [6100/10000], Loss: 0.8981\n",
            "Epoch [7/20], Step [6200/10000], Loss: 1.6962\n",
            "Epoch [7/20], Step [6300/10000], Loss: 1.7156\n",
            "Epoch [7/20], Step [6400/10000], Loss: 0.9215\n",
            "Epoch [7/20], Step [6500/10000], Loss: 1.7167\n",
            "Epoch [7/20], Step [6600/10000], Loss: 1.4863\n",
            "Epoch [7/20], Step [6700/10000], Loss: 1.0910\n",
            "Epoch [7/20], Step [6800/10000], Loss: 1.9825\n",
            "Epoch [7/20], Step [6900/10000], Loss: 1.3691\n",
            "Epoch [7/20], Step [7000/10000], Loss: 1.5765\n",
            "Epoch [7/20], Step [7100/10000], Loss: 1.2922\n",
            "Epoch [7/20], Step [7200/10000], Loss: 1.2944\n",
            "Epoch [7/20], Step [7300/10000], Loss: 1.1526\n",
            "Epoch [7/20], Step [7400/10000], Loss: 1.2363\n",
            "Epoch [7/20], Step [7500/10000], Loss: 0.9199\n",
            "Epoch [7/20], Step [7600/10000], Loss: 1.3597\n",
            "Epoch [7/20], Step [7700/10000], Loss: 1.7131\n",
            "Epoch [7/20], Step [7800/10000], Loss: 1.7585\n",
            "Epoch [7/20], Step [7900/10000], Loss: 1.8604\n",
            "Epoch [7/20], Step [8000/10000], Loss: 1.7818\n",
            "Epoch [7/20], Step [8100/10000], Loss: 0.5687\n",
            "Epoch [7/20], Step [8200/10000], Loss: 1.8012\n",
            "Epoch [7/20], Step [8300/10000], Loss: 0.6085\n",
            "Epoch [7/20], Step [8400/10000], Loss: 0.4792\n",
            "Epoch [7/20], Step [8500/10000], Loss: 1.5739\n",
            "Epoch [7/20], Step [8600/10000], Loss: 1.0558\n",
            "Epoch [7/20], Step [8700/10000], Loss: 1.9473\n",
            "Epoch [7/20], Step [8800/10000], Loss: 1.4924\n",
            "Epoch [7/20], Step [8900/10000], Loss: 1.3964\n",
            "Epoch [7/20], Step [9000/10000], Loss: 1.0783\n",
            "Epoch [7/20], Step [9100/10000], Loss: 1.2046\n",
            "Epoch [7/20], Step [9200/10000], Loss: 1.0968\n",
            "Epoch [7/20], Step [9300/10000], Loss: 1.2596\n",
            "Epoch [7/20], Step [9400/10000], Loss: 1.0064\n",
            "Epoch [7/20], Step [9500/10000], Loss: 1.4695\n",
            "Epoch [7/20], Step [9600/10000], Loss: 0.6134\n",
            "Epoch [7/20], Step [9700/10000], Loss: 2.5651\n",
            "Epoch [7/20], Step [9800/10000], Loss: 0.6281\n",
            "Epoch [7/20], Step [9900/10000], Loss: 1.7864\n",
            "Epoch [7/20], Step [10000/10000], Loss: 1.1094\n",
            "Epoch [8/20], Step [100/10000], Loss: 1.2574\n",
            "Epoch [8/20], Step [200/10000], Loss: 1.5820\n",
            "Epoch [8/20], Step [300/10000], Loss: 1.0783\n",
            "Epoch [8/20], Step [400/10000], Loss: 2.7796\n",
            "Epoch [8/20], Step [500/10000], Loss: 1.9039\n",
            "Epoch [8/20], Step [600/10000], Loss: 1.8031\n",
            "Epoch [8/20], Step [700/10000], Loss: 0.7871\n",
            "Epoch [8/20], Step [800/10000], Loss: 1.1800\n",
            "Epoch [8/20], Step [900/10000], Loss: 2.2137\n",
            "Epoch [8/20], Step [1000/10000], Loss: 1.1842\n",
            "Epoch [8/20], Step [1100/10000], Loss: 0.5028\n",
            "Epoch [8/20], Step [1200/10000], Loss: 1.6171\n",
            "Epoch [8/20], Step [1300/10000], Loss: 1.5703\n",
            "Epoch [8/20], Step [1400/10000], Loss: 0.9630\n",
            "Epoch [8/20], Step [1500/10000], Loss: 0.9666\n",
            "Epoch [8/20], Step [1600/10000], Loss: 1.3612\n",
            "Epoch [8/20], Step [1700/10000], Loss: 2.3895\n",
            "Epoch [8/20], Step [1800/10000], Loss: 1.5662\n",
            "Epoch [8/20], Step [1900/10000], Loss: 1.2879\n",
            "Epoch [8/20], Step [2000/10000], Loss: 0.7890\n",
            "Epoch [8/20], Step [2100/10000], Loss: 1.6837\n",
            "Epoch [8/20], Step [2200/10000], Loss: 1.2265\n",
            "Epoch [8/20], Step [2300/10000], Loss: 1.6021\n",
            "Epoch [8/20], Step [2400/10000], Loss: 1.7890\n",
            "Epoch [8/20], Step [2500/10000], Loss: 1.5378\n",
            "Epoch [8/20], Step [2600/10000], Loss: 2.0972\n",
            "Epoch [8/20], Step [2700/10000], Loss: 0.2466\n",
            "Epoch [8/20], Step [2800/10000], Loss: 1.9203\n",
            "Epoch [8/20], Step [2900/10000], Loss: 0.6305\n",
            "Epoch [8/20], Step [3000/10000], Loss: 2.1097\n",
            "Epoch [8/20], Step [3100/10000], Loss: 0.9436\n",
            "Epoch [8/20], Step [3200/10000], Loss: 1.3891\n",
            "Epoch [8/20], Step [3300/10000], Loss: 0.1937\n",
            "Epoch [8/20], Step [3400/10000], Loss: 1.0469\n",
            "Epoch [8/20], Step [3500/10000], Loss: 1.0811\n",
            "Epoch [8/20], Step [3600/10000], Loss: 1.2421\n",
            "Epoch [8/20], Step [3700/10000], Loss: 0.4776\n",
            "Epoch [8/20], Step [3800/10000], Loss: 0.2478\n",
            "Epoch [8/20], Step [3900/10000], Loss: 1.1940\n",
            "Epoch [8/20], Step [4000/10000], Loss: 2.2295\n",
            "Epoch [8/20], Step [4100/10000], Loss: 0.5673\n",
            "Epoch [8/20], Step [4200/10000], Loss: 2.3751\n",
            "Epoch [8/20], Step [4300/10000], Loss: 0.4192\n",
            "Epoch [8/20], Step [4400/10000], Loss: 0.5717\n",
            "Epoch [8/20], Step [4500/10000], Loss: 1.8665\n",
            "Epoch [8/20], Step [4600/10000], Loss: 1.3543\n",
            "Epoch [8/20], Step [4700/10000], Loss: 1.8776\n",
            "Epoch [8/20], Step [4800/10000], Loss: 1.4098\n",
            "Epoch [8/20], Step [4900/10000], Loss: 1.4266\n",
            "Epoch [8/20], Step [5000/10000], Loss: 2.9709\n",
            "Epoch [8/20], Step [5100/10000], Loss: 0.3858\n",
            "Epoch [8/20], Step [5200/10000], Loss: 0.8994\n",
            "Epoch [8/20], Step [5300/10000], Loss: 1.8492\n",
            "Epoch [8/20], Step [5400/10000], Loss: 0.8505\n",
            "Epoch [8/20], Step [5500/10000], Loss: 1.3090\n",
            "Epoch [8/20], Step [5600/10000], Loss: 0.8278\n",
            "Epoch [8/20], Step [5700/10000], Loss: 1.7047\n",
            "Epoch [8/20], Step [5800/10000], Loss: 1.1390\n",
            "Epoch [8/20], Step [5900/10000], Loss: 1.2444\n",
            "Epoch [8/20], Step [6000/10000], Loss: 2.0424\n",
            "Epoch [8/20], Step [6100/10000], Loss: 1.0439\n",
            "Epoch [8/20], Step [6200/10000], Loss: 1.9680\n",
            "Epoch [8/20], Step [6300/10000], Loss: 1.2615\n",
            "Epoch [8/20], Step [6400/10000], Loss: 1.6780\n",
            "Epoch [8/20], Step [6500/10000], Loss: 1.4650\n",
            "Epoch [8/20], Step [6600/10000], Loss: 1.0432\n",
            "Epoch [8/20], Step [6700/10000], Loss: 1.5763\n",
            "Epoch [8/20], Step [6800/10000], Loss: 1.8335\n",
            "Epoch [8/20], Step [6900/10000], Loss: 1.0771\n",
            "Epoch [8/20], Step [7000/10000], Loss: 1.4949\n",
            "Epoch [8/20], Step [7100/10000], Loss: 0.4145\n",
            "Epoch [8/20], Step [7200/10000], Loss: 0.9648\n",
            "Epoch [8/20], Step [7300/10000], Loss: 1.8901\n",
            "Epoch [8/20], Step [7400/10000], Loss: 0.9791\n",
            "Epoch [8/20], Step [7500/10000], Loss: 1.6371\n",
            "Epoch [8/20], Step [7600/10000], Loss: 1.0914\n",
            "Epoch [8/20], Step [7700/10000], Loss: 0.4588\n",
            "Epoch [8/20], Step [7800/10000], Loss: 0.6217\n",
            "Epoch [8/20], Step [7900/10000], Loss: 1.4267\n",
            "Epoch [8/20], Step [8000/10000], Loss: 1.6788\n",
            "Epoch [8/20], Step [8100/10000], Loss: 2.4048\n",
            "Epoch [8/20], Step [8200/10000], Loss: 1.1267\n",
            "Epoch [8/20], Step [8300/10000], Loss: 1.3721\n",
            "Epoch [8/20], Step [8400/10000], Loss: 2.0780\n",
            "Epoch [8/20], Step [8500/10000], Loss: 1.0035\n",
            "Epoch [8/20], Step [8600/10000], Loss: 0.9308\n",
            "Epoch [8/20], Step [8700/10000], Loss: 1.8494\n",
            "Epoch [8/20], Step [8800/10000], Loss: 1.6983\n",
            "Epoch [8/20], Step [8900/10000], Loss: 1.1800\n",
            "Epoch [8/20], Step [9000/10000], Loss: 2.6023\n",
            "Epoch [8/20], Step [9100/10000], Loss: 1.5041\n",
            "Epoch [8/20], Step [9200/10000], Loss: 1.6943\n",
            "Epoch [8/20], Step [9300/10000], Loss: 1.2781\n",
            "Epoch [8/20], Step [9400/10000], Loss: 0.5999\n",
            "Epoch [8/20], Step [9500/10000], Loss: 0.5572\n",
            "Epoch [8/20], Step [9600/10000], Loss: 0.9170\n",
            "Epoch [8/20], Step [9700/10000], Loss: 2.0161\n",
            "Epoch [8/20], Step [9800/10000], Loss: 2.0722\n",
            "Epoch [8/20], Step [9900/10000], Loss: 1.3599\n",
            "Epoch [8/20], Step [10000/10000], Loss: 1.6011\n",
            "Epoch [9/20], Step [100/10000], Loss: 3.2010\n",
            "Epoch [9/20], Step [200/10000], Loss: 1.8602\n",
            "Epoch [9/20], Step [300/10000], Loss: 1.0983\n",
            "Epoch [9/20], Step [400/10000], Loss: 1.6328\n",
            "Epoch [9/20], Step [500/10000], Loss: 2.0106\n",
            "Epoch [9/20], Step [600/10000], Loss: 1.1610\n",
            "Epoch [9/20], Step [700/10000], Loss: 1.5039\n",
            "Epoch [9/20], Step [800/10000], Loss: 1.9277\n",
            "Epoch [9/20], Step [900/10000], Loss: 2.5939\n",
            "Epoch [9/20], Step [1000/10000], Loss: 1.9135\n",
            "Epoch [9/20], Step [1100/10000], Loss: 1.8070\n",
            "Epoch [9/20], Step [1200/10000], Loss: 0.9153\n",
            "Epoch [9/20], Step [1300/10000], Loss: 0.6802\n",
            "Epoch [9/20], Step [1400/10000], Loss: 2.5801\n",
            "Epoch [9/20], Step [1500/10000], Loss: 1.7487\n",
            "Epoch [9/20], Step [1600/10000], Loss: 1.5490\n",
            "Epoch [9/20], Step [1700/10000], Loss: 0.8188\n",
            "Epoch [9/20], Step [1800/10000], Loss: 0.4484\n",
            "Epoch [9/20], Step [1900/10000], Loss: 0.5984\n",
            "Epoch [9/20], Step [2000/10000], Loss: 1.4620\n",
            "Epoch [9/20], Step [2100/10000], Loss: 1.0008\n",
            "Epoch [9/20], Step [2200/10000], Loss: 0.9533\n",
            "Epoch [9/20], Step [2300/10000], Loss: 1.8122\n",
            "Epoch [9/20], Step [2400/10000], Loss: 2.0251\n",
            "Epoch [9/20], Step [2500/10000], Loss: 1.7561\n",
            "Epoch [9/20], Step [2600/10000], Loss: 1.0473\n",
            "Epoch [9/20], Step [2700/10000], Loss: 1.0559\n",
            "Epoch [9/20], Step [2800/10000], Loss: 0.9629\n",
            "Epoch [9/20], Step [2900/10000], Loss: 0.8966\n",
            "Epoch [9/20], Step [3000/10000], Loss: 0.7337\n",
            "Epoch [9/20], Step [3100/10000], Loss: 1.6139\n",
            "Epoch [9/20], Step [3200/10000], Loss: 1.8399\n",
            "Epoch [9/20], Step [3300/10000], Loss: 0.9535\n",
            "Epoch [9/20], Step [3400/10000], Loss: 1.2499\n",
            "Epoch [9/20], Step [3500/10000], Loss: 1.7173\n",
            "Epoch [9/20], Step [3600/10000], Loss: 1.4660\n",
            "Epoch [9/20], Step [3700/10000], Loss: 2.4961\n",
            "Epoch [9/20], Step [3800/10000], Loss: 2.1772\n",
            "Epoch [9/20], Step [3900/10000], Loss: 1.8933\n",
            "Epoch [9/20], Step [4000/10000], Loss: 0.7512\n",
            "Epoch [9/20], Step [4100/10000], Loss: 1.7425\n",
            "Epoch [9/20], Step [4200/10000], Loss: 1.2556\n",
            "Epoch [9/20], Step [4300/10000], Loss: 1.1821\n",
            "Epoch [9/20], Step [4400/10000], Loss: 2.0018\n",
            "Epoch [9/20], Step [4500/10000], Loss: 0.7601\n",
            "Epoch [9/20], Step [4600/10000], Loss: 1.3726\n",
            "Epoch [9/20], Step [4700/10000], Loss: 0.9768\n",
            "Epoch [9/20], Step [4800/10000], Loss: 0.6315\n",
            "Epoch [9/20], Step [4900/10000], Loss: 0.9074\n",
            "Epoch [9/20], Step [5000/10000], Loss: 0.6934\n",
            "Epoch [9/20], Step [5100/10000], Loss: 1.3491\n",
            "Epoch [9/20], Step [5200/10000], Loss: 2.0028\n",
            "Epoch [9/20], Step [5300/10000], Loss: 1.2417\n",
            "Epoch [9/20], Step [5400/10000], Loss: 0.7810\n",
            "Epoch [9/20], Step [5500/10000], Loss: 1.1739\n",
            "Epoch [9/20], Step [5600/10000], Loss: 2.2661\n",
            "Epoch [9/20], Step [5700/10000], Loss: 1.1711\n",
            "Epoch [9/20], Step [5800/10000], Loss: 2.0333\n",
            "Epoch [9/20], Step [5900/10000], Loss: 0.4723\n",
            "Epoch [9/20], Step [6000/10000], Loss: 0.7457\n",
            "Epoch [9/20], Step [6100/10000], Loss: 0.9772\n",
            "Epoch [9/20], Step [6200/10000], Loss: 1.1782\n",
            "Epoch [9/20], Step [6300/10000], Loss: 1.6648\n",
            "Epoch [9/20], Step [6400/10000], Loss: 1.4266\n",
            "Epoch [9/20], Step [6500/10000], Loss: 0.5381\n",
            "Epoch [9/20], Step [6600/10000], Loss: 2.0641\n",
            "Epoch [9/20], Step [6700/10000], Loss: 0.6550\n",
            "Epoch [9/20], Step [6800/10000], Loss: 0.7569\n",
            "Epoch [9/20], Step [6900/10000], Loss: 1.3178\n",
            "Epoch [9/20], Step [7000/10000], Loss: 0.9878\n",
            "Epoch [9/20], Step [7100/10000], Loss: 1.2382\n",
            "Epoch [9/20], Step [7200/10000], Loss: 1.1324\n",
            "Epoch [9/20], Step [7300/10000], Loss: 1.4459\n",
            "Epoch [9/20], Step [7400/10000], Loss: 1.6529\n",
            "Epoch [9/20], Step [7500/10000], Loss: 1.6814\n",
            "Epoch [9/20], Step [7600/10000], Loss: 1.3432\n",
            "Epoch [9/20], Step [7700/10000], Loss: 2.3826\n",
            "Epoch [9/20], Step [7800/10000], Loss: 0.6623\n",
            "Epoch [9/20], Step [7900/10000], Loss: 1.6810\n",
            "Epoch [9/20], Step [8000/10000], Loss: 0.7887\n",
            "Epoch [9/20], Step [8100/10000], Loss: 2.1508\n",
            "Epoch [9/20], Step [8200/10000], Loss: 1.5099\n",
            "Epoch [9/20], Step [8300/10000], Loss: 1.2930\n",
            "Epoch [9/20], Step [8400/10000], Loss: 2.5107\n",
            "Epoch [9/20], Step [8500/10000], Loss: 0.6601\n",
            "Epoch [9/20], Step [8600/10000], Loss: 1.9409\n",
            "Epoch [9/20], Step [8700/10000], Loss: 1.8448\n",
            "Epoch [9/20], Step [8800/10000], Loss: 1.1312\n",
            "Epoch [9/20], Step [8900/10000], Loss: 1.0510\n",
            "Epoch [9/20], Step [9000/10000], Loss: 0.3342\n",
            "Epoch [9/20], Step [9100/10000], Loss: 1.3366\n",
            "Epoch [9/20], Step [9200/10000], Loss: 0.7831\n",
            "Epoch [9/20], Step [9300/10000], Loss: 0.9937\n",
            "Epoch [9/20], Step [9400/10000], Loss: 2.4543\n",
            "Epoch [9/20], Step [9500/10000], Loss: 0.7005\n",
            "Epoch [9/20], Step [9600/10000], Loss: 1.1790\n",
            "Epoch [9/20], Step [9700/10000], Loss: 1.6280\n",
            "Epoch [9/20], Step [9800/10000], Loss: 1.3627\n",
            "Epoch [9/20], Step [9900/10000], Loss: 0.5473\n",
            "Epoch [9/20], Step [10000/10000], Loss: 1.2006\n",
            "Epoch [10/20], Step [100/10000], Loss: 2.2140\n",
            "Epoch [10/20], Step [200/10000], Loss: 1.7403\n",
            "Epoch [10/20], Step [300/10000], Loss: 0.3646\n",
            "Epoch [10/20], Step [400/10000], Loss: 1.4905\n",
            "Epoch [10/20], Step [500/10000], Loss: 0.2614\n",
            "Epoch [10/20], Step [600/10000], Loss: 1.5850\n",
            "Epoch [10/20], Step [700/10000], Loss: 0.9718\n",
            "Epoch [10/20], Step [800/10000], Loss: 1.6583\n",
            "Epoch [10/20], Step [900/10000], Loss: 0.8310\n",
            "Epoch [10/20], Step [1000/10000], Loss: 1.3136\n",
            "Epoch [10/20], Step [1100/10000], Loss: 1.6031\n",
            "Epoch [10/20], Step [1200/10000], Loss: 1.6299\n",
            "Epoch [10/20], Step [1300/10000], Loss: 1.3759\n",
            "Epoch [10/20], Step [1400/10000], Loss: 0.7413\n",
            "Epoch [10/20], Step [1500/10000], Loss: 2.0808\n",
            "Epoch [10/20], Step [1600/10000], Loss: 0.7243\n",
            "Epoch [10/20], Step [1700/10000], Loss: 1.3846\n",
            "Epoch [10/20], Step [1800/10000], Loss: 0.6312\n",
            "Epoch [10/20], Step [1900/10000], Loss: 1.0685\n",
            "Epoch [10/20], Step [2000/10000], Loss: 0.4786\n",
            "Epoch [10/20], Step [2100/10000], Loss: 0.7608\n",
            "Epoch [10/20], Step [2200/10000], Loss: 1.4612\n",
            "Epoch [10/20], Step [2300/10000], Loss: 0.8345\n",
            "Epoch [10/20], Step [2400/10000], Loss: 0.9346\n",
            "Epoch [10/20], Step [2500/10000], Loss: 0.8607\n",
            "Epoch [10/20], Step [2600/10000], Loss: 0.6710\n",
            "Epoch [10/20], Step [2700/10000], Loss: 1.9984\n",
            "Epoch [10/20], Step [2800/10000], Loss: 1.9115\n",
            "Epoch [10/20], Step [2900/10000], Loss: 1.9576\n",
            "Epoch [10/20], Step [3000/10000], Loss: 1.6363\n",
            "Epoch [10/20], Step [3100/10000], Loss: 2.4056\n",
            "Epoch [10/20], Step [3200/10000], Loss: 1.3777\n",
            "Epoch [10/20], Step [3300/10000], Loss: 1.2561\n",
            "Epoch [10/20], Step [3400/10000], Loss: 1.2606\n",
            "Epoch [10/20], Step [3500/10000], Loss: 1.6571\n",
            "Epoch [10/20], Step [3600/10000], Loss: 1.7379\n",
            "Epoch [10/20], Step [3700/10000], Loss: 1.2626\n",
            "Epoch [10/20], Step [3800/10000], Loss: 1.8642\n",
            "Epoch [10/20], Step [3900/10000], Loss: 1.6838\n",
            "Epoch [10/20], Step [4000/10000], Loss: 0.9137\n",
            "Epoch [10/20], Step [4100/10000], Loss: 0.9893\n",
            "Epoch [10/20], Step [4200/10000], Loss: 1.2992\n",
            "Epoch [10/20], Step [4300/10000], Loss: 1.5517\n",
            "Epoch [10/20], Step [4400/10000], Loss: 2.2822\n",
            "Epoch [10/20], Step [4500/10000], Loss: 1.4204\n",
            "Epoch [10/20], Step [4600/10000], Loss: 1.0892\n",
            "Epoch [10/20], Step [4700/10000], Loss: 1.4959\n",
            "Epoch [10/20], Step [4800/10000], Loss: 1.6197\n",
            "Epoch [10/20], Step [4900/10000], Loss: 0.9217\n",
            "Epoch [10/20], Step [5000/10000], Loss: 0.5683\n",
            "Epoch [10/20], Step [5100/10000], Loss: 1.8303\n",
            "Epoch [10/20], Step [5200/10000], Loss: 1.4481\n",
            "Epoch [10/20], Step [5300/10000], Loss: 0.5957\n",
            "Epoch [10/20], Step [5400/10000], Loss: 1.9591\n",
            "Epoch [10/20], Step [5500/10000], Loss: 0.7826\n",
            "Epoch [10/20], Step [5600/10000], Loss: 1.2194\n",
            "Epoch [10/20], Step [5700/10000], Loss: 0.8876\n",
            "Epoch [10/20], Step [5800/10000], Loss: 1.1327\n",
            "Epoch [10/20], Step [5900/10000], Loss: 1.6266\n",
            "Epoch [10/20], Step [6000/10000], Loss: 1.0442\n",
            "Epoch [10/20], Step [6100/10000], Loss: 1.1120\n",
            "Epoch [10/20], Step [6200/10000], Loss: 1.0373\n",
            "Epoch [10/20], Step [6300/10000], Loss: 1.4201\n",
            "Epoch [10/20], Step [6400/10000], Loss: 0.6917\n",
            "Epoch [10/20], Step [6500/10000], Loss: 1.0968\n",
            "Epoch [10/20], Step [6600/10000], Loss: 0.7614\n",
            "Epoch [10/20], Step [6700/10000], Loss: 1.9424\n",
            "Epoch [10/20], Step [6800/10000], Loss: 1.8431\n",
            "Epoch [10/20], Step [6900/10000], Loss: 1.3211\n",
            "Epoch [10/20], Step [7000/10000], Loss: 1.2014\n",
            "Epoch [10/20], Step [7100/10000], Loss: 1.0034\n",
            "Epoch [10/20], Step [7200/10000], Loss: 1.1391\n",
            "Epoch [10/20], Step [7300/10000], Loss: 1.1928\n",
            "Epoch [10/20], Step [7400/10000], Loss: 1.1235\n",
            "Epoch [10/20], Step [7500/10000], Loss: 1.2732\n",
            "Epoch [10/20], Step [7600/10000], Loss: 1.2249\n",
            "Epoch [10/20], Step [7700/10000], Loss: 0.8228\n",
            "Epoch [10/20], Step [7800/10000], Loss: 1.2887\n",
            "Epoch [10/20], Step [7900/10000], Loss: 1.2618\n",
            "Epoch [10/20], Step [8000/10000], Loss: 1.4168\n",
            "Epoch [10/20], Step [8100/10000], Loss: 0.6777\n",
            "Epoch [10/20], Step [8200/10000], Loss: 1.1062\n",
            "Epoch [10/20], Step [8300/10000], Loss: 1.0566\n",
            "Epoch [10/20], Step [8400/10000], Loss: 1.2003\n",
            "Epoch [10/20], Step [8500/10000], Loss: 1.3531\n",
            "Epoch [10/20], Step [8600/10000], Loss: 0.7677\n",
            "Epoch [10/20], Step [8700/10000], Loss: 1.2446\n",
            "Epoch [10/20], Step [8800/10000], Loss: 1.1433\n",
            "Epoch [10/20], Step [8900/10000], Loss: 0.7317\n",
            "Epoch [10/20], Step [9000/10000], Loss: 1.1846\n",
            "Epoch [10/20], Step [9100/10000], Loss: 1.1535\n",
            "Epoch [10/20], Step [9200/10000], Loss: 0.8934\n",
            "Epoch [10/20], Step [9300/10000], Loss: 1.6348\n",
            "Epoch [10/20], Step [9400/10000], Loss: 0.6964\n",
            "Epoch [10/20], Step [9500/10000], Loss: 1.0440\n",
            "Epoch [10/20], Step [9600/10000], Loss: 1.4321\n",
            "Epoch [10/20], Step [9700/10000], Loss: 1.2350\n",
            "Epoch [10/20], Step [9800/10000], Loss: 1.6465\n",
            "Epoch [10/20], Step [9900/10000], Loss: 0.5073\n",
            "Epoch [10/20], Step [10000/10000], Loss: 0.7120\n",
            "Epoch [11/20], Step [100/10000], Loss: 0.3499\n",
            "Epoch [11/20], Step [200/10000], Loss: 1.5513\n",
            "Epoch [11/20], Step [300/10000], Loss: 1.9180\n",
            "Epoch [11/20], Step [400/10000], Loss: 3.5334\n",
            "Epoch [11/20], Step [500/10000], Loss: 0.6966\n",
            "Epoch [11/20], Step [600/10000], Loss: 1.0647\n",
            "Epoch [11/20], Step [700/10000], Loss: 1.2181\n",
            "Epoch [11/20], Step [800/10000], Loss: 1.5112\n",
            "Epoch [11/20], Step [900/10000], Loss: 0.5650\n",
            "Epoch [11/20], Step [1000/10000], Loss: 1.2726\n",
            "Epoch [11/20], Step [1100/10000], Loss: 2.0718\n",
            "Epoch [11/20], Step [1200/10000], Loss: 1.6330\n",
            "Epoch [11/20], Step [1300/10000], Loss: 1.9878\n",
            "Epoch [11/20], Step [1400/10000], Loss: 1.4073\n",
            "Epoch [11/20], Step [1500/10000], Loss: 1.6401\n",
            "Epoch [11/20], Step [1600/10000], Loss: 0.5156\n",
            "Epoch [11/20], Step [1700/10000], Loss: 1.1213\n",
            "Epoch [11/20], Step [1800/10000], Loss: 1.0248\n",
            "Epoch [11/20], Step [1900/10000], Loss: 2.0701\n",
            "Epoch [11/20], Step [2000/10000], Loss: 1.6078\n",
            "Epoch [11/20], Step [2100/10000], Loss: 1.2260\n",
            "Epoch [11/20], Step [2200/10000], Loss: 1.7778\n",
            "Epoch [11/20], Step [2300/10000], Loss: 1.3259\n",
            "Epoch [11/20], Step [2400/10000], Loss: 1.2600\n",
            "Epoch [11/20], Step [2500/10000], Loss: 1.4065\n",
            "Epoch [11/20], Step [2600/10000], Loss: 0.4476\n",
            "Epoch [11/20], Step [2700/10000], Loss: 1.1947\n",
            "Epoch [11/20], Step [2800/10000], Loss: 1.2830\n",
            "Epoch [11/20], Step [2900/10000], Loss: 1.8521\n",
            "Epoch [11/20], Step [3000/10000], Loss: 1.4619\n",
            "Epoch [11/20], Step [3100/10000], Loss: 0.8661\n",
            "Epoch [11/20], Step [3200/10000], Loss: 1.1548\n",
            "Epoch [11/20], Step [3300/10000], Loss: 0.9960\n",
            "Epoch [11/20], Step [3400/10000], Loss: 1.2825\n",
            "Epoch [11/20], Step [3500/10000], Loss: 1.3864\n",
            "Epoch [11/20], Step [3600/10000], Loss: 0.5469\n",
            "Epoch [11/20], Step [3700/10000], Loss: 1.2411\n",
            "Epoch [11/20], Step [3800/10000], Loss: 1.2225\n",
            "Epoch [11/20], Step [3900/10000], Loss: 1.5976\n",
            "Epoch [11/20], Step [4000/10000], Loss: 1.2301\n",
            "Epoch [11/20], Step [4100/10000], Loss: 0.8780\n",
            "Epoch [11/20], Step [4200/10000], Loss: 2.1036\n",
            "Epoch [11/20], Step [4300/10000], Loss: 1.0524\n",
            "Epoch [11/20], Step [4400/10000], Loss: 2.0832\n",
            "Epoch [11/20], Step [4500/10000], Loss: 1.0929\n",
            "Epoch [11/20], Step [4600/10000], Loss: 0.9517\n",
            "Epoch [11/20], Step [4700/10000], Loss: 0.8457\n",
            "Epoch [11/20], Step [4800/10000], Loss: 1.2731\n",
            "Epoch [11/20], Step [4900/10000], Loss: 0.7133\n",
            "Epoch [11/20], Step [5000/10000], Loss: 0.9044\n",
            "Epoch [11/20], Step [5100/10000], Loss: 1.5770\n",
            "Epoch [11/20], Step [5200/10000], Loss: 1.5398\n",
            "Epoch [11/20], Step [5300/10000], Loss: 0.7955\n",
            "Epoch [11/20], Step [5400/10000], Loss: 1.2299\n",
            "Epoch [11/20], Step [5500/10000], Loss: 2.0850\n",
            "Epoch [11/20], Step [5600/10000], Loss: 1.9386\n",
            "Epoch [11/20], Step [5700/10000], Loss: 0.4000\n",
            "Epoch [11/20], Step [5800/10000], Loss: 0.6076\n",
            "Epoch [11/20], Step [5900/10000], Loss: 1.1132\n",
            "Epoch [11/20], Step [6000/10000], Loss: 2.0074\n",
            "Epoch [11/20], Step [6100/10000], Loss: 1.1245\n",
            "Epoch [11/20], Step [6200/10000], Loss: 0.8472\n",
            "Epoch [11/20], Step [6300/10000], Loss: 1.3396\n",
            "Epoch [11/20], Step [6400/10000], Loss: 0.4996\n",
            "Epoch [11/20], Step [6500/10000], Loss: 1.5554\n",
            "Epoch [11/20], Step [6600/10000], Loss: 1.7263\n",
            "Epoch [11/20], Step [6700/10000], Loss: 1.8736\n",
            "Epoch [11/20], Step [6800/10000], Loss: 0.6737\n",
            "Epoch [11/20], Step [6900/10000], Loss: 1.6856\n",
            "Epoch [11/20], Step [7000/10000], Loss: 1.8536\n",
            "Epoch [11/20], Step [7100/10000], Loss: 1.0272\n",
            "Epoch [11/20], Step [7200/10000], Loss: 1.3949\n",
            "Epoch [11/20], Step [7300/10000], Loss: 1.6307\n",
            "Epoch [11/20], Step [7400/10000], Loss: 1.6041\n",
            "Epoch [11/20], Step [7500/10000], Loss: 0.6845\n",
            "Epoch [11/20], Step [7600/10000], Loss: 0.5368\n",
            "Epoch [11/20], Step [7700/10000], Loss: 1.3251\n",
            "Epoch [11/20], Step [7800/10000], Loss: 0.6828\n",
            "Epoch [11/20], Step [7900/10000], Loss: 0.8841\n",
            "Epoch [11/20], Step [8000/10000], Loss: 1.2409\n",
            "Epoch [11/20], Step [8100/10000], Loss: 1.0122\n",
            "Epoch [11/20], Step [8200/10000], Loss: 0.7266\n",
            "Epoch [11/20], Step [8300/10000], Loss: 1.4913\n",
            "Epoch [11/20], Step [8400/10000], Loss: 1.2198\n",
            "Epoch [11/20], Step [8500/10000], Loss: 0.8798\n",
            "Epoch [11/20], Step [8600/10000], Loss: 0.9297\n",
            "Epoch [11/20], Step [8700/10000], Loss: 1.1166\n",
            "Epoch [11/20], Step [8800/10000], Loss: 1.6668\n",
            "Epoch [11/20], Step [8900/10000], Loss: 1.4992\n",
            "Epoch [11/20], Step [9000/10000], Loss: 0.7275\n",
            "Epoch [11/20], Step [9100/10000], Loss: 0.9071\n",
            "Epoch [11/20], Step [9200/10000], Loss: 1.2064\n",
            "Epoch [11/20], Step [9300/10000], Loss: 0.5219\n",
            "Epoch [11/20], Step [9400/10000], Loss: 0.4897\n",
            "Epoch [11/20], Step [9500/10000], Loss: 0.6998\n",
            "Epoch [11/20], Step [9600/10000], Loss: 0.8447\n",
            "Epoch [11/20], Step [9700/10000], Loss: 3.1700\n",
            "Epoch [11/20], Step [9800/10000], Loss: 0.8853\n",
            "Epoch [11/20], Step [9900/10000], Loss: 1.4160\n",
            "Epoch [11/20], Step [10000/10000], Loss: 0.7705\n",
            "Epoch [12/20], Step [100/10000], Loss: 1.4070\n",
            "Epoch [12/20], Step [200/10000], Loss: 0.8403\n",
            "Epoch [12/20], Step [300/10000], Loss: 0.8287\n",
            "Epoch [12/20], Step [400/10000], Loss: 0.9891\n",
            "Epoch [12/20], Step [500/10000], Loss: 0.3980\n",
            "Epoch [12/20], Step [600/10000], Loss: 1.0425\n",
            "Epoch [12/20], Step [700/10000], Loss: 1.0169\n",
            "Epoch [12/20], Step [800/10000], Loss: 1.9161\n",
            "Epoch [12/20], Step [900/10000], Loss: 1.7709\n",
            "Epoch [12/20], Step [1000/10000], Loss: 1.1852\n",
            "Epoch [12/20], Step [1100/10000], Loss: 1.1556\n",
            "Epoch [12/20], Step [1200/10000], Loss: 0.4766\n",
            "Epoch [12/20], Step [1300/10000], Loss: 1.1020\n",
            "Epoch [12/20], Step [1400/10000], Loss: 1.3629\n",
            "Epoch [12/20], Step [1500/10000], Loss: 1.2167\n",
            "Epoch [12/20], Step [1600/10000], Loss: 1.9795\n",
            "Epoch [12/20], Step [1700/10000], Loss: 0.9272\n",
            "Epoch [12/20], Step [1800/10000], Loss: 0.9272\n",
            "Epoch [12/20], Step [1900/10000], Loss: 0.6526\n",
            "Epoch [12/20], Step [2000/10000], Loss: 1.6171\n",
            "Epoch [12/20], Step [2100/10000], Loss: 1.2565\n",
            "Epoch [12/20], Step [2200/10000], Loss: 0.7340\n",
            "Epoch [12/20], Step [2300/10000], Loss: 1.3542\n",
            "Epoch [12/20], Step [2400/10000], Loss: 0.9084\n",
            "Epoch [12/20], Step [2500/10000], Loss: 1.6279\n",
            "Epoch [12/20], Step [2600/10000], Loss: 0.6921\n",
            "Epoch [12/20], Step [2700/10000], Loss: 1.2063\n",
            "Epoch [12/20], Step [2800/10000], Loss: 1.1094\n",
            "Epoch [12/20], Step [2900/10000], Loss: 2.3348\n",
            "Epoch [12/20], Step [3000/10000], Loss: 1.3965\n",
            "Epoch [12/20], Step [3100/10000], Loss: 1.3186\n",
            "Epoch [12/20], Step [3200/10000], Loss: 1.5006\n",
            "Epoch [12/20], Step [3300/10000], Loss: 1.4778\n",
            "Epoch [12/20], Step [3400/10000], Loss: 0.8060\n",
            "Epoch [12/20], Step [3500/10000], Loss: 1.9208\n",
            "Epoch [12/20], Step [3600/10000], Loss: 1.0309\n",
            "Epoch [12/20], Step [3700/10000], Loss: 1.2785\n",
            "Epoch [12/20], Step [3800/10000], Loss: 0.9154\n",
            "Epoch [12/20], Step [3900/10000], Loss: 1.6260\n",
            "Epoch [12/20], Step [4000/10000], Loss: 1.3833\n",
            "Epoch [12/20], Step [4100/10000], Loss: 0.5810\n",
            "Epoch [12/20], Step [4200/10000], Loss: 1.7387\n",
            "Epoch [12/20], Step [4300/10000], Loss: 1.3500\n",
            "Epoch [12/20], Step [4400/10000], Loss: 0.5207\n",
            "Epoch [12/20], Step [4500/10000], Loss: 1.4490\n",
            "Epoch [12/20], Step [4600/10000], Loss: 1.3514\n",
            "Epoch [12/20], Step [4700/10000], Loss: 0.6496\n",
            "Epoch [12/20], Step [4800/10000], Loss: 0.9306\n",
            "Epoch [12/20], Step [4900/10000], Loss: 0.9511\n",
            "Epoch [12/20], Step [5000/10000], Loss: 1.1922\n",
            "Epoch [12/20], Step [5100/10000], Loss: 1.0272\n",
            "Epoch [12/20], Step [5200/10000], Loss: 1.3048\n",
            "Epoch [12/20], Step [5300/10000], Loss: 0.6083\n",
            "Epoch [12/20], Step [5400/10000], Loss: 1.4218\n",
            "Epoch [12/20], Step [5500/10000], Loss: 0.4634\n",
            "Epoch [12/20], Step [5600/10000], Loss: 0.8732\n",
            "Epoch [12/20], Step [5700/10000], Loss: 1.2482\n",
            "Epoch [12/20], Step [5800/10000], Loss: 3.1879\n",
            "Epoch [12/20], Step [5900/10000], Loss: 0.7158\n",
            "Epoch [12/20], Step [6000/10000], Loss: 2.1323\n",
            "Epoch [12/20], Step [6100/10000], Loss: 1.0638\n",
            "Epoch [12/20], Step [6200/10000], Loss: 1.3016\n",
            "Epoch [12/20], Step [6300/10000], Loss: 1.1702\n",
            "Epoch [12/20], Step [6400/10000], Loss: 1.0547\n",
            "Epoch [12/20], Step [6500/10000], Loss: 1.2320\n",
            "Epoch [12/20], Step [6600/10000], Loss: 1.5242\n",
            "Epoch [12/20], Step [6700/10000], Loss: 1.4563\n",
            "Epoch [12/20], Step [6800/10000], Loss: 1.3649\n",
            "Epoch [12/20], Step [6900/10000], Loss: 2.5266\n",
            "Epoch [12/20], Step [7000/10000], Loss: 3.6564\n",
            "Epoch [12/20], Step [7100/10000], Loss: 1.4425\n",
            "Epoch [12/20], Step [7200/10000], Loss: 0.8647\n",
            "Epoch [12/20], Step [7300/10000], Loss: 0.8633\n",
            "Epoch [12/20], Step [7400/10000], Loss: 1.9442\n",
            "Epoch [12/20], Step [7500/10000], Loss: 1.4316\n",
            "Epoch [12/20], Step [7600/10000], Loss: 1.1478\n",
            "Epoch [12/20], Step [7700/10000], Loss: 0.6803\n",
            "Epoch [12/20], Step [7800/10000], Loss: 1.1182\n",
            "Epoch [12/20], Step [7900/10000], Loss: 0.3464\n",
            "Epoch [12/20], Step [8000/10000], Loss: 0.9564\n",
            "Epoch [12/20], Step [8100/10000], Loss: 0.8849\n",
            "Epoch [12/20], Step [8200/10000], Loss: 1.3587\n",
            "Epoch [12/20], Step [8300/10000], Loss: 2.3275\n",
            "Epoch [12/20], Step [8400/10000], Loss: 0.5293\n",
            "Epoch [12/20], Step [8500/10000], Loss: 0.6536\n",
            "Epoch [12/20], Step [8600/10000], Loss: 0.9968\n",
            "Epoch [12/20], Step [8700/10000], Loss: 1.0950\n",
            "Epoch [12/20], Step [8800/10000], Loss: 1.5056\n",
            "Epoch [12/20], Step [8900/10000], Loss: 0.7165\n",
            "Epoch [12/20], Step [9000/10000], Loss: 0.0803\n",
            "Epoch [12/20], Step [9100/10000], Loss: 1.3902\n",
            "Epoch [12/20], Step [9200/10000], Loss: 1.5462\n",
            "Epoch [12/20], Step [9300/10000], Loss: 0.5582\n",
            "Epoch [12/20], Step [9400/10000], Loss: 2.3701\n",
            "Epoch [12/20], Step [9500/10000], Loss: 0.5844\n",
            "Epoch [12/20], Step [9600/10000], Loss: 0.6873\n",
            "Epoch [12/20], Step [9700/10000], Loss: 2.0498\n",
            "Epoch [12/20], Step [9800/10000], Loss: 1.4138\n",
            "Epoch [12/20], Step [9900/10000], Loss: 1.3884\n",
            "Epoch [12/20], Step [10000/10000], Loss: 1.1241\n",
            "Epoch [13/20], Step [100/10000], Loss: 0.9113\n",
            "Epoch [13/20], Step [200/10000], Loss: 1.1860\n",
            "Epoch [13/20], Step [300/10000], Loss: 0.6276\n",
            "Epoch [13/20], Step [400/10000], Loss: 0.8828\n",
            "Epoch [13/20], Step [500/10000], Loss: 1.4166\n",
            "Epoch [13/20], Step [600/10000], Loss: 1.4749\n",
            "Epoch [13/20], Step [700/10000], Loss: 1.2201\n",
            "Epoch [13/20], Step [800/10000], Loss: 1.1079\n",
            "Epoch [13/20], Step [900/10000], Loss: 0.9063\n",
            "Epoch [13/20], Step [1000/10000], Loss: 1.5970\n",
            "Epoch [13/20], Step [1100/10000], Loss: 1.6407\n",
            "Epoch [13/20], Step [1200/10000], Loss: 0.4899\n",
            "Epoch [13/20], Step [1300/10000], Loss: 1.1375\n",
            "Epoch [13/20], Step [1400/10000], Loss: 0.8090\n",
            "Epoch [13/20], Step [1500/10000], Loss: 1.3599\n",
            "Epoch [13/20], Step [1600/10000], Loss: 1.0086\n",
            "Epoch [13/20], Step [1700/10000], Loss: 0.4310\n",
            "Epoch [13/20], Step [1800/10000], Loss: 1.5218\n",
            "Epoch [13/20], Step [1900/10000], Loss: 1.2664\n",
            "Epoch [13/20], Step [2000/10000], Loss: 0.7020\n",
            "Epoch [13/20], Step [2100/10000], Loss: 2.3109\n",
            "Epoch [13/20], Step [2200/10000], Loss: 0.7929\n",
            "Epoch [13/20], Step [2300/10000], Loss: 0.7666\n",
            "Epoch [13/20], Step [2400/10000], Loss: 1.1468\n",
            "Epoch [13/20], Step [2500/10000], Loss: 0.7434\n",
            "Epoch [13/20], Step [2600/10000], Loss: 0.4625\n",
            "Epoch [13/20], Step [2700/10000], Loss: 0.8536\n",
            "Epoch [13/20], Step [2800/10000], Loss: 1.3368\n",
            "Epoch [13/20], Step [2900/10000], Loss: 0.7104\n",
            "Epoch [13/20], Step [3000/10000], Loss: 1.8571\n",
            "Epoch [13/20], Step [3100/10000], Loss: 0.9756\n",
            "Epoch [13/20], Step [3200/10000], Loss: 0.7752\n",
            "Epoch [13/20], Step [3300/10000], Loss: 0.8626\n",
            "Epoch [13/20], Step [3400/10000], Loss: 1.2285\n",
            "Epoch [13/20], Step [3500/10000], Loss: 0.9617\n",
            "Epoch [13/20], Step [3600/10000], Loss: 0.9389\n",
            "Epoch [13/20], Step [3700/10000], Loss: 1.4717\n",
            "Epoch [13/20], Step [3800/10000], Loss: 1.7553\n",
            "Epoch [13/20], Step [3900/10000], Loss: 1.4282\n",
            "Epoch [13/20], Step [4000/10000], Loss: 1.3877\n",
            "Epoch [13/20], Step [4100/10000], Loss: 1.0373\n",
            "Epoch [13/20], Step [4200/10000], Loss: 1.2575\n",
            "Epoch [13/20], Step [4300/10000], Loss: 1.3870\n",
            "Epoch [13/20], Step [4400/10000], Loss: 0.9798\n",
            "Epoch [13/20], Step [4500/10000], Loss: 0.8413\n",
            "Epoch [13/20], Step [4600/10000], Loss: 0.1018\n",
            "Epoch [13/20], Step [4700/10000], Loss: 1.5740\n",
            "Epoch [13/20], Step [4800/10000], Loss: 0.5601\n",
            "Epoch [13/20], Step [4900/10000], Loss: 2.0658\n",
            "Epoch [13/20], Step [5000/10000], Loss: 0.7669\n",
            "Epoch [13/20], Step [5100/10000], Loss: 1.7898\n",
            "Epoch [13/20], Step [5200/10000], Loss: 0.8038\n",
            "Epoch [13/20], Step [5300/10000], Loss: 1.3014\n",
            "Epoch [13/20], Step [5400/10000], Loss: 1.2697\n",
            "Epoch [13/20], Step [5500/10000], Loss: 1.5418\n",
            "Epoch [13/20], Step [5600/10000], Loss: 2.0189\n",
            "Epoch [13/20], Step [5700/10000], Loss: 1.1516\n",
            "Epoch [13/20], Step [5800/10000], Loss: 0.5738\n",
            "Epoch [13/20], Step [5900/10000], Loss: 1.4150\n",
            "Epoch [13/20], Step [6000/10000], Loss: 0.9074\n",
            "Epoch [13/20], Step [6100/10000], Loss: 1.1270\n",
            "Epoch [13/20], Step [6200/10000], Loss: 1.0946\n",
            "Epoch [13/20], Step [6300/10000], Loss: 0.9328\n",
            "Epoch [13/20], Step [6400/10000], Loss: 1.1356\n",
            "Epoch [13/20], Step [6500/10000], Loss: 0.5966\n",
            "Epoch [13/20], Step [6600/10000], Loss: 0.8283\n",
            "Epoch [13/20], Step [6700/10000], Loss: 1.8652\n",
            "Epoch [13/20], Step [6800/10000], Loss: 1.5250\n",
            "Epoch [13/20], Step [6900/10000], Loss: 0.1651\n",
            "Epoch [13/20], Step [7000/10000], Loss: 1.1349\n",
            "Epoch [13/20], Step [7100/10000], Loss: 1.1560\n",
            "Epoch [13/20], Step [7200/10000], Loss: 1.1801\n",
            "Epoch [13/20], Step [7300/10000], Loss: 2.0269\n",
            "Epoch [13/20], Step [7400/10000], Loss: 1.7899\n",
            "Epoch [13/20], Step [7500/10000], Loss: 1.3465\n",
            "Epoch [13/20], Step [7600/10000], Loss: 1.0304\n",
            "Epoch [13/20], Step [7700/10000], Loss: 0.8546\n",
            "Epoch [13/20], Step [7800/10000], Loss: 0.3318\n",
            "Epoch [13/20], Step [7900/10000], Loss: 1.1037\n",
            "Epoch [13/20], Step [8000/10000], Loss: 0.7242\n",
            "Epoch [13/20], Step [8100/10000], Loss: 1.1585\n",
            "Epoch [13/20], Step [8200/10000], Loss: 2.1419\n",
            "Epoch [13/20], Step [8300/10000], Loss: 1.1048\n",
            "Epoch [13/20], Step [8400/10000], Loss: 0.9776\n",
            "Epoch [13/20], Step [8500/10000], Loss: 0.9925\n",
            "Epoch [13/20], Step [8600/10000], Loss: 1.8538\n",
            "Epoch [13/20], Step [8700/10000], Loss: 1.3859\n",
            "Epoch [13/20], Step [8800/10000], Loss: 1.6519\n",
            "Epoch [13/20], Step [8900/10000], Loss: 1.0987\n",
            "Epoch [13/20], Step [9000/10000], Loss: 1.4575\n",
            "Epoch [13/20], Step [9100/10000], Loss: 0.4815\n",
            "Epoch [13/20], Step [9200/10000], Loss: 1.4551\n",
            "Epoch [13/20], Step [9300/10000], Loss: 0.5026\n",
            "Epoch [13/20], Step [9400/10000], Loss: 1.3171\n",
            "Epoch [13/20], Step [9500/10000], Loss: 0.8569\n",
            "Epoch [13/20], Step [9600/10000], Loss: 1.5925\n",
            "Epoch [13/20], Step [9700/10000], Loss: 0.8254\n",
            "Epoch [13/20], Step [9800/10000], Loss: 1.3210\n",
            "Epoch [13/20], Step [9900/10000], Loss: 1.3582\n",
            "Epoch [13/20], Step [10000/10000], Loss: 1.0785\n",
            "Epoch [14/20], Step [100/10000], Loss: 0.6330\n",
            "Epoch [14/20], Step [200/10000], Loss: 0.9760\n",
            "Epoch [14/20], Step [300/10000], Loss: 1.5647\n",
            "Epoch [14/20], Step [400/10000], Loss: 1.5791\n",
            "Epoch [14/20], Step [500/10000], Loss: 0.7894\n",
            "Epoch [14/20], Step [600/10000], Loss: 0.5132\n",
            "Epoch [14/20], Step [700/10000], Loss: 1.1736\n",
            "Epoch [14/20], Step [800/10000], Loss: 1.1283\n",
            "Epoch [14/20], Step [900/10000], Loss: 1.8201\n",
            "Epoch [14/20], Step [1000/10000], Loss: 0.9348\n",
            "Epoch [14/20], Step [1100/10000], Loss: 1.8329\n",
            "Epoch [14/20], Step [1200/10000], Loss: 0.9486\n",
            "Epoch [14/20], Step [1300/10000], Loss: 1.0330\n",
            "Epoch [14/20], Step [1400/10000], Loss: 1.2622\n",
            "Epoch [14/20], Step [1500/10000], Loss: 1.3326\n",
            "Epoch [14/20], Step [1600/10000], Loss: 1.0252\n",
            "Epoch [14/20], Step [1700/10000], Loss: 1.8082\n",
            "Epoch [14/20], Step [1800/10000], Loss: 0.6639\n",
            "Epoch [14/20], Step [1900/10000], Loss: 0.4831\n",
            "Epoch [14/20], Step [2000/10000], Loss: 0.8887\n",
            "Epoch [14/20], Step [2100/10000], Loss: 0.8593\n",
            "Epoch [14/20], Step [2200/10000], Loss: 1.3032\n",
            "Epoch [14/20], Step [2300/10000], Loss: 1.1598\n",
            "Epoch [14/20], Step [2400/10000], Loss: 0.5841\n",
            "Epoch [14/20], Step [2500/10000], Loss: 1.0707\n",
            "Epoch [14/20], Step [2600/10000], Loss: 0.2205\n",
            "Epoch [14/20], Step [2700/10000], Loss: 0.5935\n",
            "Epoch [14/20], Step [2800/10000], Loss: 1.6106\n",
            "Epoch [14/20], Step [2900/10000], Loss: 1.3787\n",
            "Epoch [14/20], Step [3000/10000], Loss: 1.3429\n",
            "Epoch [14/20], Step [3100/10000], Loss: 2.3175\n",
            "Epoch [14/20], Step [3200/10000], Loss: 1.6614\n",
            "Epoch [14/20], Step [3300/10000], Loss: 0.6889\n",
            "Epoch [14/20], Step [3400/10000], Loss: 1.8759\n",
            "Epoch [14/20], Step [3500/10000], Loss: 0.8958\n",
            "Epoch [14/20], Step [3600/10000], Loss: 0.3896\n",
            "Epoch [14/20], Step [3700/10000], Loss: 1.9082\n",
            "Epoch [14/20], Step [3800/10000], Loss: 0.9851\n",
            "Epoch [14/20], Step [3900/10000], Loss: 0.7768\n",
            "Epoch [14/20], Step [4000/10000], Loss: 0.7295\n",
            "Epoch [14/20], Step [4100/10000], Loss: 1.4314\n",
            "Epoch [14/20], Step [4200/10000], Loss: 1.5083\n",
            "Epoch [14/20], Step [4300/10000], Loss: 1.6859\n",
            "Epoch [14/20], Step [4400/10000], Loss: 0.9980\n",
            "Epoch [14/20], Step [4500/10000], Loss: 0.2077\n",
            "Epoch [14/20], Step [4600/10000], Loss: 1.5924\n",
            "Epoch [14/20], Step [4700/10000], Loss: 1.2131\n",
            "Epoch [14/20], Step [4800/10000], Loss: 1.1835\n",
            "Epoch [14/20], Step [4900/10000], Loss: 1.3507\n",
            "Epoch [14/20], Step [5000/10000], Loss: 1.2285\n",
            "Epoch [14/20], Step [5100/10000], Loss: 0.8493\n",
            "Epoch [14/20], Step [5200/10000], Loss: 1.6216\n",
            "Epoch [14/20], Step [5300/10000], Loss: 0.7674\n",
            "Epoch [14/20], Step [5400/10000], Loss: 1.5626\n",
            "Epoch [14/20], Step [5500/10000], Loss: 0.8972\n",
            "Epoch [14/20], Step [5600/10000], Loss: 0.8667\n",
            "Epoch [14/20], Step [5700/10000], Loss: 0.8405\n",
            "Epoch [14/20], Step [5800/10000], Loss: 0.8176\n",
            "Epoch [14/20], Step [5900/10000], Loss: 0.4861\n",
            "Epoch [14/20], Step [6000/10000], Loss: 1.5286\n",
            "Epoch [14/20], Step [6100/10000], Loss: 1.2804\n",
            "Epoch [14/20], Step [6200/10000], Loss: 0.6111\n",
            "Epoch [14/20], Step [6300/10000], Loss: 0.6174\n",
            "Epoch [14/20], Step [6400/10000], Loss: 1.0621\n",
            "Epoch [14/20], Step [6500/10000], Loss: 1.2800\n",
            "Epoch [14/20], Step [6600/10000], Loss: 0.2342\n",
            "Epoch [14/20], Step [6700/10000], Loss: 0.7045\n",
            "Epoch [14/20], Step [6800/10000], Loss: 0.9681\n",
            "Epoch [14/20], Step [6900/10000], Loss: 0.7669\n",
            "Epoch [14/20], Step [7000/10000], Loss: 1.0747\n",
            "Epoch [14/20], Step [7100/10000], Loss: 2.0513\n",
            "Epoch [14/20], Step [7200/10000], Loss: 1.8543\n",
            "Epoch [14/20], Step [7300/10000], Loss: 1.2061\n",
            "Epoch [14/20], Step [7400/10000], Loss: 2.1746\n",
            "Epoch [14/20], Step [7500/10000], Loss: 1.0014\n",
            "Epoch [14/20], Step [7600/10000], Loss: 0.6643\n",
            "Epoch [14/20], Step [7700/10000], Loss: 0.8790\n",
            "Epoch [14/20], Step [7800/10000], Loss: 2.0408\n",
            "Epoch [14/20], Step [7900/10000], Loss: 0.4636\n",
            "Epoch [14/20], Step [8000/10000], Loss: 0.7695\n",
            "Epoch [14/20], Step [8100/10000], Loss: 1.6442\n",
            "Epoch [14/20], Step [8200/10000], Loss: 1.2046\n",
            "Epoch [14/20], Step [8300/10000], Loss: 1.1080\n",
            "Epoch [14/20], Step [8400/10000], Loss: 0.5843\n",
            "Epoch [14/20], Step [8500/10000], Loss: 1.1917\n",
            "Epoch [14/20], Step [8600/10000], Loss: 1.0109\n",
            "Epoch [14/20], Step [8700/10000], Loss: 0.6286\n",
            "Epoch [14/20], Step [8800/10000], Loss: 1.3311\n",
            "Epoch [14/20], Step [8900/10000], Loss: 1.5960\n",
            "Epoch [14/20], Step [9000/10000], Loss: 0.8275\n",
            "Epoch [14/20], Step [9100/10000], Loss: 0.4138\n",
            "Epoch [14/20], Step [9200/10000], Loss: 0.7269\n",
            "Epoch [14/20], Step [9300/10000], Loss: 0.4790\n",
            "Epoch [14/20], Step [9400/10000], Loss: 0.7693\n",
            "Epoch [14/20], Step [9500/10000], Loss: 1.7459\n",
            "Epoch [14/20], Step [9600/10000], Loss: 1.2158\n",
            "Epoch [14/20], Step [9700/10000], Loss: 0.9418\n",
            "Epoch [14/20], Step [9800/10000], Loss: 1.9339\n",
            "Epoch [14/20], Step [9900/10000], Loss: 1.9181\n",
            "Epoch [14/20], Step [10000/10000], Loss: 1.5750\n",
            "Epoch [15/20], Step [100/10000], Loss: 0.3350\n",
            "Epoch [15/20], Step [200/10000], Loss: 1.5355\n",
            "Epoch [15/20], Step [300/10000], Loss: 1.7055\n",
            "Epoch [15/20], Step [400/10000], Loss: 0.8544\n",
            "Epoch [15/20], Step [500/10000], Loss: 1.7804\n",
            "Epoch [15/20], Step [600/10000], Loss: 0.7509\n",
            "Epoch [15/20], Step [700/10000], Loss: 1.8893\n",
            "Epoch [15/20], Step [800/10000], Loss: 1.3290\n",
            "Epoch [15/20], Step [900/10000], Loss: 0.5426\n",
            "Epoch [15/20], Step [1000/10000], Loss: 1.7768\n",
            "Epoch [15/20], Step [1100/10000], Loss: 1.6914\n",
            "Epoch [15/20], Step [1200/10000], Loss: 1.4303\n",
            "Epoch [15/20], Step [1300/10000], Loss: 1.5917\n",
            "Epoch [15/20], Step [1400/10000], Loss: 0.3815\n",
            "Epoch [15/20], Step [1500/10000], Loss: 1.4272\n",
            "Epoch [15/20], Step [1600/10000], Loss: 1.8007\n",
            "Epoch [15/20], Step [1700/10000], Loss: 1.4382\n",
            "Epoch [15/20], Step [1800/10000], Loss: 1.1707\n",
            "Epoch [15/20], Step [1900/10000], Loss: 1.2268\n",
            "Epoch [15/20], Step [2000/10000], Loss: 2.0246\n",
            "Epoch [15/20], Step [2100/10000], Loss: 1.0709\n",
            "Epoch [15/20], Step [2200/10000], Loss: 0.9354\n",
            "Epoch [15/20], Step [2300/10000], Loss: 1.1550\n",
            "Epoch [15/20], Step [2400/10000], Loss: 1.1854\n",
            "Epoch [15/20], Step [2500/10000], Loss: 0.6161\n",
            "Epoch [15/20], Step [2600/10000], Loss: 0.9383\n",
            "Epoch [15/20], Step [2700/10000], Loss: 0.8016\n",
            "Epoch [15/20], Step [2800/10000], Loss: 1.2833\n",
            "Epoch [15/20], Step [2900/10000], Loss: 0.6701\n",
            "Epoch [15/20], Step [3000/10000], Loss: 1.1928\n",
            "Epoch [15/20], Step [3100/10000], Loss: 0.3932\n",
            "Epoch [15/20], Step [3200/10000], Loss: 0.7212\n",
            "Epoch [15/20], Step [3300/10000], Loss: 1.6935\n",
            "Epoch [15/20], Step [3400/10000], Loss: 1.0503\n",
            "Epoch [15/20], Step [3500/10000], Loss: 0.9203\n",
            "Epoch [15/20], Step [3600/10000], Loss: 0.8572\n",
            "Epoch [15/20], Step [3700/10000], Loss: 0.9652\n",
            "Epoch [15/20], Step [3800/10000], Loss: 0.8991\n",
            "Epoch [15/20], Step [3900/10000], Loss: 1.1540\n",
            "Epoch [15/20], Step [4000/10000], Loss: 0.7692\n",
            "Epoch [15/20], Step [4100/10000], Loss: 2.8152\n",
            "Epoch [15/20], Step [4200/10000], Loss: 1.6540\n",
            "Epoch [15/20], Step [4300/10000], Loss: 0.2335\n",
            "Epoch [15/20], Step [4400/10000], Loss: 1.1480\n",
            "Epoch [15/20], Step [4500/10000], Loss: 0.9763\n",
            "Epoch [15/20], Step [4600/10000], Loss: 1.7300\n",
            "Epoch [15/20], Step [4700/10000], Loss: 1.2976\n",
            "Epoch [15/20], Step [4800/10000], Loss: 1.1003\n",
            "Epoch [15/20], Step [4900/10000], Loss: 0.6492\n",
            "Epoch [15/20], Step [5000/10000], Loss: 1.2729\n",
            "Epoch [15/20], Step [5100/10000], Loss: 0.5421\n",
            "Epoch [15/20], Step [5200/10000], Loss: 1.3647\n",
            "Epoch [15/20], Step [5300/10000], Loss: 2.0682\n",
            "Epoch [15/20], Step [5400/10000], Loss: 0.5751\n",
            "Epoch [15/20], Step [5500/10000], Loss: 1.1947\n",
            "Epoch [15/20], Step [5600/10000], Loss: 2.2806\n",
            "Epoch [15/20], Step [5700/10000], Loss: 1.6624\n",
            "Epoch [15/20], Step [5800/10000], Loss: 1.5697\n",
            "Epoch [15/20], Step [5900/10000], Loss: 1.7846\n",
            "Epoch [15/20], Step [6000/10000], Loss: 0.3568\n",
            "Epoch [15/20], Step [6100/10000], Loss: 1.5676\n",
            "Epoch [15/20], Step [6200/10000], Loss: 2.4424\n",
            "Epoch [15/20], Step [6300/10000], Loss: 1.3758\n",
            "Epoch [15/20], Step [6400/10000], Loss: 1.7657\n",
            "Epoch [15/20], Step [6500/10000], Loss: 0.4598\n",
            "Epoch [15/20], Step [6600/10000], Loss: 1.5279\n",
            "Epoch [15/20], Step [6700/10000], Loss: 1.5912\n",
            "Epoch [15/20], Step [6800/10000], Loss: 0.4548\n",
            "Epoch [15/20], Step [6900/10000], Loss: 0.6664\n",
            "Epoch [15/20], Step [7000/10000], Loss: 2.1628\n",
            "Epoch [15/20], Step [7100/10000], Loss: 2.1164\n",
            "Epoch [15/20], Step [7200/10000], Loss: 1.5728\n",
            "Epoch [15/20], Step [7300/10000], Loss: 1.3144\n",
            "Epoch [15/20], Step [7400/10000], Loss: 1.5252\n",
            "Epoch [15/20], Step [7500/10000], Loss: 0.9107\n",
            "Epoch [15/20], Step [7600/10000], Loss: 0.6544\n",
            "Epoch [15/20], Step [7700/10000], Loss: 0.5525\n",
            "Epoch [15/20], Step [7800/10000], Loss: 0.4768\n",
            "Epoch [15/20], Step [7900/10000], Loss: 1.5988\n",
            "Epoch [15/20], Step [8000/10000], Loss: 0.4771\n",
            "Epoch [15/20], Step [8100/10000], Loss: 0.8902\n",
            "Epoch [15/20], Step [8200/10000], Loss: 0.7031\n",
            "Epoch [15/20], Step [8300/10000], Loss: 1.3408\n",
            "Epoch [15/20], Step [8400/10000], Loss: 1.0855\n",
            "Epoch [15/20], Step [8500/10000], Loss: 1.9388\n",
            "Epoch [15/20], Step [8600/10000], Loss: 0.2407\n",
            "Epoch [15/20], Step [8700/10000], Loss: 1.0993\n",
            "Epoch [15/20], Step [8800/10000], Loss: 1.3847\n",
            "Epoch [15/20], Step [8900/10000], Loss: 1.0967\n",
            "Epoch [15/20], Step [9000/10000], Loss: 0.7035\n",
            "Epoch [15/20], Step [9100/10000], Loss: 2.2295\n",
            "Epoch [15/20], Step [9200/10000], Loss: 0.6610\n",
            "Epoch [15/20], Step [9300/10000], Loss: 0.1982\n",
            "Epoch [15/20], Step [9400/10000], Loss: 1.5452\n",
            "Epoch [15/20], Step [9500/10000], Loss: 0.7557\n",
            "Epoch [15/20], Step [9600/10000], Loss: 1.1514\n",
            "Epoch [15/20], Step [9700/10000], Loss: 1.3500\n",
            "Epoch [15/20], Step [9800/10000], Loss: 0.7187\n",
            "Epoch [15/20], Step [9900/10000], Loss: 1.2457\n",
            "Epoch [15/20], Step [10000/10000], Loss: 0.7452\n",
            "Epoch [16/20], Step [100/10000], Loss: 1.3797\n",
            "Epoch [16/20], Step [200/10000], Loss: 1.8339\n",
            "Epoch [16/20], Step [300/10000], Loss: 1.0538\n",
            "Epoch [16/20], Step [400/10000], Loss: 0.8130\n",
            "Epoch [16/20], Step [500/10000], Loss: 0.4084\n",
            "Epoch [16/20], Step [600/10000], Loss: 0.7705\n",
            "Epoch [16/20], Step [700/10000], Loss: 1.8498\n",
            "Epoch [16/20], Step [800/10000], Loss: 1.0404\n",
            "Epoch [16/20], Step [900/10000], Loss: 1.8144\n",
            "Epoch [16/20], Step [1000/10000], Loss: 1.0834\n",
            "Epoch [16/20], Step [1100/10000], Loss: 0.5142\n",
            "Epoch [16/20], Step [1200/10000], Loss: 1.0338\n",
            "Epoch [16/20], Step [1300/10000], Loss: 0.5202\n",
            "Epoch [16/20], Step [1400/10000], Loss: 1.0176\n",
            "Epoch [16/20], Step [1500/10000], Loss: 1.6227\n",
            "Epoch [16/20], Step [1600/10000], Loss: 1.2173\n",
            "Epoch [16/20], Step [1700/10000], Loss: 0.5195\n",
            "Epoch [16/20], Step [1800/10000], Loss: 1.2919\n",
            "Epoch [16/20], Step [1900/10000], Loss: 0.9957\n",
            "Epoch [16/20], Step [2000/10000], Loss: 1.8020\n",
            "Epoch [16/20], Step [2100/10000], Loss: 0.6927\n",
            "Epoch [16/20], Step [2200/10000], Loss: 0.4230\n",
            "Epoch [16/20], Step [2300/10000], Loss: 1.5893\n",
            "Epoch [16/20], Step [2400/10000], Loss: 1.4560\n",
            "Epoch [16/20], Step [2500/10000], Loss: 1.0003\n",
            "Epoch [16/20], Step [2600/10000], Loss: 1.0268\n",
            "Epoch [16/20], Step [2700/10000], Loss: 1.4813\n",
            "Epoch [16/20], Step [2800/10000], Loss: 1.2723\n",
            "Epoch [16/20], Step [2900/10000], Loss: 2.1515\n",
            "Epoch [16/20], Step [3000/10000], Loss: 1.0204\n",
            "Epoch [16/20], Step [3100/10000], Loss: 0.8595\n",
            "Epoch [16/20], Step [3200/10000], Loss: 1.4467\n",
            "Epoch [16/20], Step [3300/10000], Loss: 1.5578\n",
            "Epoch [16/20], Step [3400/10000], Loss: 1.0481\n",
            "Epoch [16/20], Step [3500/10000], Loss: 0.9540\n",
            "Epoch [16/20], Step [3600/10000], Loss: 0.9654\n",
            "Epoch [16/20], Step [3700/10000], Loss: 1.2152\n",
            "Epoch [16/20], Step [3800/10000], Loss: 0.7756\n",
            "Epoch [16/20], Step [3900/10000], Loss: 0.8571\n",
            "Epoch [16/20], Step [4000/10000], Loss: 0.9449\n",
            "Epoch [16/20], Step [4100/10000], Loss: 2.0206\n",
            "Epoch [16/20], Step [4200/10000], Loss: 2.0431\n",
            "Epoch [16/20], Step [4300/10000], Loss: 1.2305\n",
            "Epoch [16/20], Step [4400/10000], Loss: 1.6811\n",
            "Epoch [16/20], Step [4500/10000], Loss: 0.7441\n",
            "Epoch [16/20], Step [4600/10000], Loss: 0.5664\n",
            "Epoch [16/20], Step [4700/10000], Loss: 1.4427\n",
            "Epoch [16/20], Step [4800/10000], Loss: 2.4181\n",
            "Epoch [16/20], Step [4900/10000], Loss: 0.7374\n",
            "Epoch [16/20], Step [5000/10000], Loss: 1.9540\n",
            "Epoch [16/20], Step [5100/10000], Loss: 0.8794\n",
            "Epoch [16/20], Step [5200/10000], Loss: 1.6863\n",
            "Epoch [16/20], Step [5300/10000], Loss: 0.2463\n",
            "Epoch [16/20], Step [5400/10000], Loss: 0.5838\n",
            "Epoch [16/20], Step [5500/10000], Loss: 1.3775\n",
            "Epoch [16/20], Step [5600/10000], Loss: 1.0803\n",
            "Epoch [16/20], Step [5700/10000], Loss: 1.2939\n",
            "Epoch [16/20], Step [5800/10000], Loss: 0.7419\n",
            "Epoch [16/20], Step [5900/10000], Loss: 0.5909\n",
            "Epoch [16/20], Step [6000/10000], Loss: 1.3345\n",
            "Epoch [16/20], Step [6100/10000], Loss: 0.6980\n",
            "Epoch [16/20], Step [6200/10000], Loss: 1.2821\n",
            "Epoch [16/20], Step [6300/10000], Loss: 0.8174\n",
            "Epoch [16/20], Step [6400/10000], Loss: 0.6847\n",
            "Epoch [16/20], Step [6500/10000], Loss: 1.6768\n",
            "Epoch [16/20], Step [6600/10000], Loss: 0.4868\n",
            "Epoch [16/20], Step [6700/10000], Loss: 2.6404\n",
            "Epoch [16/20], Step [6800/10000], Loss: 0.7568\n",
            "Epoch [16/20], Step [6900/10000], Loss: 0.2585\n",
            "Epoch [16/20], Step [7000/10000], Loss: 0.8046\n",
            "Epoch [16/20], Step [7100/10000], Loss: 0.4712\n",
            "Epoch [16/20], Step [7200/10000], Loss: 1.1423\n",
            "Epoch [16/20], Step [7300/10000], Loss: 0.7004\n",
            "Epoch [16/20], Step [7400/10000], Loss: 1.2110\n",
            "Epoch [16/20], Step [7500/10000], Loss: 0.6346\n",
            "Epoch [16/20], Step [7600/10000], Loss: 0.5931\n",
            "Epoch [16/20], Step [7700/10000], Loss: 0.6872\n",
            "Epoch [16/20], Step [7800/10000], Loss: 0.9104\n",
            "Epoch [16/20], Step [7900/10000], Loss: 1.9907\n",
            "Epoch [16/20], Step [8000/10000], Loss: 1.7266\n",
            "Epoch [16/20], Step [8100/10000], Loss: 1.0405\n",
            "Epoch [16/20], Step [8200/10000], Loss: 3.1685\n",
            "Epoch [16/20], Step [8300/10000], Loss: 1.3412\n",
            "Epoch [16/20], Step [8400/10000], Loss: 0.6164\n",
            "Epoch [16/20], Step [8500/10000], Loss: 1.5439\n",
            "Epoch [16/20], Step [8600/10000], Loss: 1.5343\n",
            "Epoch [16/20], Step [8700/10000], Loss: 0.3287\n",
            "Epoch [16/20], Step [8800/10000], Loss: 0.3796\n",
            "Epoch [16/20], Step [8900/10000], Loss: 2.1979\n",
            "Epoch [16/20], Step [9000/10000], Loss: 1.7542\n",
            "Epoch [16/20], Step [9100/10000], Loss: 0.7338\n",
            "Epoch [16/20], Step [9200/10000], Loss: 0.9161\n",
            "Epoch [16/20], Step [9300/10000], Loss: 0.4246\n",
            "Epoch [16/20], Step [9400/10000], Loss: 1.5249\n",
            "Epoch [16/20], Step [9500/10000], Loss: 0.8964\n",
            "Epoch [16/20], Step [9600/10000], Loss: 1.9470\n",
            "Epoch [16/20], Step [9700/10000], Loss: 0.4274\n",
            "Epoch [16/20], Step [9800/10000], Loss: 0.5790\n",
            "Epoch [16/20], Step [9900/10000], Loss: 0.3977\n",
            "Epoch [16/20], Step [10000/10000], Loss: 0.7037\n",
            "Epoch [17/20], Step [100/10000], Loss: 0.3965\n",
            "Epoch [17/20], Step [200/10000], Loss: 2.8815\n",
            "Epoch [17/20], Step [300/10000], Loss: 0.7619\n",
            "Epoch [17/20], Step [400/10000], Loss: 0.9685\n",
            "Epoch [17/20], Step [500/10000], Loss: 1.1581\n",
            "Epoch [17/20], Step [600/10000], Loss: 0.8129\n",
            "Epoch [17/20], Step [700/10000], Loss: 0.5627\n",
            "Epoch [17/20], Step [800/10000], Loss: 2.4841\n",
            "Epoch [17/20], Step [900/10000], Loss: 0.7560\n",
            "Epoch [17/20], Step [1000/10000], Loss: 1.5383\n",
            "Epoch [17/20], Step [1100/10000], Loss: 1.6742\n",
            "Epoch [17/20], Step [1200/10000], Loss: 0.9547\n",
            "Epoch [17/20], Step [1300/10000], Loss: 0.6135\n",
            "Epoch [17/20], Step [1400/10000], Loss: 1.1332\n",
            "Epoch [17/20], Step [1500/10000], Loss: 0.9194\n",
            "Epoch [17/20], Step [1600/10000], Loss: 1.0394\n",
            "Epoch [17/20], Step [1700/10000], Loss: 1.0729\n",
            "Epoch [17/20], Step [1800/10000], Loss: 1.2049\n",
            "Epoch [17/20], Step [1900/10000], Loss: 0.6592\n",
            "Epoch [17/20], Step [2000/10000], Loss: 1.5261\n",
            "Epoch [17/20], Step [2100/10000], Loss: 2.3673\n",
            "Epoch [17/20], Step [2200/10000], Loss: 1.1531\n",
            "Epoch [17/20], Step [2300/10000], Loss: 0.8548\n",
            "Epoch [17/20], Step [2400/10000], Loss: 1.0764\n",
            "Epoch [17/20], Step [2500/10000], Loss: 2.3298\n",
            "Epoch [17/20], Step [2600/10000], Loss: 0.9268\n",
            "Epoch [17/20], Step [2700/10000], Loss: 0.6050\n",
            "Epoch [17/20], Step [2800/10000], Loss: 1.6427\n",
            "Epoch [17/20], Step [2900/10000], Loss: 1.2776\n",
            "Epoch [17/20], Step [3000/10000], Loss: 0.8694\n",
            "Epoch [17/20], Step [3100/10000], Loss: 1.2167\n",
            "Epoch [17/20], Step [3200/10000], Loss: 0.5193\n",
            "Epoch [17/20], Step [3300/10000], Loss: 0.7025\n",
            "Epoch [17/20], Step [3400/10000], Loss: 1.1975\n",
            "Epoch [17/20], Step [3500/10000], Loss: 1.4462\n",
            "Epoch [17/20], Step [3600/10000], Loss: 0.5240\n",
            "Epoch [17/20], Step [3700/10000], Loss: 2.4168\n",
            "Epoch [17/20], Step [3800/10000], Loss: 2.2009\n",
            "Epoch [17/20], Step [3900/10000], Loss: 2.0163\n",
            "Epoch [17/20], Step [4000/10000], Loss: 0.6263\n",
            "Epoch [17/20], Step [4100/10000], Loss: 0.9067\n",
            "Epoch [17/20], Step [4200/10000], Loss: 1.4178\n",
            "Epoch [17/20], Step [4300/10000], Loss: 0.5660\n",
            "Epoch [17/20], Step [4400/10000], Loss: 0.6884\n",
            "Epoch [17/20], Step [4500/10000], Loss: 1.3263\n",
            "Epoch [17/20], Step [4600/10000], Loss: 1.0182\n",
            "Epoch [17/20], Step [4700/10000], Loss: 1.4826\n",
            "Epoch [17/20], Step [4800/10000], Loss: 0.8590\n",
            "Epoch [17/20], Step [4900/10000], Loss: 0.9572\n",
            "Epoch [17/20], Step [5000/10000], Loss: 1.7974\n",
            "Epoch [17/20], Step [5100/10000], Loss: 0.9985\n",
            "Epoch [17/20], Step [5200/10000], Loss: 1.9254\n",
            "Epoch [17/20], Step [5300/10000], Loss: 1.1085\n",
            "Epoch [17/20], Step [5400/10000], Loss: 1.0591\n",
            "Epoch [17/20], Step [5500/10000], Loss: 1.6238\n",
            "Epoch [17/20], Step [5600/10000], Loss: 1.9659\n",
            "Epoch [17/20], Step [5700/10000], Loss: 0.1296\n",
            "Epoch [17/20], Step [5800/10000], Loss: 1.5879\n",
            "Epoch [17/20], Step [5900/10000], Loss: 1.0496\n",
            "Epoch [17/20], Step [6000/10000], Loss: 0.8560\n",
            "Epoch [17/20], Step [6100/10000], Loss: 0.4927\n",
            "Epoch [17/20], Step [6200/10000], Loss: 0.6958\n",
            "Epoch [17/20], Step [6300/10000], Loss: 1.1948\n",
            "Epoch [17/20], Step [6400/10000], Loss: 1.8746\n",
            "Epoch [17/20], Step [6500/10000], Loss: 1.1155\n",
            "Epoch [17/20], Step [6600/10000], Loss: 1.1479\n",
            "Epoch [17/20], Step [6700/10000], Loss: 1.2158\n",
            "Epoch [17/20], Step [6800/10000], Loss: 0.5924\n",
            "Epoch [17/20], Step [6900/10000], Loss: 1.5977\n",
            "Epoch [17/20], Step [7000/10000], Loss: 1.4061\n",
            "Epoch [17/20], Step [7100/10000], Loss: 1.4670\n",
            "Epoch [17/20], Step [7200/10000], Loss: 2.6381\n",
            "Epoch [17/20], Step [7300/10000], Loss: 0.9665\n",
            "Epoch [17/20], Step [7400/10000], Loss: 0.9378\n",
            "Epoch [17/20], Step [7500/10000], Loss: 1.5183\n",
            "Epoch [17/20], Step [7600/10000], Loss: 1.7728\n",
            "Epoch [17/20], Step [7700/10000], Loss: 2.2261\n",
            "Epoch [17/20], Step [7800/10000], Loss: 1.4845\n",
            "Epoch [17/20], Step [7900/10000], Loss: 0.7840\n",
            "Epoch [17/20], Step [8000/10000], Loss: 1.0774\n",
            "Epoch [17/20], Step [8100/10000], Loss: 1.1341\n",
            "Epoch [17/20], Step [8200/10000], Loss: 0.9409\n",
            "Epoch [17/20], Step [8300/10000], Loss: 1.6484\n",
            "Epoch [17/20], Step [8400/10000], Loss: 1.1522\n",
            "Epoch [17/20], Step [8500/10000], Loss: 0.9880\n",
            "Epoch [17/20], Step [8600/10000], Loss: 1.2566\n",
            "Epoch [17/20], Step [8700/10000], Loss: 1.3988\n",
            "Epoch [17/20], Step [8800/10000], Loss: 1.4663\n",
            "Epoch [17/20], Step [8900/10000], Loss: 1.8892\n",
            "Epoch [17/20], Step [9000/10000], Loss: 2.5491\n",
            "Epoch [17/20], Step [9100/10000], Loss: 1.0903\n",
            "Epoch [17/20], Step [9200/10000], Loss: 0.0976\n",
            "Epoch [17/20], Step [9300/10000], Loss: 1.1838\n",
            "Epoch [17/20], Step [9400/10000], Loss: 1.3953\n",
            "Epoch [17/20], Step [9500/10000], Loss: 1.0956\n",
            "Epoch [17/20], Step [9600/10000], Loss: 1.1685\n",
            "Epoch [17/20], Step [9700/10000], Loss: 1.3726\n",
            "Epoch [17/20], Step [9800/10000], Loss: 1.8548\n",
            "Epoch [17/20], Step [9900/10000], Loss: 1.1067\n",
            "Epoch [17/20], Step [10000/10000], Loss: 1.5483\n",
            "Epoch [18/20], Step [100/10000], Loss: 0.6427\n",
            "Epoch [18/20], Step [200/10000], Loss: 1.0833\n",
            "Epoch [18/20], Step [300/10000], Loss: 1.3017\n",
            "Epoch [18/20], Step [400/10000], Loss: 1.4140\n",
            "Epoch [18/20], Step [500/10000], Loss: 1.2950\n",
            "Epoch [18/20], Step [600/10000], Loss: 1.7718\n",
            "Epoch [18/20], Step [700/10000], Loss: 1.1233\n",
            "Epoch [18/20], Step [800/10000], Loss: 1.8658\n",
            "Epoch [18/20], Step [900/10000], Loss: 0.7033\n",
            "Epoch [18/20], Step [1000/10000], Loss: 1.7058\n",
            "Epoch [18/20], Step [1100/10000], Loss: 0.5265\n",
            "Epoch [18/20], Step [1200/10000], Loss: 1.1467\n",
            "Epoch [18/20], Step [1300/10000], Loss: 0.9924\n",
            "Epoch [18/20], Step [1400/10000], Loss: 0.1952\n",
            "Epoch [18/20], Step [1500/10000], Loss: 0.9514\n",
            "Epoch [18/20], Step [1600/10000], Loss: 0.4576\n",
            "Epoch [18/20], Step [1700/10000], Loss: 0.2976\n",
            "Epoch [18/20], Step [1800/10000], Loss: 1.8698\n",
            "Epoch [18/20], Step [1900/10000], Loss: 0.9490\n",
            "Epoch [18/20], Step [2000/10000], Loss: 1.2071\n",
            "Epoch [18/20], Step [2100/10000], Loss: 1.4916\n",
            "Epoch [18/20], Step [2200/10000], Loss: 1.1057\n",
            "Epoch [18/20], Step [2300/10000], Loss: 0.7724\n",
            "Epoch [18/20], Step [2400/10000], Loss: 0.2760\n",
            "Epoch [18/20], Step [2500/10000], Loss: 0.9961\n",
            "Epoch [18/20], Step [2600/10000], Loss: 1.3053\n",
            "Epoch [18/20], Step [2700/10000], Loss: 0.6508\n",
            "Epoch [18/20], Step [2800/10000], Loss: 0.1602\n",
            "Epoch [18/20], Step [2900/10000], Loss: 1.0761\n",
            "Epoch [18/20], Step [3000/10000], Loss: 0.9091\n",
            "Epoch [18/20], Step [3100/10000], Loss: 0.5329\n",
            "Epoch [18/20], Step [3200/10000], Loss: 1.3325\n",
            "Epoch [18/20], Step [3300/10000], Loss: 0.4775\n",
            "Epoch [18/20], Step [3400/10000], Loss: 0.8862\n",
            "Epoch [18/20], Step [3500/10000], Loss: 1.0083\n",
            "Epoch [18/20], Step [3600/10000], Loss: 0.9294\n",
            "Epoch [18/20], Step [3700/10000], Loss: 0.8564\n",
            "Epoch [18/20], Step [3800/10000], Loss: 0.5280\n",
            "Epoch [18/20], Step [3900/10000], Loss: 0.5189\n",
            "Epoch [18/20], Step [4000/10000], Loss: 1.7957\n",
            "Epoch [18/20], Step [4100/10000], Loss: 2.0411\n",
            "Epoch [18/20], Step [4200/10000], Loss: 1.1792\n",
            "Epoch [18/20], Step [4300/10000], Loss: 1.1104\n",
            "Epoch [18/20], Step [4400/10000], Loss: 0.7624\n",
            "Epoch [18/20], Step [4500/10000], Loss: 1.1143\n",
            "Epoch [18/20], Step [4600/10000], Loss: 1.4812\n",
            "Epoch [18/20], Step [4700/10000], Loss: 1.6035\n",
            "Epoch [18/20], Step [4800/10000], Loss: 0.7620\n",
            "Epoch [18/20], Step [4900/10000], Loss: 0.9412\n",
            "Epoch [18/20], Step [5000/10000], Loss: 0.5395\n",
            "Epoch [18/20], Step [5100/10000], Loss: 1.3041\n",
            "Epoch [18/20], Step [5200/10000], Loss: 0.8264\n",
            "Epoch [18/20], Step [5300/10000], Loss: 2.0308\n",
            "Epoch [18/20], Step [5400/10000], Loss: 1.0647\n",
            "Epoch [18/20], Step [5500/10000], Loss: 1.0082\n",
            "Epoch [18/20], Step [5600/10000], Loss: 0.6516\n",
            "Epoch [18/20], Step [5700/10000], Loss: 0.7637\n",
            "Epoch [18/20], Step [5800/10000], Loss: 0.9283\n",
            "Epoch [18/20], Step [5900/10000], Loss: 0.7079\n",
            "Epoch [18/20], Step [6000/10000], Loss: 1.3624\n",
            "Epoch [18/20], Step [6100/10000], Loss: 0.3017\n",
            "Epoch [18/20], Step [6200/10000], Loss: 1.6450\n",
            "Epoch [18/20], Step [6300/10000], Loss: 1.2021\n",
            "Epoch [18/20], Step [6400/10000], Loss: 1.1000\n",
            "Epoch [18/20], Step [6500/10000], Loss: 2.1084\n",
            "Epoch [18/20], Step [6600/10000], Loss: 1.6550\n",
            "Epoch [18/20], Step [6700/10000], Loss: 0.9842\n",
            "Epoch [18/20], Step [6800/10000], Loss: 0.5538\n",
            "Epoch [18/20], Step [6900/10000], Loss: 1.6005\n",
            "Epoch [18/20], Step [7000/10000], Loss: 0.8099\n",
            "Epoch [18/20], Step [7100/10000], Loss: 1.1672\n",
            "Epoch [18/20], Step [7200/10000], Loss: 1.4382\n",
            "Epoch [18/20], Step [7300/10000], Loss: 1.1562\n",
            "Epoch [18/20], Step [7400/10000], Loss: 0.7728\n",
            "Epoch [18/20], Step [7500/10000], Loss: 0.9840\n",
            "Epoch [18/20], Step [7600/10000], Loss: 0.4838\n",
            "Epoch [18/20], Step [7700/10000], Loss: 1.7789\n",
            "Epoch [18/20], Step [7800/10000], Loss: 0.6816\n",
            "Epoch [18/20], Step [7900/10000], Loss: 0.8032\n",
            "Epoch [18/20], Step [8000/10000], Loss: 0.9510\n",
            "Epoch [18/20], Step [8100/10000], Loss: 0.7035\n",
            "Epoch [18/20], Step [8200/10000], Loss: 1.2829\n",
            "Epoch [18/20], Step [8300/10000], Loss: 1.4047\n",
            "Epoch [18/20], Step [8400/10000], Loss: 0.5716\n",
            "Epoch [18/20], Step [8500/10000], Loss: 1.5681\n",
            "Epoch [18/20], Step [8600/10000], Loss: 1.4150\n",
            "Epoch [18/20], Step [8700/10000], Loss: 0.9982\n",
            "Epoch [18/20], Step [8800/10000], Loss: 0.9750\n",
            "Epoch [18/20], Step [8900/10000], Loss: 1.1539\n",
            "Epoch [18/20], Step [9000/10000], Loss: 1.4695\n",
            "Epoch [18/20], Step [9100/10000], Loss: 0.5419\n",
            "Epoch [18/20], Step [9200/10000], Loss: 1.0719\n",
            "Epoch [18/20], Step [9300/10000], Loss: 0.9158\n",
            "Epoch [18/20], Step [9400/10000], Loss: 0.8165\n",
            "Epoch [18/20], Step [9500/10000], Loss: 2.2732\n",
            "Epoch [18/20], Step [9600/10000], Loss: 1.1332\n",
            "Epoch [18/20], Step [9700/10000], Loss: 0.4205\n",
            "Epoch [18/20], Step [9800/10000], Loss: 1.0995\n",
            "Epoch [18/20], Step [9900/10000], Loss: 0.7203\n",
            "Epoch [18/20], Step [10000/10000], Loss: 0.9056\n",
            "Epoch [19/20], Step [100/10000], Loss: 0.4147\n",
            "Epoch [19/20], Step [200/10000], Loss: 1.0474\n",
            "Epoch [19/20], Step [300/10000], Loss: 1.2398\n",
            "Epoch [19/20], Step [400/10000], Loss: 1.0266\n",
            "Epoch [19/20], Step [500/10000], Loss: 1.5697\n",
            "Epoch [19/20], Step [600/10000], Loss: 0.7807\n",
            "Epoch [19/20], Step [700/10000], Loss: 0.1733\n",
            "Epoch [19/20], Step [800/10000], Loss: 1.2093\n",
            "Epoch [19/20], Step [900/10000], Loss: 1.1677\n",
            "Epoch [19/20], Step [1000/10000], Loss: 0.6671\n",
            "Epoch [19/20], Step [1100/10000], Loss: 1.6800\n",
            "Epoch [19/20], Step [1200/10000], Loss: 1.3911\n",
            "Epoch [19/20], Step [1300/10000], Loss: 1.9152\n",
            "Epoch [19/20], Step [1400/10000], Loss: 0.3620\n",
            "Epoch [19/20], Step [1500/10000], Loss: 1.7262\n",
            "Epoch [19/20], Step [1600/10000], Loss: 0.9303\n",
            "Epoch [19/20], Step [1700/10000], Loss: 1.5127\n",
            "Epoch [19/20], Step [1800/10000], Loss: 1.3827\n",
            "Epoch [19/20], Step [1900/10000], Loss: 0.8271\n",
            "Epoch [19/20], Step [2000/10000], Loss: 0.5481\n",
            "Epoch [19/20], Step [2100/10000], Loss: 0.6499\n",
            "Epoch [19/20], Step [2200/10000], Loss: 1.1118\n",
            "Epoch [19/20], Step [2300/10000], Loss: 0.9883\n",
            "Epoch [19/20], Step [2400/10000], Loss: 2.0021\n",
            "Epoch [19/20], Step [2500/10000], Loss: 0.6971\n",
            "Epoch [19/20], Step [2600/10000], Loss: 0.7537\n",
            "Epoch [19/20], Step [2700/10000], Loss: 1.3768\n",
            "Epoch [19/20], Step [2800/10000], Loss: 1.1873\n",
            "Epoch [19/20], Step [2900/10000], Loss: 0.6678\n",
            "Epoch [19/20], Step [3000/10000], Loss: 1.2685\n",
            "Epoch [19/20], Step [3100/10000], Loss: 0.5471\n",
            "Epoch [19/20], Step [3200/10000], Loss: 0.8482\n",
            "Epoch [19/20], Step [3300/10000], Loss: 0.9022\n",
            "Epoch [19/20], Step [3400/10000], Loss: 0.3927\n",
            "Epoch [19/20], Step [3500/10000], Loss: 1.5520\n",
            "Epoch [19/20], Step [3600/10000], Loss: 0.2858\n",
            "Epoch [19/20], Step [3700/10000], Loss: 0.4193\n",
            "Epoch [19/20], Step [3800/10000], Loss: 0.5397\n",
            "Epoch [19/20], Step [3900/10000], Loss: 0.8866\n",
            "Epoch [19/20], Step [4000/10000], Loss: 2.6983\n",
            "Epoch [19/20], Step [4100/10000], Loss: 0.8228\n",
            "Epoch [19/20], Step [4200/10000], Loss: 0.6538\n",
            "Epoch [19/20], Step [4300/10000], Loss: 2.4888\n",
            "Epoch [19/20], Step [4400/10000], Loss: 1.0932\n",
            "Epoch [19/20], Step [4500/10000], Loss: 0.8699\n",
            "Epoch [19/20], Step [4600/10000], Loss: 0.8562\n",
            "Epoch [19/20], Step [4700/10000], Loss: 1.7277\n",
            "Epoch [19/20], Step [4800/10000], Loss: 1.5086\n",
            "Epoch [19/20], Step [4900/10000], Loss: 1.2676\n",
            "Epoch [19/20], Step [5000/10000], Loss: 1.4852\n",
            "Epoch [19/20], Step [5100/10000], Loss: 0.8129\n",
            "Epoch [19/20], Step [5200/10000], Loss: 1.3926\n",
            "Epoch [19/20], Step [5300/10000], Loss: 0.6254\n",
            "Epoch [19/20], Step [5400/10000], Loss: 0.8120\n",
            "Epoch [19/20], Step [5500/10000], Loss: 0.9187\n",
            "Epoch [19/20], Step [5600/10000], Loss: 1.2774\n",
            "Epoch [19/20], Step [5700/10000], Loss: 1.3004\n",
            "Epoch [19/20], Step [5800/10000], Loss: 0.9113\n",
            "Epoch [19/20], Step [5900/10000], Loss: 0.7472\n",
            "Epoch [19/20], Step [6000/10000], Loss: 0.6570\n",
            "Epoch [19/20], Step [6100/10000], Loss: 0.3140\n",
            "Epoch [19/20], Step [6200/10000], Loss: 1.4266\n",
            "Epoch [19/20], Step [6300/10000], Loss: 1.1104\n",
            "Epoch [19/20], Step [6400/10000], Loss: 1.6996\n",
            "Epoch [19/20], Step [6500/10000], Loss: 0.3128\n",
            "Epoch [19/20], Step [6600/10000], Loss: 0.9763\n",
            "Epoch [19/20], Step [6700/10000], Loss: 1.5356\n",
            "Epoch [19/20], Step [6800/10000], Loss: 1.2380\n",
            "Epoch [19/20], Step [6900/10000], Loss: 2.4336\n",
            "Epoch [19/20], Step [7000/10000], Loss: 0.9295\n",
            "Epoch [19/20], Step [7100/10000], Loss: 1.1545\n",
            "Epoch [19/20], Step [7200/10000], Loss: 1.1436\n",
            "Epoch [19/20], Step [7300/10000], Loss: 1.1622\n",
            "Epoch [19/20], Step [7400/10000], Loss: 1.3039\n",
            "Epoch [19/20], Step [7500/10000], Loss: 0.5812\n",
            "Epoch [19/20], Step [7600/10000], Loss: 0.8155\n",
            "Epoch [19/20], Step [7700/10000], Loss: 0.5108\n",
            "Epoch [19/20], Step [7800/10000], Loss: 1.2009\n",
            "Epoch [19/20], Step [7900/10000], Loss: 2.4219\n",
            "Epoch [19/20], Step [8000/10000], Loss: 1.6019\n",
            "Epoch [19/20], Step [8100/10000], Loss: 0.2008\n",
            "Epoch [19/20], Step [8200/10000], Loss: 0.3536\n",
            "Epoch [19/20], Step [8300/10000], Loss: 0.4816\n",
            "Epoch [19/20], Step [8400/10000], Loss: 1.8866\n",
            "Epoch [19/20], Step [8500/10000], Loss: 1.7514\n",
            "Epoch [19/20], Step [8600/10000], Loss: 1.0870\n",
            "Epoch [19/20], Step [8700/10000], Loss: 0.5546\n",
            "Epoch [19/20], Step [8800/10000], Loss: 1.0398\n",
            "Epoch [19/20], Step [8900/10000], Loss: 1.0105\n",
            "Epoch [19/20], Step [9000/10000], Loss: 0.9463\n",
            "Epoch [19/20], Step [9100/10000], Loss: 1.7754\n",
            "Epoch [19/20], Step [9200/10000], Loss: 1.1366\n",
            "Epoch [19/20], Step [9300/10000], Loss: 0.7828\n",
            "Epoch [19/20], Step [9400/10000], Loss: 0.4442\n",
            "Epoch [19/20], Step [9500/10000], Loss: 0.6154\n",
            "Epoch [19/20], Step [9600/10000], Loss: 1.4161\n",
            "Epoch [19/20], Step [9700/10000], Loss: 1.4489\n",
            "Epoch [19/20], Step [9800/10000], Loss: 1.7480\n",
            "Epoch [19/20], Step [9900/10000], Loss: 1.4385\n",
            "Epoch [19/20], Step [10000/10000], Loss: 1.6791\n",
            "Epoch [20/20], Step [100/10000], Loss: 0.9368\n",
            "Epoch [20/20], Step [200/10000], Loss: 1.6510\n",
            "Epoch [20/20], Step [300/10000], Loss: 1.1875\n",
            "Epoch [20/20], Step [400/10000], Loss: 1.2657\n",
            "Epoch [20/20], Step [500/10000], Loss: 0.2148\n",
            "Epoch [20/20], Step [600/10000], Loss: 1.0938\n",
            "Epoch [20/20], Step [700/10000], Loss: 1.4114\n",
            "Epoch [20/20], Step [800/10000], Loss: 1.1863\n",
            "Epoch [20/20], Step [900/10000], Loss: 0.7473\n",
            "Epoch [20/20], Step [1000/10000], Loss: 1.0302\n",
            "Epoch [20/20], Step [1100/10000], Loss: 1.1192\n",
            "Epoch [20/20], Step [1200/10000], Loss: 0.5671\n",
            "Epoch [20/20], Step [1300/10000], Loss: 0.3295\n",
            "Epoch [20/20], Step [1400/10000], Loss: 1.3895\n",
            "Epoch [20/20], Step [1500/10000], Loss: 1.5554\n",
            "Epoch [20/20], Step [1600/10000], Loss: 1.0299\n",
            "Epoch [20/20], Step [1700/10000], Loss: 1.0200\n",
            "Epoch [20/20], Step [1800/10000], Loss: 0.7421\n",
            "Epoch [20/20], Step [1900/10000], Loss: 1.6833\n",
            "Epoch [20/20], Step [2000/10000], Loss: 0.8005\n",
            "Epoch [20/20], Step [2100/10000], Loss: 0.4314\n",
            "Epoch [20/20], Step [2200/10000], Loss: 0.8198\n",
            "Epoch [20/20], Step [2300/10000], Loss: 0.5576\n",
            "Epoch [20/20], Step [2400/10000], Loss: 0.7972\n",
            "Epoch [20/20], Step [2500/10000], Loss: 0.8512\n",
            "Epoch [20/20], Step [2600/10000], Loss: 1.1606\n",
            "Epoch [20/20], Step [2700/10000], Loss: 0.1181\n",
            "Epoch [20/20], Step [2800/10000], Loss: 0.9995\n",
            "Epoch [20/20], Step [2900/10000], Loss: 1.6053\n",
            "Epoch [20/20], Step [3000/10000], Loss: 0.6456\n",
            "Epoch [20/20], Step [3100/10000], Loss: 1.1140\n",
            "Epoch [20/20], Step [3200/10000], Loss: 1.3658\n",
            "Epoch [20/20], Step [3300/10000], Loss: 0.3503\n",
            "Epoch [20/20], Step [3400/10000], Loss: 0.7940\n",
            "Epoch [20/20], Step [3500/10000], Loss: 0.7864\n",
            "Epoch [20/20], Step [3600/10000], Loss: 1.7724\n",
            "Epoch [20/20], Step [3700/10000], Loss: 2.0681\n",
            "Epoch [20/20], Step [3800/10000], Loss: 1.4785\n",
            "Epoch [20/20], Step [3900/10000], Loss: 0.6828\n",
            "Epoch [20/20], Step [4000/10000], Loss: 0.6631\n",
            "Epoch [20/20], Step [4100/10000], Loss: 0.8330\n",
            "Epoch [20/20], Step [4200/10000], Loss: 1.4560\n",
            "Epoch [20/20], Step [4300/10000], Loss: 1.6606\n",
            "Epoch [20/20], Step [4400/10000], Loss: 1.2018\n",
            "Epoch [20/20], Step [4500/10000], Loss: 0.5092\n",
            "Epoch [20/20], Step [4600/10000], Loss: 1.7513\n",
            "Epoch [20/20], Step [4700/10000], Loss: 1.2874\n",
            "Epoch [20/20], Step [4800/10000], Loss: 0.9829\n",
            "Epoch [20/20], Step [4900/10000], Loss: 0.8969\n",
            "Epoch [20/20], Step [5000/10000], Loss: 1.9428\n",
            "Epoch [20/20], Step [5100/10000], Loss: 0.6213\n",
            "Epoch [20/20], Step [5200/10000], Loss: 0.8406\n",
            "Epoch [20/20], Step [5300/10000], Loss: 2.0179\n",
            "Epoch [20/20], Step [5400/10000], Loss: 0.7789\n",
            "Epoch [20/20], Step [5500/10000], Loss: 2.4928\n",
            "Epoch [20/20], Step [5600/10000], Loss: 0.6704\n",
            "Epoch [20/20], Step [5700/10000], Loss: 1.2635\n",
            "Epoch [20/20], Step [5800/10000], Loss: 1.2415\n",
            "Epoch [20/20], Step [5900/10000], Loss: 1.7446\n",
            "Epoch [20/20], Step [6000/10000], Loss: 1.0639\n",
            "Epoch [20/20], Step [6100/10000], Loss: 1.2117\n",
            "Epoch [20/20], Step [6200/10000], Loss: 1.3283\n",
            "Epoch [20/20], Step [6300/10000], Loss: 0.6852\n",
            "Epoch [20/20], Step [6400/10000], Loss: 1.0944\n",
            "Epoch [20/20], Step [6500/10000], Loss: 1.0076\n",
            "Epoch [20/20], Step [6600/10000], Loss: 1.1173\n",
            "Epoch [20/20], Step [6700/10000], Loss: 1.1963\n",
            "Epoch [20/20], Step [6800/10000], Loss: 1.4386\n",
            "Epoch [20/20], Step [6900/10000], Loss: 1.1570\n",
            "Epoch [20/20], Step [7000/10000], Loss: 0.5324\n",
            "Epoch [20/20], Step [7100/10000], Loss: 1.1838\n",
            "Epoch [20/20], Step [7200/10000], Loss: 1.3203\n",
            "Epoch [20/20], Step [7300/10000], Loss: 0.8391\n",
            "Epoch [20/20], Step [7400/10000], Loss: 0.3092\n",
            "Epoch [20/20], Step [7500/10000], Loss: 0.9812\n",
            "Epoch [20/20], Step [7600/10000], Loss: 1.5276\n",
            "Epoch [20/20], Step [7700/10000], Loss: 1.5703\n",
            "Epoch [20/20], Step [7800/10000], Loss: 0.5598\n",
            "Epoch [20/20], Step [7900/10000], Loss: 1.0065\n",
            "Epoch [20/20], Step [8000/10000], Loss: 1.5002\n",
            "Epoch [20/20], Step [8100/10000], Loss: 1.7411\n",
            "Epoch [20/20], Step [8200/10000], Loss: 2.2612\n",
            "Epoch [20/20], Step [8300/10000], Loss: 0.2665\n",
            "Epoch [20/20], Step [8400/10000], Loss: 0.4329\n",
            "Epoch [20/20], Step [8500/10000], Loss: 1.2598\n",
            "Epoch [20/20], Step [8600/10000], Loss: 1.5326\n",
            "Epoch [20/20], Step [8700/10000], Loss: 0.3880\n",
            "Epoch [20/20], Step [8800/10000], Loss: 0.6087\n",
            "Epoch [20/20], Step [8900/10000], Loss: 1.5919\n",
            "Epoch [20/20], Step [9000/10000], Loss: 1.3514\n",
            "Epoch [20/20], Step [9100/10000], Loss: 1.4793\n",
            "Epoch [20/20], Step [9200/10000], Loss: 2.3322\n",
            "Epoch [20/20], Step [9300/10000], Loss: 0.4134\n",
            "Epoch [20/20], Step [9400/10000], Loss: 2.4817\n",
            "Epoch [20/20], Step [9500/10000], Loss: 1.9088\n",
            "Epoch [20/20], Step [9600/10000], Loss: 0.7132\n",
            "Epoch [20/20], Step [9700/10000], Loss: 0.9791\n",
            "Epoch [20/20], Step [9800/10000], Loss: 2.2023\n",
            "Epoch [20/20], Step [9900/10000], Loss: 0.2994\n",
            "Epoch [20/20], Step [10000/10000], Loss: 1.2782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置为评估模式\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqgepKuunHH2",
        "outputId": "577bdcba-2308-4774-9883-dcd8d15e92b8"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvNet(\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv2d(3, 5, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv2d(5, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv3): Sequential(\n",
              "    (0): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv4): Sequential(\n",
              "    (0): Conv2d(16, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv5): Sequential(\n",
              "    (0): Conv2d(24, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=1568, out_features=120, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=120, out_features=84, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=84, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 输出测试集精度\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU9l2rwJnRgT",
        "outputId": "f3208a4b-7385-4a4a-8dcb-e4f94e789c16"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the model on the test images: 66.89 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  保存模型\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "metadata": {
        "id": "f1oRlNU4ndmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 可视化数据查看\n",
        "# 查看数据,取一组batch\n",
        "data_iter = iter(test_loader)\n",
        "images, labels = next(data_iter)\n",
        "# 取batch中的一张图像，显示图像和真实标签\n",
        "idx = 4\n",
        "image = images[idx].numpy()\n",
        "image = np.transpose(image, (1,2,0))\n",
        "plt.imshow(image)\n",
        "print('true:',classes[labels[idx].numpy()])\n",
        "# 转换为（B,C,H,W）大小\n",
        "imagebatch = image.reshape(-1,3,32,32)\n",
        "\n",
        "# 转换为torch tensor\n",
        "image_tensor = torch.from_numpy(imagebatch)\n",
        "image_tensor = image_tensor.cuda()\n",
        "# 调用模型进行评估\n",
        "model.eval()\n",
        "output = model(image_tensor)\n",
        "precise, predicted = torch.max(output.data, 1)\n",
        "pre = predicted.cpu().numpy()\n",
        "pci = precise.cpu().numpy()\n",
        "print(pre) # 查看预测结果ID\n",
        "print('result:',classes[pre[0]])\n",
        "#print(pci[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "OLPAo19Dn2kL",
        "outputId": "3d35e21b-16dc-46d1-9980-ed63f8170f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true: frog\n",
            "[8]\n",
            "result: ship\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}