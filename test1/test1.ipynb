{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHO9ZcicUtvv",
        "outputId": "13d16168-9638-4bf6-a0f0-ba8484ef55c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jul 14 05:01:26 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0    32W /  70W |   1171MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "# Importing drive method from colab for accessing google drive\n",
        "from google.colab import drive\n",
        "# Mounting drive\n",
        "# This will require authentication : Follow the steps as guided\n",
        "drive.mount('/Data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP7T3VlxWiCp",
        "outputId": "e39c720b-f26d-4aff-b6ac-f2588afa8835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /Data; to attempt to forcibly remount, call drive.mount(\"/Data\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /Data/'My Drive'/dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBSdroL7fSA8",
        "outputId": "f3a1dae5-9c5a-49fc-af77-967ee5f35bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " cifar-10-batches-py   cifar-10-python.tar.gz  'LeGO-LOAM Public Dataset'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "import gradio as gr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ9X6NLculhz",
        "outputId": "f82db373-5be8-498b-ba3b-246111d35915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (3.36.1)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.10/dist-packages (from gradio) (23.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from gradio) (3.8.4)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.100.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.0)\n",
            "Requirement already satisfied: gradio-client>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.2.9)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.16.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gradio) (1.22.4)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.10/dist-packages (from gradio) (3.9.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio) (8.4.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.11)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.14.0)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gradio) (2.27.1)\n",
            "Requirement already satisfied: semantic-version in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.22.0)\n",
            "Requirement already satisfied: websockets>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (2023.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (23.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.65.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2022.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.4)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.27.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (2023.5.7)\n",
            "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (0.17.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (3.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gradio) (1.26.16)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio) (1.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->gradio) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio) (1.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt # plt 用于显示图片\n",
        "import matplotlib.image as mpimg # mpimg 用于读取图片\n",
        "import numpy as np\n",
        "\n",
        "#resize功能\n",
        "from scipy import misc\n",
        "from PIL import Image\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "num_classes = 10\n",
        "# 超参数设置\n",
        "num_epochs = 100\n",
        "batch_size = 5\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "y9tms4NwcMFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "# 数据预处理\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "    ])"
      ],
      "metadata": {
        "id": "TxoYrdwKits0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOAD_CIFAR = True\n",
        "DOWNLOAD_CIFAR = False\n",
        "# 从data继承读取数据集的类\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 训练数据集\n",
        "train_data = torchvision.datasets.CIFAR10(\n",
        "    root='/Data/My Drive/dataset',\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=DOWNLOAD_CIFAR,\n",
        ")\n",
        "\n",
        "# 测试数据集\n",
        "test_data = torchvision.datasets.CIFAR10(\n",
        "    root='/Data/My Drive/dataset',\n",
        "    train=False,\n",
        "    transform=transform\n",
        ")"
      ],
      "metadata": {
        "id": "c0IVXuIfcbtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练数据加载器\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "# 测试数据加载器\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "UJDkPPJiBbqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 查看数据,取一组batch\n",
        "data_iter = iter(test_loader)\n",
        "\n",
        "images, labels = next(data_iter)\n",
        "# 取batch中的一张图像\n",
        "idx = 2\n",
        "image = images[idx].numpy()\n",
        "image = np.transpose(image, (1,2,0))\n",
        "plt.imshow(image)\n",
        "print(classes[labels[idx].numpy()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "OqUP_tXQjYk_",
        "outputId": "f93ce4a1-afe1-489a-856a-4ff2fdca8716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ship\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 搭建卷积神经网络模型\n",
        "# 三个卷积层\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "            # 32*32*3\n",
        "        self.conv1 = nn.Sequential(\n",
        "            # 卷积层计算\n",
        "            nn.Conv2d(3, 5, kernel_size=5, stride=1, padding=2),\n",
        "            #  批归一化\n",
        "            nn.BatchNorm2d(5),\n",
        "            #ReLU激活函数\n",
        "            nn.ReLU(),\n",
        "            # 池化层：最大池化\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1))\n",
        "            # 31*31*5\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(5, 8, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1))# 搭建卷积神经网络模型\n",
        "            # 30*30*8\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(8, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))# 搭建卷积神经网络模型\n",
        "            # 15*15*16\n",
        "\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(16, 24, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1))# 搭建卷积神经网络模型\n",
        "            # 14*14*24\n",
        "\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(24, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))# 搭建卷积神经网络模型\n",
        "            # 7*7*32\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(7*7*32, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(400, 120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, num_classes),\n",
        "        )\n",
        "\n",
        "    # 定义前向传播顺序\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.conv5(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "5aS7E2BBkpLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 实例化一个模型\n",
        "model = ConvNet(num_classes)"
      ],
      "metadata": {
        "id": "VK07kYOHk2Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义损失函数和优化器\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# 自动调整学习率\n",
        "#scheduler = StepLR(optimizer, step_size=10, gamma=0.9)"
      ],
      "metadata": {
        "id": "2qUHkTdhksvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置cuda-gpu\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTuV8mucco2z",
        "outputId": "3de349ad-e3eb-4128-db37-d3cbb2336093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Fri Jul 14 05:01:35 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0    32W /  70W |   1171MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 开始训练\n",
        "model = model.cuda()\n",
        "# 存储损失与精度\n",
        "loss_history = []\n",
        "accuracy_history = []\n",
        "loss_times_history = []\n",
        "\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        # 前向传播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 反向传播和优化\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #累计损失\n",
        "        running_loss += loss.item()\n",
        "        loss_times_history.append(loss.item())\n",
        "        # 计算准确率\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        if (i+1) % 1000 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "    #scheduler.step()\n",
        "    #计算各epoch中的平均损失值和准确率\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100.0 * correct_train / total_train\n",
        "    #存储平均损失值和准确率\n",
        "    loss_history.append(avg_loss)\n",
        "    accuracy_history.append(accuracy)\n",
        "    #输出学习率\n",
        "    #print(f\"Epoch [{epoch+1}/{num_epochs}], Learning Rate: {scheduler.get_lr()[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbRva835lY5b",
        "outputId": "a9aff043-8349-4236-b104-172e90a01e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Step [1000/10000], Loss: 1.3494\n",
            "Epoch [1/100], Step [2000/10000], Loss: 1.3223\n",
            "Epoch [1/100], Step [3000/10000], Loss: 1.3823\n",
            "Epoch [1/100], Step [4000/10000], Loss: 1.5643\n",
            "Epoch [1/100], Step [5000/10000], Loss: 1.2320\n",
            "Epoch [1/100], Step [6000/10000], Loss: 1.1237\n",
            "Epoch [1/100], Step [7000/10000], Loss: 1.3282\n",
            "Epoch [1/100], Step [8000/10000], Loss: 2.0840\n",
            "Epoch [1/100], Step [9000/10000], Loss: 1.7427\n",
            "Epoch [1/100], Step [10000/10000], Loss: 1.4063\n",
            "Epoch [2/100], Step [1000/10000], Loss: 1.5620\n",
            "Epoch [2/100], Step [2000/10000], Loss: 2.1181\n",
            "Epoch [2/100], Step [3000/10000], Loss: 1.1890\n",
            "Epoch [2/100], Step [4000/10000], Loss: 1.5455\n",
            "Epoch [2/100], Step [5000/10000], Loss: 1.3084\n",
            "Epoch [2/100], Step [6000/10000], Loss: 1.5834\n",
            "Epoch [2/100], Step [7000/10000], Loss: 0.8817\n",
            "Epoch [2/100], Step [8000/10000], Loss: 1.1666\n",
            "Epoch [2/100], Step [9000/10000], Loss: 1.9252\n",
            "Epoch [2/100], Step [10000/10000], Loss: 1.5154\n",
            "Epoch [3/100], Step [1000/10000], Loss: 1.5563\n",
            "Epoch [3/100], Step [2000/10000], Loss: 1.3087\n",
            "Epoch [3/100], Step [3000/10000], Loss: 1.3565\n",
            "Epoch [3/100], Step [4000/10000], Loss: 1.2874\n",
            "Epoch [3/100], Step [5000/10000], Loss: 0.7231\n",
            "Epoch [3/100], Step [6000/10000], Loss: 3.5922\n",
            "Epoch [3/100], Step [7000/10000], Loss: 0.3462\n",
            "Epoch [3/100], Step [8000/10000], Loss: 1.1852\n",
            "Epoch [3/100], Step [9000/10000], Loss: 1.4569\n",
            "Epoch [3/100], Step [10000/10000], Loss: 1.1968\n",
            "Epoch [4/100], Step [1000/10000], Loss: 1.5620\n",
            "Epoch [4/100], Step [2000/10000], Loss: 1.9589\n",
            "Epoch [4/100], Step [3000/10000], Loss: 0.5808\n",
            "Epoch [4/100], Step [4000/10000], Loss: 0.7671\n",
            "Epoch [4/100], Step [5000/10000], Loss: 0.6779\n",
            "Epoch [4/100], Step [6000/10000], Loss: 0.8096\n",
            "Epoch [4/100], Step [7000/10000], Loss: 2.0866\n",
            "Epoch [4/100], Step [8000/10000], Loss: 0.8777\n",
            "Epoch [4/100], Step [9000/10000], Loss: 1.2369\n",
            "Epoch [4/100], Step [10000/10000], Loss: 1.7347\n",
            "Epoch [5/100], Step [1000/10000], Loss: 1.9792\n",
            "Epoch [5/100], Step [2000/10000], Loss: 1.2119\n",
            "Epoch [5/100], Step [3000/10000], Loss: 1.0481\n",
            "Epoch [5/100], Step [4000/10000], Loss: 1.0943\n",
            "Epoch [5/100], Step [5000/10000], Loss: 1.3490\n",
            "Epoch [5/100], Step [6000/10000], Loss: 1.6116\n",
            "Epoch [5/100], Step [7000/10000], Loss: 1.1905\n",
            "Epoch [5/100], Step [8000/10000], Loss: 1.0858\n",
            "Epoch [5/100], Step [9000/10000], Loss: 0.4375\n",
            "Epoch [5/100], Step [10000/10000], Loss: 0.3593\n",
            "Epoch [6/100], Step [1000/10000], Loss: 0.6299\n",
            "Epoch [6/100], Step [2000/10000], Loss: 1.3142\n",
            "Epoch [6/100], Step [3000/10000], Loss: 0.9477\n",
            "Epoch [6/100], Step [4000/10000], Loss: 0.5329\n",
            "Epoch [6/100], Step [5000/10000], Loss: 1.1197\n",
            "Epoch [6/100], Step [6000/10000], Loss: 0.4940\n",
            "Epoch [6/100], Step [7000/10000], Loss: 1.0125\n",
            "Epoch [6/100], Step [8000/10000], Loss: 0.7109\n",
            "Epoch [6/100], Step [9000/10000], Loss: 0.5463\n",
            "Epoch [6/100], Step [10000/10000], Loss: 1.2301\n",
            "Epoch [7/100], Step [1000/10000], Loss: 0.8516\n",
            "Epoch [7/100], Step [2000/10000], Loss: 0.7668\n",
            "Epoch [7/100], Step [3000/10000], Loss: 1.4555\n",
            "Epoch [7/100], Step [4000/10000], Loss: 1.6091\n",
            "Epoch [7/100], Step [5000/10000], Loss: 1.9738\n",
            "Epoch [7/100], Step [6000/10000], Loss: 1.3375\n",
            "Epoch [7/100], Step [7000/10000], Loss: 0.8035\n",
            "Epoch [7/100], Step [8000/10000], Loss: 1.2293\n",
            "Epoch [7/100], Step [9000/10000], Loss: 1.2158\n",
            "Epoch [7/100], Step [10000/10000], Loss: 1.0930\n",
            "Epoch [8/100], Step [1000/10000], Loss: 1.8324\n",
            "Epoch [8/100], Step [2000/10000], Loss: 1.0792\n",
            "Epoch [8/100], Step [3000/10000], Loss: 0.6415\n",
            "Epoch [8/100], Step [4000/10000], Loss: 0.7858\n",
            "Epoch [8/100], Step [5000/10000], Loss: 0.2886\n",
            "Epoch [8/100], Step [6000/10000], Loss: 0.4093\n",
            "Epoch [8/100], Step [7000/10000], Loss: 1.0792\n",
            "Epoch [8/100], Step [8000/10000], Loss: 0.7146\n",
            "Epoch [8/100], Step [9000/10000], Loss: 1.2181\n",
            "Epoch [8/100], Step [10000/10000], Loss: 0.8353\n",
            "Epoch [9/100], Step [1000/10000], Loss: 0.8221\n",
            "Epoch [9/100], Step [2000/10000], Loss: 0.8607\n",
            "Epoch [9/100], Step [3000/10000], Loss: 1.3603\n",
            "Epoch [9/100], Step [4000/10000], Loss: 1.0043\n",
            "Epoch [9/100], Step [5000/10000], Loss: 0.8696\n",
            "Epoch [9/100], Step [6000/10000], Loss: 0.9141\n",
            "Epoch [9/100], Step [7000/10000], Loss: 0.9278\n",
            "Epoch [9/100], Step [8000/10000], Loss: 1.6302\n",
            "Epoch [9/100], Step [9000/10000], Loss: 1.9385\n",
            "Epoch [9/100], Step [10000/10000], Loss: 0.7807\n",
            "Epoch [10/100], Step [1000/10000], Loss: 2.4654\n",
            "Epoch [10/100], Step [2000/10000], Loss: 0.2734\n",
            "Epoch [10/100], Step [3000/10000], Loss: 1.4565\n",
            "Epoch [10/100], Step [4000/10000], Loss: 1.1964\n",
            "Epoch [10/100], Step [5000/10000], Loss: 0.4607\n",
            "Epoch [10/100], Step [6000/10000], Loss: 1.8199\n",
            "Epoch [10/100], Step [7000/10000], Loss: 0.5411\n",
            "Epoch [10/100], Step [8000/10000], Loss: 0.3797\n",
            "Epoch [10/100], Step [9000/10000], Loss: 1.2821\n",
            "Epoch [10/100], Step [10000/10000], Loss: 0.5227\n",
            "Epoch [11/100], Step [1000/10000], Loss: 0.9973\n",
            "Epoch [11/100], Step [2000/10000], Loss: 1.2429\n",
            "Epoch [11/100], Step [3000/10000], Loss: 2.0478\n",
            "Epoch [11/100], Step [4000/10000], Loss: 2.0380\n",
            "Epoch [11/100], Step [5000/10000], Loss: 1.9403\n",
            "Epoch [11/100], Step [6000/10000], Loss: 1.8739\n",
            "Epoch [11/100], Step [7000/10000], Loss: 1.6372\n",
            "Epoch [11/100], Step [8000/10000], Loss: 0.5631\n",
            "Epoch [11/100], Step [9000/10000], Loss: 1.0732\n",
            "Epoch [11/100], Step [10000/10000], Loss: 0.5080\n",
            "Epoch [12/100], Step [1000/10000], Loss: 0.2513\n",
            "Epoch [12/100], Step [2000/10000], Loss: 0.6672\n",
            "Epoch [12/100], Step [3000/10000], Loss: 1.1257\n",
            "Epoch [12/100], Step [4000/10000], Loss: 0.3211\n",
            "Epoch [12/100], Step [5000/10000], Loss: 1.0975\n",
            "Epoch [12/100], Step [6000/10000], Loss: 1.5214\n",
            "Epoch [12/100], Step [7000/10000], Loss: 0.8482\n",
            "Epoch [12/100], Step [8000/10000], Loss: 1.1642\n",
            "Epoch [12/100], Step [9000/10000], Loss: 0.4967\n",
            "Epoch [12/100], Step [10000/10000], Loss: 1.1532\n",
            "Epoch [13/100], Step [1000/10000], Loss: 0.2316\n",
            "Epoch [13/100], Step [2000/10000], Loss: 1.8910\n",
            "Epoch [13/100], Step [3000/10000], Loss: 0.7445\n",
            "Epoch [13/100], Step [4000/10000], Loss: 1.3650\n",
            "Epoch [13/100], Step [5000/10000], Loss: 0.7669\n",
            "Epoch [13/100], Step [6000/10000], Loss: 0.4134\n",
            "Epoch [13/100], Step [7000/10000], Loss: 2.6366\n",
            "Epoch [13/100], Step [8000/10000], Loss: 1.3253\n",
            "Epoch [13/100], Step [9000/10000], Loss: 0.7984\n",
            "Epoch [13/100], Step [10000/10000], Loss: 1.0855\n",
            "Epoch [14/100], Step [1000/10000], Loss: 1.2899\n",
            "Epoch [14/100], Step [2000/10000], Loss: 0.6144\n",
            "Epoch [14/100], Step [3000/10000], Loss: 0.9984\n",
            "Epoch [14/100], Step [4000/10000], Loss: 0.3356\n",
            "Epoch [14/100], Step [5000/10000], Loss: 1.5292\n",
            "Epoch [14/100], Step [6000/10000], Loss: 0.6444\n",
            "Epoch [14/100], Step [7000/10000], Loss: 1.8340\n",
            "Epoch [14/100], Step [8000/10000], Loss: 1.7841\n",
            "Epoch [14/100], Step [9000/10000], Loss: 0.9367\n",
            "Epoch [14/100], Step [10000/10000], Loss: 0.0367\n",
            "Epoch [15/100], Step [1000/10000], Loss: 0.2103\n",
            "Epoch [15/100], Step [2000/10000], Loss: 1.4210\n",
            "Epoch [15/100], Step [3000/10000], Loss: 0.9092\n",
            "Epoch [15/100], Step [4000/10000], Loss: 0.4873\n",
            "Epoch [15/100], Step [5000/10000], Loss: 0.8998\n",
            "Epoch [15/100], Step [6000/10000], Loss: 0.2871\n",
            "Epoch [15/100], Step [7000/10000], Loss: 0.6275\n",
            "Epoch [15/100], Step [8000/10000], Loss: 1.2546\n",
            "Epoch [15/100], Step [9000/10000], Loss: 0.4820\n",
            "Epoch [15/100], Step [10000/10000], Loss: 0.4407\n",
            "Epoch [16/100], Step [1000/10000], Loss: 1.2824\n",
            "Epoch [16/100], Step [2000/10000], Loss: 0.2668\n",
            "Epoch [16/100], Step [3000/10000], Loss: 1.0601\n",
            "Epoch [16/100], Step [4000/10000], Loss: 1.4363\n",
            "Epoch [16/100], Step [5000/10000], Loss: 0.4449\n",
            "Epoch [16/100], Step [6000/10000], Loss: 0.9353\n",
            "Epoch [16/100], Step [7000/10000], Loss: 0.3950\n",
            "Epoch [16/100], Step [8000/10000], Loss: 1.0789\n",
            "Epoch [16/100], Step [9000/10000], Loss: 0.5740\n",
            "Epoch [16/100], Step [10000/10000], Loss: 0.6431\n",
            "Epoch [17/100], Step [1000/10000], Loss: 0.5688\n",
            "Epoch [17/100], Step [2000/10000], Loss: 0.4742\n",
            "Epoch [17/100], Step [3000/10000], Loss: 1.1515\n",
            "Epoch [17/100], Step [4000/10000], Loss: 0.6420\n",
            "Epoch [17/100], Step [5000/10000], Loss: 0.6386\n",
            "Epoch [17/100], Step [6000/10000], Loss: 0.7724\n",
            "Epoch [17/100], Step [7000/10000], Loss: 0.5648\n",
            "Epoch [17/100], Step [8000/10000], Loss: 0.3290\n",
            "Epoch [17/100], Step [9000/10000], Loss: 0.9600\n",
            "Epoch [17/100], Step [10000/10000], Loss: 1.0064\n",
            "Epoch [18/100], Step [1000/10000], Loss: 1.5226\n",
            "Epoch [18/100], Step [2000/10000], Loss: 1.5456\n",
            "Epoch [18/100], Step [3000/10000], Loss: 0.3578\n",
            "Epoch [18/100], Step [4000/10000], Loss: 0.4296\n",
            "Epoch [18/100], Step [5000/10000], Loss: 0.9940\n",
            "Epoch [18/100], Step [6000/10000], Loss: 1.9245\n",
            "Epoch [18/100], Step [7000/10000], Loss: 0.7977\n",
            "Epoch [18/100], Step [8000/10000], Loss: 0.9412\n",
            "Epoch [18/100], Step [9000/10000], Loss: 0.2319\n",
            "Epoch [18/100], Step [10000/10000], Loss: 0.8797\n",
            "Epoch [19/100], Step [1000/10000], Loss: 0.4882\n",
            "Epoch [19/100], Step [2000/10000], Loss: 0.5311\n",
            "Epoch [19/100], Step [3000/10000], Loss: 0.0985\n",
            "Epoch [19/100], Step [4000/10000], Loss: 0.5482\n",
            "Epoch [19/100], Step [5000/10000], Loss: 1.3623\n",
            "Epoch [19/100], Step [6000/10000], Loss: 0.3352\n",
            "Epoch [19/100], Step [7000/10000], Loss: 0.7010\n",
            "Epoch [19/100], Step [8000/10000], Loss: 0.8293\n",
            "Epoch [19/100], Step [9000/10000], Loss: 0.3139\n",
            "Epoch [19/100], Step [10000/10000], Loss: 0.5613\n",
            "Epoch [20/100], Step [1000/10000], Loss: 0.6250\n",
            "Epoch [20/100], Step [2000/10000], Loss: 0.4476\n",
            "Epoch [20/100], Step [3000/10000], Loss: 1.0206\n",
            "Epoch [20/100], Step [4000/10000], Loss: 1.1442\n",
            "Epoch [20/100], Step [5000/10000], Loss: 1.3697\n",
            "Epoch [20/100], Step [6000/10000], Loss: 0.2271\n",
            "Epoch [20/100], Step [7000/10000], Loss: 0.8356\n",
            "Epoch [20/100], Step [8000/10000], Loss: 0.6049\n",
            "Epoch [20/100], Step [9000/10000], Loss: 0.8377\n",
            "Epoch [20/100], Step [10000/10000], Loss: 0.5638\n",
            "Epoch [21/100], Step [1000/10000], Loss: 1.3181\n",
            "Epoch [21/100], Step [2000/10000], Loss: 0.2340\n",
            "Epoch [21/100], Step [3000/10000], Loss: 0.4061\n",
            "Epoch [21/100], Step [4000/10000], Loss: 0.2978\n",
            "Epoch [21/100], Step [5000/10000], Loss: 1.2123\n",
            "Epoch [21/100], Step [6000/10000], Loss: 1.2311\n",
            "Epoch [21/100], Step [7000/10000], Loss: 0.9104\n",
            "Epoch [21/100], Step [8000/10000], Loss: 0.1781\n",
            "Epoch [21/100], Step [9000/10000], Loss: 1.1641\n",
            "Epoch [21/100], Step [10000/10000], Loss: 0.1371\n",
            "Epoch [22/100], Step [1000/10000], Loss: 0.3138\n",
            "Epoch [22/100], Step [2000/10000], Loss: 0.5751\n",
            "Epoch [22/100], Step [3000/10000], Loss: 0.2432\n",
            "Epoch [22/100], Step [4000/10000], Loss: 0.1220\n",
            "Epoch [22/100], Step [5000/10000], Loss: 0.7388\n",
            "Epoch [22/100], Step [6000/10000], Loss: 0.9334\n",
            "Epoch [22/100], Step [7000/10000], Loss: 0.7806\n",
            "Epoch [22/100], Step [8000/10000], Loss: 0.5857\n",
            "Epoch [22/100], Step [9000/10000], Loss: 1.2055\n",
            "Epoch [22/100], Step [10000/10000], Loss: 1.2001\n",
            "Epoch [23/100], Step [1000/10000], Loss: 1.1729\n",
            "Epoch [23/100], Step [2000/10000], Loss: 1.7991\n",
            "Epoch [23/100], Step [3000/10000], Loss: 0.6423\n",
            "Epoch [23/100], Step [4000/10000], Loss: 1.2897\n",
            "Epoch [23/100], Step [5000/10000], Loss: 0.6346\n",
            "Epoch [23/100], Step [6000/10000], Loss: 1.1882\n",
            "Epoch [23/100], Step [7000/10000], Loss: 0.6687\n",
            "Epoch [23/100], Step [8000/10000], Loss: 0.9705\n",
            "Epoch [23/100], Step [9000/10000], Loss: 0.5103\n",
            "Epoch [23/100], Step [10000/10000], Loss: 0.7055\n",
            "Epoch [24/100], Step [1000/10000], Loss: 1.9840\n",
            "Epoch [24/100], Step [2000/10000], Loss: 0.3862\n",
            "Epoch [24/100], Step [3000/10000], Loss: 0.6433\n",
            "Epoch [24/100], Step [4000/10000], Loss: 2.5309\n",
            "Epoch [24/100], Step [5000/10000], Loss: 1.5364\n",
            "Epoch [24/100], Step [6000/10000], Loss: 0.4518\n",
            "Epoch [24/100], Step [7000/10000], Loss: 0.1516\n",
            "Epoch [24/100], Step [8000/10000], Loss: 0.5505\n",
            "Epoch [24/100], Step [9000/10000], Loss: 0.2791\n",
            "Epoch [24/100], Step [10000/10000], Loss: 0.8669\n",
            "Epoch [25/100], Step [1000/10000], Loss: 0.1602\n",
            "Epoch [25/100], Step [2000/10000], Loss: 1.0536\n",
            "Epoch [25/100], Step [3000/10000], Loss: 1.7021\n",
            "Epoch [25/100], Step [4000/10000], Loss: 0.9289\n",
            "Epoch [25/100], Step [5000/10000], Loss: 0.9977\n",
            "Epoch [25/100], Step [6000/10000], Loss: 0.1785\n",
            "Epoch [25/100], Step [7000/10000], Loss: 0.4797\n",
            "Epoch [25/100], Step [8000/10000], Loss: 1.0793\n",
            "Epoch [25/100], Step [9000/10000], Loss: 0.8403\n",
            "Epoch [25/100], Step [10000/10000], Loss: 0.8592\n",
            "Epoch [26/100], Step [1000/10000], Loss: 0.2963\n",
            "Epoch [26/100], Step [2000/10000], Loss: 0.9872\n",
            "Epoch [26/100], Step [3000/10000], Loss: 0.6436\n",
            "Epoch [26/100], Step [4000/10000], Loss: 0.6169\n",
            "Epoch [26/100], Step [5000/10000], Loss: 0.9242\n",
            "Epoch [26/100], Step [6000/10000], Loss: 1.1455\n",
            "Epoch [26/100], Step [7000/10000], Loss: 1.0212\n",
            "Epoch [26/100], Step [8000/10000], Loss: 0.4056\n",
            "Epoch [26/100], Step [9000/10000], Loss: 0.7506\n",
            "Epoch [26/100], Step [10000/10000], Loss: 0.9733\n",
            "Epoch [27/100], Step [1000/10000], Loss: 1.2250\n",
            "Epoch [27/100], Step [2000/10000], Loss: 0.0739\n",
            "Epoch [27/100], Step [3000/10000], Loss: 0.2020\n",
            "Epoch [27/100], Step [4000/10000], Loss: 1.5267\n",
            "Epoch [27/100], Step [5000/10000], Loss: 0.5646\n",
            "Epoch [27/100], Step [6000/10000], Loss: 2.2607\n",
            "Epoch [27/100], Step [7000/10000], Loss: 0.2674\n",
            "Epoch [27/100], Step [8000/10000], Loss: 0.8395\n",
            "Epoch [27/100], Step [9000/10000], Loss: 0.5421\n",
            "Epoch [27/100], Step [10000/10000], Loss: 0.2546\n",
            "Epoch [28/100], Step [1000/10000], Loss: 0.4055\n",
            "Epoch [28/100], Step [2000/10000], Loss: 1.5752\n",
            "Epoch [28/100], Step [3000/10000], Loss: 0.3973\n",
            "Epoch [28/100], Step [4000/10000], Loss: 0.2159\n",
            "Epoch [28/100], Step [5000/10000], Loss: 0.2857\n",
            "Epoch [28/100], Step [6000/10000], Loss: 0.4477\n",
            "Epoch [28/100], Step [7000/10000], Loss: 1.0475\n",
            "Epoch [28/100], Step [8000/10000], Loss: 1.4293\n",
            "Epoch [28/100], Step [9000/10000], Loss: 0.6729\n",
            "Epoch [28/100], Step [10000/10000], Loss: 1.1732\n",
            "Epoch [29/100], Step [1000/10000], Loss: 0.6158\n",
            "Epoch [29/100], Step [2000/10000], Loss: 0.2047\n",
            "Epoch [29/100], Step [3000/10000], Loss: 0.7778\n",
            "Epoch [29/100], Step [4000/10000], Loss: 0.6671\n",
            "Epoch [29/100], Step [5000/10000], Loss: 1.9731\n",
            "Epoch [29/100], Step [6000/10000], Loss: 0.5198\n",
            "Epoch [29/100], Step [7000/10000], Loss: 0.4368\n",
            "Epoch [29/100], Step [8000/10000], Loss: 0.7519\n",
            "Epoch [29/100], Step [9000/10000], Loss: 0.7873\n",
            "Epoch [29/100], Step [10000/10000], Loss: 0.6271\n",
            "Epoch [30/100], Step [1000/10000], Loss: 0.3735\n",
            "Epoch [30/100], Step [2000/10000], Loss: 0.3776\n",
            "Epoch [30/100], Step [3000/10000], Loss: 0.2479\n",
            "Epoch [30/100], Step [4000/10000], Loss: 0.4375\n",
            "Epoch [30/100], Step [5000/10000], Loss: 0.7567\n",
            "Epoch [30/100], Step [6000/10000], Loss: 1.0046\n",
            "Epoch [30/100], Step [7000/10000], Loss: 1.4451\n",
            "Epoch [30/100], Step [8000/10000], Loss: 0.5316\n",
            "Epoch [30/100], Step [9000/10000], Loss: 1.1196\n",
            "Epoch [30/100], Step [10000/10000], Loss: 1.1895\n",
            "Epoch [31/100], Step [1000/10000], Loss: 0.6122\n",
            "Epoch [31/100], Step [2000/10000], Loss: 0.3039\n",
            "Epoch [31/100], Step [3000/10000], Loss: 0.9805\n",
            "Epoch [31/100], Step [4000/10000], Loss: 0.3777\n",
            "Epoch [31/100], Step [5000/10000], Loss: 0.1344\n",
            "Epoch [31/100], Step [6000/10000], Loss: 0.3439\n",
            "Epoch [31/100], Step [7000/10000], Loss: 0.0586\n",
            "Epoch [31/100], Step [8000/10000], Loss: 0.2607\n",
            "Epoch [31/100], Step [9000/10000], Loss: 1.2905\n",
            "Epoch [31/100], Step [10000/10000], Loss: 0.8524\n",
            "Epoch [32/100], Step [1000/10000], Loss: 0.9812\n",
            "Epoch [32/100], Step [2000/10000], Loss: 0.6958\n",
            "Epoch [32/100], Step [3000/10000], Loss: 0.5242\n",
            "Epoch [32/100], Step [4000/10000], Loss: 0.8400\n",
            "Epoch [32/100], Step [5000/10000], Loss: 0.5669\n",
            "Epoch [32/100], Step [6000/10000], Loss: 1.2939\n",
            "Epoch [32/100], Step [7000/10000], Loss: 0.0823\n",
            "Epoch [32/100], Step [8000/10000], Loss: 1.0002\n",
            "Epoch [32/100], Step [9000/10000], Loss: 1.6197\n",
            "Epoch [32/100], Step [10000/10000], Loss: 0.8577\n",
            "Epoch [33/100], Step [1000/10000], Loss: 1.7972\n",
            "Epoch [33/100], Step [2000/10000], Loss: 0.3358\n",
            "Epoch [33/100], Step [3000/10000], Loss: 0.5872\n",
            "Epoch [33/100], Step [4000/10000], Loss: 1.0200\n",
            "Epoch [33/100], Step [5000/10000], Loss: 1.3219\n",
            "Epoch [33/100], Step [6000/10000], Loss: 0.3933\n",
            "Epoch [33/100], Step [7000/10000], Loss: 0.4863\n",
            "Epoch [33/100], Step [8000/10000], Loss: 0.4460\n",
            "Epoch [33/100], Step [9000/10000], Loss: 0.7722\n",
            "Epoch [33/100], Step [10000/10000], Loss: 0.8392\n",
            "Epoch [34/100], Step [1000/10000], Loss: 0.9294\n",
            "Epoch [34/100], Step [2000/10000], Loss: 1.0815\n",
            "Epoch [34/100], Step [3000/10000], Loss: 0.1650\n",
            "Epoch [34/100], Step [4000/10000], Loss: 0.7571\n",
            "Epoch [34/100], Step [5000/10000], Loss: 1.8412\n",
            "Epoch [34/100], Step [6000/10000], Loss: 0.0307\n",
            "Epoch [34/100], Step [7000/10000], Loss: 1.2834\n",
            "Epoch [34/100], Step [8000/10000], Loss: 1.1035\n",
            "Epoch [34/100], Step [9000/10000], Loss: 0.5526\n",
            "Epoch [34/100], Step [10000/10000], Loss: 0.5462\n",
            "Epoch [35/100], Step [1000/10000], Loss: 0.1766\n",
            "Epoch [35/100], Step [2000/10000], Loss: 1.2301\n",
            "Epoch [35/100], Step [3000/10000], Loss: 1.3113\n",
            "Epoch [35/100], Step [4000/10000], Loss: 0.3315\n",
            "Epoch [35/100], Step [5000/10000], Loss: 0.7208\n",
            "Epoch [35/100], Step [6000/10000], Loss: 0.6544\n",
            "Epoch [35/100], Step [7000/10000], Loss: 1.2824\n",
            "Epoch [35/100], Step [8000/10000], Loss: 1.1732\n",
            "Epoch [35/100], Step [9000/10000], Loss: 0.2900\n",
            "Epoch [35/100], Step [10000/10000], Loss: 1.0286\n",
            "Epoch [36/100], Step [1000/10000], Loss: 1.2025\n",
            "Epoch [36/100], Step [2000/10000], Loss: 0.0486\n",
            "Epoch [36/100], Step [3000/10000], Loss: 0.6591\n",
            "Epoch [36/100], Step [4000/10000], Loss: 1.1794\n",
            "Epoch [36/100], Step [5000/10000], Loss: 0.4367\n",
            "Epoch [36/100], Step [6000/10000], Loss: 1.0491\n",
            "Epoch [36/100], Step [7000/10000], Loss: 0.8805\n",
            "Epoch [36/100], Step [8000/10000], Loss: 1.8148\n",
            "Epoch [36/100], Step [9000/10000], Loss: 1.5698\n",
            "Epoch [36/100], Step [10000/10000], Loss: 2.0885\n",
            "Epoch [37/100], Step [1000/10000], Loss: 1.9050\n",
            "Epoch [37/100], Step [2000/10000], Loss: 5.1064\n",
            "Epoch [37/100], Step [3000/10000], Loss: 0.5651\n",
            "Epoch [37/100], Step [4000/10000], Loss: 0.7851\n",
            "Epoch [37/100], Step [5000/10000], Loss: 1.0477\n",
            "Epoch [37/100], Step [6000/10000], Loss: 0.9531\n",
            "Epoch [37/100], Step [7000/10000], Loss: 0.7670\n",
            "Epoch [37/100], Step [8000/10000], Loss: 0.7241\n",
            "Epoch [37/100], Step [9000/10000], Loss: 0.6486\n",
            "Epoch [37/100], Step [10000/10000], Loss: 1.7668\n",
            "Epoch [38/100], Step [1000/10000], Loss: 0.2354\n",
            "Epoch [38/100], Step [2000/10000], Loss: 1.4560\n",
            "Epoch [38/100], Step [3000/10000], Loss: 0.2900\n",
            "Epoch [38/100], Step [4000/10000], Loss: 0.6791\n",
            "Epoch [38/100], Step [5000/10000], Loss: 0.9551\n",
            "Epoch [38/100], Step [6000/10000], Loss: 1.0830\n",
            "Epoch [38/100], Step [7000/10000], Loss: 0.4933\n",
            "Epoch [38/100], Step [8000/10000], Loss: 1.7861\n",
            "Epoch [38/100], Step [9000/10000], Loss: 0.2880\n",
            "Epoch [38/100], Step [10000/10000], Loss: 1.1386\n",
            "Epoch [39/100], Step [1000/10000], Loss: 0.7572\n",
            "Epoch [39/100], Step [2000/10000], Loss: 1.8096\n",
            "Epoch [39/100], Step [3000/10000], Loss: 0.6568\n",
            "Epoch [39/100], Step [4000/10000], Loss: 0.3951\n",
            "Epoch [39/100], Step [5000/10000], Loss: 0.1051\n",
            "Epoch [39/100], Step [6000/10000], Loss: 0.6222\n",
            "Epoch [39/100], Step [7000/10000], Loss: 0.4659\n",
            "Epoch [39/100], Step [8000/10000], Loss: 0.6962\n",
            "Epoch [39/100], Step [9000/10000], Loss: 3.7962\n",
            "Epoch [39/100], Step [10000/10000], Loss: 0.3600\n",
            "Epoch [40/100], Step [1000/10000], Loss: 0.1649\n",
            "Epoch [40/100], Step [2000/10000], Loss: 0.5957\n",
            "Epoch [40/100], Step [3000/10000], Loss: 0.9595\n",
            "Epoch [40/100], Step [4000/10000], Loss: 0.6772\n",
            "Epoch [40/100], Step [5000/10000], Loss: 0.4041\n",
            "Epoch [40/100], Step [6000/10000], Loss: 0.8435\n",
            "Epoch [40/100], Step [7000/10000], Loss: 3.0855\n",
            "Epoch [40/100], Step [8000/10000], Loss: 1.3137\n",
            "Epoch [40/100], Step [9000/10000], Loss: 0.3954\n",
            "Epoch [40/100], Step [10000/10000], Loss: 1.1204\n",
            "Epoch [41/100], Step [1000/10000], Loss: 0.4741\n",
            "Epoch [41/100], Step [2000/10000], Loss: 0.2651\n",
            "Epoch [41/100], Step [3000/10000], Loss: 0.5471\n",
            "Epoch [41/100], Step [4000/10000], Loss: 0.2513\n",
            "Epoch [41/100], Step [5000/10000], Loss: 0.7866\n",
            "Epoch [41/100], Step [6000/10000], Loss: 0.2863\n",
            "Epoch [41/100], Step [7000/10000], Loss: 0.4863\n",
            "Epoch [41/100], Step [8000/10000], Loss: 1.2790\n",
            "Epoch [41/100], Step [9000/10000], Loss: 0.6745\n",
            "Epoch [41/100], Step [10000/10000], Loss: 1.0340\n",
            "Epoch [42/100], Step [1000/10000], Loss: 0.9018\n",
            "Epoch [42/100], Step [2000/10000], Loss: 1.0233\n",
            "Epoch [42/100], Step [3000/10000], Loss: 2.2378\n",
            "Epoch [42/100], Step [4000/10000], Loss: 0.1795\n",
            "Epoch [42/100], Step [5000/10000], Loss: 1.0196\n",
            "Epoch [42/100], Step [6000/10000], Loss: 1.3141\n",
            "Epoch [42/100], Step [7000/10000], Loss: 0.4529\n",
            "Epoch [42/100], Step [8000/10000], Loss: 0.3596\n",
            "Epoch [42/100], Step [9000/10000], Loss: 0.2653\n",
            "Epoch [42/100], Step [10000/10000], Loss: 0.2853\n",
            "Epoch [43/100], Step [1000/10000], Loss: 0.0890\n",
            "Epoch [43/100], Step [2000/10000], Loss: 0.9765\n",
            "Epoch [43/100], Step [3000/10000], Loss: 0.4300\n",
            "Epoch [43/100], Step [4000/10000], Loss: 1.0346\n",
            "Epoch [43/100], Step [5000/10000], Loss: 0.6006\n",
            "Epoch [43/100], Step [6000/10000], Loss: 0.8325\n",
            "Epoch [43/100], Step [7000/10000], Loss: 0.8169\n",
            "Epoch [43/100], Step [8000/10000], Loss: 1.1705\n",
            "Epoch [43/100], Step [9000/10000], Loss: 0.2630\n",
            "Epoch [43/100], Step [10000/10000], Loss: 2.4414\n",
            "Epoch [44/100], Step [1000/10000], Loss: 1.3211\n",
            "Epoch [44/100], Step [2000/10000], Loss: 0.8212\n",
            "Epoch [44/100], Step [3000/10000], Loss: 0.4377\n",
            "Epoch [44/100], Step [4000/10000], Loss: 1.0015\n",
            "Epoch [44/100], Step [5000/10000], Loss: 0.3075\n",
            "Epoch [44/100], Step [6000/10000], Loss: 0.4958\n",
            "Epoch [44/100], Step [7000/10000], Loss: 1.5152\n",
            "Epoch [44/100], Step [8000/10000], Loss: 0.4651\n",
            "Epoch [44/100], Step [9000/10000], Loss: 0.4801\n",
            "Epoch [44/100], Step [10000/10000], Loss: 0.4361\n",
            "Epoch [45/100], Step [1000/10000], Loss: 0.1849\n",
            "Epoch [45/100], Step [2000/10000], Loss: 0.6737\n",
            "Epoch [45/100], Step [3000/10000], Loss: 0.5411\n",
            "Epoch [45/100], Step [4000/10000], Loss: 0.7421\n",
            "Epoch [45/100], Step [5000/10000], Loss: 0.5443\n",
            "Epoch [45/100], Step [6000/10000], Loss: 0.8579\n",
            "Epoch [45/100], Step [7000/10000], Loss: 0.3873\n",
            "Epoch [45/100], Step [8000/10000], Loss: 0.6797\n",
            "Epoch [45/100], Step [9000/10000], Loss: 0.9118\n",
            "Epoch [45/100], Step [10000/10000], Loss: 1.2155\n",
            "Epoch [46/100], Step [1000/10000], Loss: 1.9435\n",
            "Epoch [46/100], Step [2000/10000], Loss: 0.1359\n",
            "Epoch [46/100], Step [3000/10000], Loss: 0.4746\n",
            "Epoch [46/100], Step [4000/10000], Loss: 0.3944\n",
            "Epoch [46/100], Step [5000/10000], Loss: 1.8874\n",
            "Epoch [46/100], Step [6000/10000], Loss: 0.0467\n",
            "Epoch [46/100], Step [7000/10000], Loss: 0.1459\n",
            "Epoch [46/100], Step [8000/10000], Loss: 1.5412\n",
            "Epoch [46/100], Step [9000/10000], Loss: 1.0097\n",
            "Epoch [46/100], Step [10000/10000], Loss: 1.7476\n",
            "Epoch [47/100], Step [1000/10000], Loss: 1.5496\n",
            "Epoch [47/100], Step [2000/10000], Loss: 0.5395\n",
            "Epoch [47/100], Step [3000/10000], Loss: 0.3743\n",
            "Epoch [47/100], Step [4000/10000], Loss: 0.6451\n",
            "Epoch [47/100], Step [5000/10000], Loss: 0.9877\n",
            "Epoch [47/100], Step [6000/10000], Loss: 0.6207\n",
            "Epoch [47/100], Step [7000/10000], Loss: 0.3671\n",
            "Epoch [47/100], Step [8000/10000], Loss: 0.0176\n",
            "Epoch [47/100], Step [9000/10000], Loss: 0.5825\n",
            "Epoch [47/100], Step [10000/10000], Loss: 1.2211\n",
            "Epoch [48/100], Step [1000/10000], Loss: 0.7246\n",
            "Epoch [48/100], Step [2000/10000], Loss: 0.6912\n",
            "Epoch [48/100], Step [3000/10000], Loss: 0.2690\n",
            "Epoch [48/100], Step [4000/10000], Loss: 0.5143\n",
            "Epoch [48/100], Step [5000/10000], Loss: 0.1838\n",
            "Epoch [48/100], Step [6000/10000], Loss: 0.6722\n",
            "Epoch [48/100], Step [7000/10000], Loss: 0.7064\n",
            "Epoch [48/100], Step [8000/10000], Loss: 0.9359\n",
            "Epoch [48/100], Step [9000/10000], Loss: 0.0967\n",
            "Epoch [48/100], Step [10000/10000], Loss: 0.4975\n",
            "Epoch [49/100], Step [1000/10000], Loss: 0.6568\n",
            "Epoch [49/100], Step [2000/10000], Loss: 0.2397\n",
            "Epoch [49/100], Step [3000/10000], Loss: 0.9003\n",
            "Epoch [49/100], Step [4000/10000], Loss: 0.1473\n",
            "Epoch [49/100], Step [5000/10000], Loss: 0.3141\n",
            "Epoch [49/100], Step [6000/10000], Loss: 0.4431\n",
            "Epoch [49/100], Step [7000/10000], Loss: 1.1803\n",
            "Epoch [49/100], Step [8000/10000], Loss: 0.1986\n",
            "Epoch [49/100], Step [9000/10000], Loss: 0.7109\n",
            "Epoch [49/100], Step [10000/10000], Loss: 0.4802\n",
            "Epoch [50/100], Step [1000/10000], Loss: 0.6287\n",
            "Epoch [50/100], Step [2000/10000], Loss: 0.4146\n",
            "Epoch [50/100], Step [3000/10000], Loss: 1.3673\n",
            "Epoch [50/100], Step [4000/10000], Loss: 0.2822\n",
            "Epoch [50/100], Step [5000/10000], Loss: 0.1647\n",
            "Epoch [50/100], Step [6000/10000], Loss: 0.1953\n",
            "Epoch [50/100], Step [7000/10000], Loss: 1.2745\n",
            "Epoch [50/100], Step [8000/10000], Loss: 0.7364\n",
            "Epoch [50/100], Step [9000/10000], Loss: 0.7556\n",
            "Epoch [50/100], Step [10000/10000], Loss: 1.2703\n",
            "Epoch [51/100], Step [1000/10000], Loss: 0.3926\n",
            "Epoch [51/100], Step [2000/10000], Loss: 1.4828\n",
            "Epoch [51/100], Step [3000/10000], Loss: 0.7339\n",
            "Epoch [51/100], Step [4000/10000], Loss: 1.0751\n",
            "Epoch [51/100], Step [5000/10000], Loss: 0.6019\n",
            "Epoch [51/100], Step [6000/10000], Loss: 1.4101\n",
            "Epoch [51/100], Step [7000/10000], Loss: 1.9811\n",
            "Epoch [51/100], Step [8000/10000], Loss: 0.6532\n",
            "Epoch [51/100], Step [9000/10000], Loss: 0.9030\n",
            "Epoch [51/100], Step [10000/10000], Loss: 0.9330\n",
            "Epoch [52/100], Step [1000/10000], Loss: 0.0979\n",
            "Epoch [52/100], Step [2000/10000], Loss: 0.9805\n",
            "Epoch [52/100], Step [3000/10000], Loss: 0.8453\n",
            "Epoch [52/100], Step [4000/10000], Loss: 0.3857\n",
            "Epoch [52/100], Step [5000/10000], Loss: 0.7378\n",
            "Epoch [52/100], Step [6000/10000], Loss: 0.6754\n",
            "Epoch [52/100], Step [7000/10000], Loss: 0.2804\n",
            "Epoch [52/100], Step [8000/10000], Loss: 0.4925\n",
            "Epoch [52/100], Step [9000/10000], Loss: 0.4151\n",
            "Epoch [52/100], Step [10000/10000], Loss: 0.7940\n",
            "Epoch [53/100], Step [1000/10000], Loss: 1.1959\n",
            "Epoch [53/100], Step [2000/10000], Loss: 0.7471\n",
            "Epoch [53/100], Step [3000/10000], Loss: 0.4323\n",
            "Epoch [53/100], Step [4000/10000], Loss: 0.3456\n",
            "Epoch [53/100], Step [5000/10000], Loss: 0.2328\n",
            "Epoch [53/100], Step [6000/10000], Loss: 0.7044\n",
            "Epoch [53/100], Step [7000/10000], Loss: 1.4139\n",
            "Epoch [53/100], Step [8000/10000], Loss: 0.8613\n",
            "Epoch [53/100], Step [9000/10000], Loss: 0.6840\n",
            "Epoch [53/100], Step [10000/10000], Loss: 1.8972\n",
            "Epoch [54/100], Step [1000/10000], Loss: 1.1348\n",
            "Epoch [54/100], Step [2000/10000], Loss: 0.0067\n",
            "Epoch [54/100], Step [3000/10000], Loss: 0.9070\n",
            "Epoch [54/100], Step [4000/10000], Loss: 0.4034\n",
            "Epoch [54/100], Step [5000/10000], Loss: 2.3383\n",
            "Epoch [54/100], Step [6000/10000], Loss: 0.0470\n",
            "Epoch [54/100], Step [7000/10000], Loss: 0.0324\n",
            "Epoch [54/100], Step [8000/10000], Loss: 0.4632\n",
            "Epoch [54/100], Step [9000/10000], Loss: 0.5189\n",
            "Epoch [54/100], Step [10000/10000], Loss: 0.2550\n",
            "Epoch [55/100], Step [1000/10000], Loss: 0.5855\n",
            "Epoch [55/100], Step [2000/10000], Loss: 0.4911\n",
            "Epoch [55/100], Step [3000/10000], Loss: 1.0834\n",
            "Epoch [55/100], Step [4000/10000], Loss: 0.8847\n",
            "Epoch [55/100], Step [5000/10000], Loss: 0.7464\n",
            "Epoch [55/100], Step [6000/10000], Loss: 1.4065\n",
            "Epoch [55/100], Step [7000/10000], Loss: 1.0000\n",
            "Epoch [55/100], Step [8000/10000], Loss: 0.7262\n",
            "Epoch [55/100], Step [9000/10000], Loss: 0.5381\n",
            "Epoch [55/100], Step [10000/10000], Loss: 0.1569\n",
            "Epoch [56/100], Step [1000/10000], Loss: 0.7341\n",
            "Epoch [56/100], Step [2000/10000], Loss: 1.6925\n",
            "Epoch [56/100], Step [3000/10000], Loss: 1.2534\n",
            "Epoch [56/100], Step [4000/10000], Loss: 1.1987\n",
            "Epoch [56/100], Step [5000/10000], Loss: 0.2893\n",
            "Epoch [56/100], Step [6000/10000], Loss: 0.4990\n",
            "Epoch [56/100], Step [7000/10000], Loss: 0.9285\n",
            "Epoch [56/100], Step [8000/10000], Loss: 0.6296\n",
            "Epoch [56/100], Step [9000/10000], Loss: 0.3427\n",
            "Epoch [56/100], Step [10000/10000], Loss: 0.4023\n",
            "Epoch [57/100], Step [1000/10000], Loss: 0.5569\n",
            "Epoch [57/100], Step [2000/10000], Loss: 1.3161\n",
            "Epoch [57/100], Step [3000/10000], Loss: 0.3828\n",
            "Epoch [57/100], Step [4000/10000], Loss: 0.1954\n",
            "Epoch [57/100], Step [5000/10000], Loss: 0.5946\n",
            "Epoch [57/100], Step [6000/10000], Loss: 1.1780\n",
            "Epoch [57/100], Step [7000/10000], Loss: 0.6597\n",
            "Epoch [57/100], Step [8000/10000], Loss: 0.3562\n",
            "Epoch [57/100], Step [9000/10000], Loss: 0.3047\n",
            "Epoch [57/100], Step [10000/10000], Loss: 0.6656\n",
            "Epoch [58/100], Step [1000/10000], Loss: 0.9789\n",
            "Epoch [58/100], Step [2000/10000], Loss: 0.8583\n",
            "Epoch [58/100], Step [3000/10000], Loss: 0.6398\n",
            "Epoch [58/100], Step [4000/10000], Loss: 0.2068\n",
            "Epoch [58/100], Step [5000/10000], Loss: 0.5426\n",
            "Epoch [58/100], Step [6000/10000], Loss: 0.1002\n",
            "Epoch [58/100], Step [7000/10000], Loss: 2.0549\n",
            "Epoch [58/100], Step [8000/10000], Loss: 0.3384\n",
            "Epoch [58/100], Step [9000/10000], Loss: 0.9321\n",
            "Epoch [58/100], Step [10000/10000], Loss: 0.5282\n",
            "Epoch [59/100], Step [1000/10000], Loss: 0.5060\n",
            "Epoch [59/100], Step [2000/10000], Loss: 0.1338\n",
            "Epoch [59/100], Step [3000/10000], Loss: 0.9386\n",
            "Epoch [59/100], Step [4000/10000], Loss: 1.0861\n",
            "Epoch [59/100], Step [5000/10000], Loss: 0.4432\n",
            "Epoch [59/100], Step [6000/10000], Loss: 0.4183\n",
            "Epoch [59/100], Step [7000/10000], Loss: 0.8833\n",
            "Epoch [59/100], Step [8000/10000], Loss: 0.6227\n",
            "Epoch [59/100], Step [9000/10000], Loss: 0.5851\n",
            "Epoch [59/100], Step [10000/10000], Loss: 0.2973\n",
            "Epoch [60/100], Step [1000/10000], Loss: 0.4306\n",
            "Epoch [60/100], Step [2000/10000], Loss: 1.4778\n",
            "Epoch [60/100], Step [3000/10000], Loss: 0.1384\n",
            "Epoch [60/100], Step [4000/10000], Loss: 2.3826\n",
            "Epoch [60/100], Step [5000/10000], Loss: 0.0564\n",
            "Epoch [60/100], Step [6000/10000], Loss: 1.8110\n",
            "Epoch [60/100], Step [7000/10000], Loss: 4.7649\n",
            "Epoch [60/100], Step [8000/10000], Loss: 0.1008\n",
            "Epoch [60/100], Step [9000/10000], Loss: 0.2198\n",
            "Epoch [60/100], Step [10000/10000], Loss: 1.1969\n",
            "Epoch [61/100], Step [1000/10000], Loss: 0.4339\n",
            "Epoch [61/100], Step [2000/10000], Loss: 0.3863\n",
            "Epoch [61/100], Step [3000/10000], Loss: 1.0071\n",
            "Epoch [61/100], Step [4000/10000], Loss: 1.5116\n",
            "Epoch [61/100], Step [5000/10000], Loss: 0.2745\n",
            "Epoch [61/100], Step [6000/10000], Loss: 0.3930\n",
            "Epoch [61/100], Step [7000/10000], Loss: 0.6863\n",
            "Epoch [61/100], Step [8000/10000], Loss: 0.7964\n",
            "Epoch [61/100], Step [9000/10000], Loss: 0.0310\n",
            "Epoch [61/100], Step [10000/10000], Loss: 1.0145\n",
            "Epoch [62/100], Step [1000/10000], Loss: 0.8507\n",
            "Epoch [62/100], Step [2000/10000], Loss: 0.1697\n",
            "Epoch [62/100], Step [3000/10000], Loss: 0.9727\n",
            "Epoch [62/100], Step [4000/10000], Loss: 0.8741\n",
            "Epoch [62/100], Step [5000/10000], Loss: 0.1796\n",
            "Epoch [62/100], Step [6000/10000], Loss: 1.3043\n",
            "Epoch [62/100], Step [7000/10000], Loss: 0.3280\n",
            "Epoch [62/100], Step [8000/10000], Loss: 0.6777\n",
            "Epoch [62/100], Step [9000/10000], Loss: 0.6233\n",
            "Epoch [62/100], Step [10000/10000], Loss: 1.3519\n",
            "Epoch [63/100], Step [1000/10000], Loss: 0.0136\n",
            "Epoch [63/100], Step [2000/10000], Loss: 0.2909\n",
            "Epoch [63/100], Step [3000/10000], Loss: 0.5721\n",
            "Epoch [63/100], Step [4000/10000], Loss: 1.0380\n",
            "Epoch [63/100], Step [5000/10000], Loss: 0.0274\n",
            "Epoch [63/100], Step [6000/10000], Loss: 0.8974\n",
            "Epoch [63/100], Step [7000/10000], Loss: 0.0805\n",
            "Epoch [63/100], Step [8000/10000], Loss: 1.3547\n",
            "Epoch [63/100], Step [9000/10000], Loss: 0.1657\n",
            "Epoch [63/100], Step [10000/10000], Loss: 0.3370\n",
            "Epoch [64/100], Step [1000/10000], Loss: 0.4437\n",
            "Epoch [64/100], Step [2000/10000], Loss: 0.3048\n",
            "Epoch [64/100], Step [3000/10000], Loss: 0.0545\n",
            "Epoch [64/100], Step [4000/10000], Loss: 0.8928\n",
            "Epoch [64/100], Step [5000/10000], Loss: 0.2955\n",
            "Epoch [64/100], Step [6000/10000], Loss: 0.4817\n",
            "Epoch [64/100], Step [7000/10000], Loss: 0.8154\n",
            "Epoch [64/100], Step [8000/10000], Loss: 0.5952\n",
            "Epoch [64/100], Step [9000/10000], Loss: 1.5681\n",
            "Epoch [64/100], Step [10000/10000], Loss: 2.3115\n",
            "Epoch [65/100], Step [1000/10000], Loss: 1.8674\n",
            "Epoch [65/100], Step [2000/10000], Loss: 0.8458\n",
            "Epoch [65/100], Step [3000/10000], Loss: 0.9493\n",
            "Epoch [65/100], Step [4000/10000], Loss: 1.3753\n",
            "Epoch [65/100], Step [5000/10000], Loss: 0.1268\n",
            "Epoch [65/100], Step [6000/10000], Loss: 0.4085\n",
            "Epoch [65/100], Step [7000/10000], Loss: 0.3719\n",
            "Epoch [65/100], Step [8000/10000], Loss: 0.7056\n",
            "Epoch [65/100], Step [9000/10000], Loss: 0.4863\n",
            "Epoch [65/100], Step [10000/10000], Loss: 0.4923\n",
            "Epoch [66/100], Step [1000/10000], Loss: 0.3548\n",
            "Epoch [66/100], Step [2000/10000], Loss: 0.2971\n",
            "Epoch [66/100], Step [3000/10000], Loss: 0.5368\n",
            "Epoch [66/100], Step [4000/10000], Loss: 0.4477\n",
            "Epoch [66/100], Step [5000/10000], Loss: 0.2694\n",
            "Epoch [66/100], Step [6000/10000], Loss: 1.2685\n",
            "Epoch [66/100], Step [7000/10000], Loss: 0.8051\n",
            "Epoch [66/100], Step [8000/10000], Loss: 0.8125\n",
            "Epoch [66/100], Step [9000/10000], Loss: 0.5961\n",
            "Epoch [66/100], Step [10000/10000], Loss: 0.8977\n",
            "Epoch [67/100], Step [1000/10000], Loss: 1.4388\n",
            "Epoch [67/100], Step [2000/10000], Loss: 0.4972\n",
            "Epoch [67/100], Step [3000/10000], Loss: 1.9497\n",
            "Epoch [67/100], Step [4000/10000], Loss: 0.2286\n",
            "Epoch [67/100], Step [5000/10000], Loss: 0.5540\n",
            "Epoch [67/100], Step [6000/10000], Loss: 0.8294\n",
            "Epoch [67/100], Step [7000/10000], Loss: 0.0597\n",
            "Epoch [67/100], Step [8000/10000], Loss: 2.1591\n",
            "Epoch [67/100], Step [9000/10000], Loss: 0.0614\n",
            "Epoch [67/100], Step [10000/10000], Loss: 1.0548\n",
            "Epoch [68/100], Step [1000/10000], Loss: 0.6205\n",
            "Epoch [68/100], Step [2000/10000], Loss: 0.2669\n",
            "Epoch [68/100], Step [3000/10000], Loss: 1.6789\n",
            "Epoch [68/100], Step [4000/10000], Loss: 1.4162\n",
            "Epoch [68/100], Step [5000/10000], Loss: 3.2750\n",
            "Epoch [68/100], Step [6000/10000], Loss: 0.5662\n",
            "Epoch [68/100], Step [7000/10000], Loss: 0.1685\n",
            "Epoch [68/100], Step [8000/10000], Loss: 0.5054\n",
            "Epoch [68/100], Step [9000/10000], Loss: 0.3576\n",
            "Epoch [68/100], Step [10000/10000], Loss: 0.7737\n",
            "Epoch [69/100], Step [1000/10000], Loss: 0.6320\n",
            "Epoch [69/100], Step [2000/10000], Loss: 0.2545\n",
            "Epoch [69/100], Step [3000/10000], Loss: 1.4477\n",
            "Epoch [69/100], Step [4000/10000], Loss: 1.5859\n",
            "Epoch [69/100], Step [5000/10000], Loss: 1.6111\n",
            "Epoch [69/100], Step [6000/10000], Loss: 0.4765\n",
            "Epoch [69/100], Step [7000/10000], Loss: 0.8222\n",
            "Epoch [69/100], Step [8000/10000], Loss: 0.0852\n",
            "Epoch [69/100], Step [9000/10000], Loss: 0.4666\n",
            "Epoch [69/100], Step [10000/10000], Loss: 0.7928\n",
            "Epoch [70/100], Step [1000/10000], Loss: 0.2677\n",
            "Epoch [70/100], Step [2000/10000], Loss: 0.9195\n",
            "Epoch [70/100], Step [3000/10000], Loss: 0.1333\n",
            "Epoch [70/100], Step [4000/10000], Loss: 1.2535\n",
            "Epoch [70/100], Step [5000/10000], Loss: 0.4147\n",
            "Epoch [70/100], Step [6000/10000], Loss: 0.0819\n",
            "Epoch [70/100], Step [7000/10000], Loss: 0.4265\n",
            "Epoch [70/100], Step [8000/10000], Loss: 0.1566\n",
            "Epoch [70/100], Step [9000/10000], Loss: 1.0900\n",
            "Epoch [70/100], Step [10000/10000], Loss: 0.4165\n",
            "Epoch [71/100], Step [1000/10000], Loss: 0.6012\n",
            "Epoch [71/100], Step [2000/10000], Loss: 1.9793\n",
            "Epoch [71/100], Step [3000/10000], Loss: 0.1978\n",
            "Epoch [71/100], Step [4000/10000], Loss: 1.3583\n",
            "Epoch [71/100], Step [5000/10000], Loss: 1.7821\n",
            "Epoch [71/100], Step [6000/10000], Loss: 0.3641\n",
            "Epoch [71/100], Step [7000/10000], Loss: 0.2430\n",
            "Epoch [71/100], Step [8000/10000], Loss: 0.1469\n",
            "Epoch [71/100], Step [9000/10000], Loss: 0.1884\n",
            "Epoch [71/100], Step [10000/10000], Loss: 2.0153\n",
            "Epoch [72/100], Step [1000/10000], Loss: 0.7877\n",
            "Epoch [72/100], Step [2000/10000], Loss: 1.7059\n",
            "Epoch [72/100], Step [3000/10000], Loss: 1.9252\n",
            "Epoch [72/100], Step [4000/10000], Loss: 0.2025\n",
            "Epoch [72/100], Step [5000/10000], Loss: 0.6912\n",
            "Epoch [72/100], Step [6000/10000], Loss: 1.0354\n",
            "Epoch [72/100], Step [7000/10000], Loss: 0.0903\n",
            "Epoch [72/100], Step [8000/10000], Loss: 0.0080\n",
            "Epoch [72/100], Step [9000/10000], Loss: 0.5038\n",
            "Epoch [72/100], Step [10000/10000], Loss: 0.9796\n",
            "Epoch [73/100], Step [1000/10000], Loss: 0.3274\n",
            "Epoch [73/100], Step [2000/10000], Loss: 0.6074\n",
            "Epoch [73/100], Step [3000/10000], Loss: 0.4484\n",
            "Epoch [73/100], Step [4000/10000], Loss: 0.6609\n",
            "Epoch [73/100], Step [5000/10000], Loss: 0.3149\n",
            "Epoch [73/100], Step [6000/10000], Loss: 1.0684\n",
            "Epoch [73/100], Step [7000/10000], Loss: 0.5066\n",
            "Epoch [73/100], Step [8000/10000], Loss: 1.0667\n",
            "Epoch [73/100], Step [9000/10000], Loss: 0.3278\n",
            "Epoch [73/100], Step [10000/10000], Loss: 0.4478\n",
            "Epoch [74/100], Step [1000/10000], Loss: 0.5042\n",
            "Epoch [74/100], Step [2000/10000], Loss: 0.7220\n",
            "Epoch [74/100], Step [3000/10000], Loss: 0.4064\n",
            "Epoch [74/100], Step [4000/10000], Loss: 0.3496\n",
            "Epoch [74/100], Step [5000/10000], Loss: 0.5174\n",
            "Epoch [74/100], Step [6000/10000], Loss: 0.5375\n",
            "Epoch [74/100], Step [7000/10000], Loss: 0.8623\n",
            "Epoch [74/100], Step [8000/10000], Loss: 0.3826\n",
            "Epoch [74/100], Step [9000/10000], Loss: 0.3569\n",
            "Epoch [74/100], Step [10000/10000], Loss: 0.7853\n",
            "Epoch [75/100], Step [1000/10000], Loss: 0.1560\n",
            "Epoch [75/100], Step [2000/10000], Loss: 0.6364\n",
            "Epoch [75/100], Step [3000/10000], Loss: 0.2519\n",
            "Epoch [75/100], Step [4000/10000], Loss: 0.2695\n",
            "Epoch [75/100], Step [5000/10000], Loss: 0.9762\n",
            "Epoch [75/100], Step [6000/10000], Loss: 0.7774\n",
            "Epoch [75/100], Step [7000/10000], Loss: 0.6174\n",
            "Epoch [75/100], Step [8000/10000], Loss: 1.4532\n",
            "Epoch [75/100], Step [9000/10000], Loss: 0.4718\n",
            "Epoch [75/100], Step [10000/10000], Loss: 0.2320\n",
            "Epoch [76/100], Step [1000/10000], Loss: 0.1129\n",
            "Epoch [76/100], Step [2000/10000], Loss: 0.2875\n",
            "Epoch [76/100], Step [3000/10000], Loss: 0.8021\n",
            "Epoch [76/100], Step [4000/10000], Loss: 0.2226\n",
            "Epoch [76/100], Step [5000/10000], Loss: 0.3349\n",
            "Epoch [76/100], Step [6000/10000], Loss: 0.1360\n",
            "Epoch [76/100], Step [7000/10000], Loss: 0.8796\n",
            "Epoch [76/100], Step [8000/10000], Loss: 0.8477\n",
            "Epoch [76/100], Step [9000/10000], Loss: 0.6914\n",
            "Epoch [76/100], Step [10000/10000], Loss: 0.8051\n",
            "Epoch [77/100], Step [1000/10000], Loss: 0.4293\n",
            "Epoch [77/100], Step [2000/10000], Loss: 0.2913\n",
            "Epoch [77/100], Step [3000/10000], Loss: 1.0754\n",
            "Epoch [77/100], Step [4000/10000], Loss: 1.0500\n",
            "Epoch [77/100], Step [5000/10000], Loss: 1.5317\n",
            "Epoch [77/100], Step [6000/10000], Loss: 0.4497\n",
            "Epoch [77/100], Step [7000/10000], Loss: 0.1026\n",
            "Epoch [77/100], Step [8000/10000], Loss: 0.8852\n",
            "Epoch [77/100], Step [9000/10000], Loss: 0.8783\n",
            "Epoch [77/100], Step [10000/10000], Loss: 0.6467\n",
            "Epoch [78/100], Step [1000/10000], Loss: 2.1280\n",
            "Epoch [78/100], Step [2000/10000], Loss: 0.3074\n",
            "Epoch [78/100], Step [3000/10000], Loss: 1.2618\n",
            "Epoch [78/100], Step [4000/10000], Loss: 0.7864\n",
            "Epoch [78/100], Step [5000/10000], Loss: 1.1840\n",
            "Epoch [78/100], Step [6000/10000], Loss: 0.6485\n",
            "Epoch [78/100], Step [7000/10000], Loss: 0.7134\n",
            "Epoch [78/100], Step [8000/10000], Loss: 0.6487\n",
            "Epoch [78/100], Step [9000/10000], Loss: 0.9723\n",
            "Epoch [78/100], Step [10000/10000], Loss: 0.0942\n",
            "Epoch [79/100], Step [1000/10000], Loss: 0.7106\n",
            "Epoch [79/100], Step [2000/10000], Loss: 0.1682\n",
            "Epoch [79/100], Step [3000/10000], Loss: 0.6119\n",
            "Epoch [79/100], Step [4000/10000], Loss: 0.1068\n",
            "Epoch [79/100], Step [5000/10000], Loss: 0.0209\n",
            "Epoch [79/100], Step [6000/10000], Loss: 0.1968\n",
            "Epoch [79/100], Step [7000/10000], Loss: 0.5495\n",
            "Epoch [79/100], Step [8000/10000], Loss: 0.9249\n",
            "Epoch [79/100], Step [9000/10000], Loss: 2.3051\n",
            "Epoch [79/100], Step [10000/10000], Loss: 0.1071\n",
            "Epoch [80/100], Step [1000/10000], Loss: 0.5053\n",
            "Epoch [80/100], Step [2000/10000], Loss: 2.2343\n",
            "Epoch [80/100], Step [3000/10000], Loss: 1.9679\n",
            "Epoch [80/100], Step [4000/10000], Loss: 1.1197\n",
            "Epoch [80/100], Step [5000/10000], Loss: 1.1166\n",
            "Epoch [80/100], Step [6000/10000], Loss: 0.9991\n",
            "Epoch [80/100], Step [7000/10000], Loss: 0.5732\n",
            "Epoch [80/100], Step [8000/10000], Loss: 0.2000\n",
            "Epoch [80/100], Step [9000/10000], Loss: 0.7448\n",
            "Epoch [80/100], Step [10000/10000], Loss: 0.7877\n",
            "Epoch [81/100], Step [1000/10000], Loss: 0.1745\n",
            "Epoch [81/100], Step [2000/10000], Loss: 1.0230\n",
            "Epoch [81/100], Step [3000/10000], Loss: 1.0785\n",
            "Epoch [81/100], Step [4000/10000], Loss: 0.2209\n",
            "Epoch [81/100], Step [5000/10000], Loss: 0.7791\n",
            "Epoch [81/100], Step [6000/10000], Loss: 0.1431\n",
            "Epoch [81/100], Step [7000/10000], Loss: 0.5657\n",
            "Epoch [81/100], Step [8000/10000], Loss: 0.0531\n",
            "Epoch [81/100], Step [9000/10000], Loss: 0.2281\n",
            "Epoch [81/100], Step [10000/10000], Loss: 1.4669\n",
            "Epoch [82/100], Step [1000/10000], Loss: 0.1411\n",
            "Epoch [82/100], Step [2000/10000], Loss: 0.4732\n",
            "Epoch [82/100], Step [3000/10000], Loss: 1.0258\n",
            "Epoch [82/100], Step [4000/10000], Loss: 0.7487\n",
            "Epoch [82/100], Step [5000/10000], Loss: 0.0022\n",
            "Epoch [82/100], Step [6000/10000], Loss: 0.1845\n",
            "Epoch [82/100], Step [7000/10000], Loss: 0.9974\n",
            "Epoch [82/100], Step [8000/10000], Loss: 0.9751\n",
            "Epoch [82/100], Step [9000/10000], Loss: 0.9674\n",
            "Epoch [82/100], Step [10000/10000], Loss: 0.9042\n",
            "Epoch [83/100], Step [1000/10000], Loss: 1.6118\n",
            "Epoch [83/100], Step [2000/10000], Loss: 0.9584\n",
            "Epoch [83/100], Step [3000/10000], Loss: 0.5573\n",
            "Epoch [83/100], Step [4000/10000], Loss: 0.7761\n",
            "Epoch [83/100], Step [5000/10000], Loss: 0.5632\n",
            "Epoch [83/100], Step [6000/10000], Loss: 0.1612\n",
            "Epoch [83/100], Step [7000/10000], Loss: 0.0103\n",
            "Epoch [83/100], Step [8000/10000], Loss: 0.3744\n",
            "Epoch [83/100], Step [9000/10000], Loss: 2.6632\n",
            "Epoch [83/100], Step [10000/10000], Loss: 0.5709\n",
            "Epoch [84/100], Step [1000/10000], Loss: 1.2390\n",
            "Epoch [84/100], Step [2000/10000], Loss: 0.6945\n",
            "Epoch [84/100], Step [3000/10000], Loss: 0.9048\n",
            "Epoch [84/100], Step [4000/10000], Loss: 0.4988\n",
            "Epoch [84/100], Step [5000/10000], Loss: 0.7720\n",
            "Epoch [84/100], Step [6000/10000], Loss: 0.7070\n",
            "Epoch [84/100], Step [7000/10000], Loss: 0.5975\n",
            "Epoch [84/100], Step [8000/10000], Loss: 0.9764\n",
            "Epoch [84/100], Step [9000/10000], Loss: 0.0016\n",
            "Epoch [84/100], Step [10000/10000], Loss: 0.7679\n",
            "Epoch [85/100], Step [1000/10000], Loss: 0.5563\n",
            "Epoch [85/100], Step [2000/10000], Loss: 0.2219\n",
            "Epoch [85/100], Step [3000/10000], Loss: 0.9235\n",
            "Epoch [85/100], Step [4000/10000], Loss: 0.4178\n",
            "Epoch [85/100], Step [5000/10000], Loss: 1.2147\n",
            "Epoch [85/100], Step [6000/10000], Loss: 0.0311\n",
            "Epoch [85/100], Step [7000/10000], Loss: 0.3184\n",
            "Epoch [85/100], Step [8000/10000], Loss: 0.9142\n",
            "Epoch [85/100], Step [9000/10000], Loss: 0.9208\n",
            "Epoch [85/100], Step [10000/10000], Loss: 0.4374\n",
            "Epoch [86/100], Step [1000/10000], Loss: 0.4006\n",
            "Epoch [86/100], Step [2000/10000], Loss: 0.3549\n",
            "Epoch [86/100], Step [3000/10000], Loss: 0.5914\n",
            "Epoch [86/100], Step [4000/10000], Loss: 0.6294\n",
            "Epoch [86/100], Step [5000/10000], Loss: 0.9071\n",
            "Epoch [86/100], Step [6000/10000], Loss: 1.4639\n",
            "Epoch [86/100], Step [7000/10000], Loss: 0.1820\n",
            "Epoch [86/100], Step [8000/10000], Loss: 0.6345\n",
            "Epoch [86/100], Step [9000/10000], Loss: 0.2464\n",
            "Epoch [86/100], Step [10000/10000], Loss: 0.3125\n",
            "Epoch [87/100], Step [1000/10000], Loss: 2.2254\n",
            "Epoch [87/100], Step [2000/10000], Loss: 0.5372\n",
            "Epoch [87/100], Step [3000/10000], Loss: 1.4140\n",
            "Epoch [87/100], Step [4000/10000], Loss: 0.5224\n",
            "Epoch [87/100], Step [5000/10000], Loss: 0.4019\n",
            "Epoch [87/100], Step [6000/10000], Loss: 0.1589\n",
            "Epoch [87/100], Step [7000/10000], Loss: 0.9613\n",
            "Epoch [87/100], Step [8000/10000], Loss: 0.5591\n",
            "Epoch [87/100], Step [9000/10000], Loss: 1.0358\n",
            "Epoch [87/100], Step [10000/10000], Loss: 0.5799\n",
            "Epoch [88/100], Step [1000/10000], Loss: 0.5698\n",
            "Epoch [88/100], Step [2000/10000], Loss: 0.2903\n",
            "Epoch [88/100], Step [3000/10000], Loss: 0.3905\n",
            "Epoch [88/100], Step [4000/10000], Loss: 1.7121\n",
            "Epoch [88/100], Step [5000/10000], Loss: 0.9010\n",
            "Epoch [88/100], Step [6000/10000], Loss: 0.1877\n",
            "Epoch [88/100], Step [7000/10000], Loss: 0.2382\n",
            "Epoch [88/100], Step [8000/10000], Loss: 0.1141\n",
            "Epoch [88/100], Step [9000/10000], Loss: 0.9027\n",
            "Epoch [88/100], Step [10000/10000], Loss: 0.9856\n",
            "Epoch [89/100], Step [1000/10000], Loss: 0.3641\n",
            "Epoch [89/100], Step [2000/10000], Loss: 0.8790\n",
            "Epoch [89/100], Step [3000/10000], Loss: 1.0039\n",
            "Epoch [89/100], Step [4000/10000], Loss: 0.1565\n",
            "Epoch [89/100], Step [5000/10000], Loss: 0.2045\n",
            "Epoch [89/100], Step [6000/10000], Loss: 1.1645\n",
            "Epoch [89/100], Step [7000/10000], Loss: 0.7226\n",
            "Epoch [89/100], Step [8000/10000], Loss: 0.0912\n",
            "Epoch [89/100], Step [9000/10000], Loss: 0.9956\n",
            "Epoch [89/100], Step [10000/10000], Loss: 2.0840\n",
            "Epoch [90/100], Step [1000/10000], Loss: 0.3714\n",
            "Epoch [90/100], Step [2000/10000], Loss: 0.2710\n",
            "Epoch [90/100], Step [3000/10000], Loss: 0.3748\n",
            "Epoch [90/100], Step [4000/10000], Loss: 0.4526\n",
            "Epoch [90/100], Step [5000/10000], Loss: 1.6960\n",
            "Epoch [90/100], Step [6000/10000], Loss: 0.4164\n",
            "Epoch [90/100], Step [7000/10000], Loss: 1.6744\n",
            "Epoch [90/100], Step [8000/10000], Loss: 0.8935\n",
            "Epoch [90/100], Step [9000/10000], Loss: 0.0701\n",
            "Epoch [90/100], Step [10000/10000], Loss: 0.0706\n",
            "Epoch [91/100], Step [1000/10000], Loss: 1.4846\n",
            "Epoch [91/100], Step [2000/10000], Loss: 1.3820\n",
            "Epoch [91/100], Step [3000/10000], Loss: 0.2615\n",
            "Epoch [91/100], Step [4000/10000], Loss: 1.6791\n",
            "Epoch [91/100], Step [5000/10000], Loss: 0.2283\n",
            "Epoch [91/100], Step [6000/10000], Loss: 0.2583\n",
            "Epoch [91/100], Step [7000/10000], Loss: 1.1581\n",
            "Epoch [91/100], Step [8000/10000], Loss: 0.9404\n",
            "Epoch [91/100], Step [9000/10000], Loss: 2.6357\n",
            "Epoch [91/100], Step [10000/10000], Loss: 1.4379\n",
            "Epoch [92/100], Step [1000/10000], Loss: 0.0079\n",
            "Epoch [92/100], Step [2000/10000], Loss: 0.8693\n",
            "Epoch [92/100], Step [3000/10000], Loss: 0.3990\n",
            "Epoch [92/100], Step [4000/10000], Loss: 1.2052\n",
            "Epoch [92/100], Step [5000/10000], Loss: 0.9062\n",
            "Epoch [92/100], Step [6000/10000], Loss: 0.3900\n",
            "Epoch [92/100], Step [7000/10000], Loss: 0.6932\n",
            "Epoch [92/100], Step [8000/10000], Loss: 0.5775\n",
            "Epoch [92/100], Step [9000/10000], Loss: 0.5474\n",
            "Epoch [92/100], Step [10000/10000], Loss: 0.9613\n",
            "Epoch [93/100], Step [1000/10000], Loss: 1.3628\n",
            "Epoch [93/100], Step [2000/10000], Loss: 0.3079\n",
            "Epoch [93/100], Step [3000/10000], Loss: 0.5312\n",
            "Epoch [93/100], Step [4000/10000], Loss: 1.2398\n",
            "Epoch [93/100], Step [5000/10000], Loss: 1.7506\n",
            "Epoch [93/100], Step [6000/10000], Loss: 0.1225\n",
            "Epoch [93/100], Step [7000/10000], Loss: 0.0361\n",
            "Epoch [93/100], Step [8000/10000], Loss: 0.9731\n",
            "Epoch [93/100], Step [9000/10000], Loss: 1.1192\n",
            "Epoch [93/100], Step [10000/10000], Loss: 0.7495\n",
            "Epoch [94/100], Step [1000/10000], Loss: 0.5830\n",
            "Epoch [94/100], Step [2000/10000], Loss: 0.9359\n",
            "Epoch [94/100], Step [3000/10000], Loss: 0.4693\n",
            "Epoch [94/100], Step [4000/10000], Loss: 0.6094\n",
            "Epoch [94/100], Step [5000/10000], Loss: 0.1268\n",
            "Epoch [94/100], Step [6000/10000], Loss: 0.0996\n",
            "Epoch [94/100], Step [7000/10000], Loss: 0.1483\n",
            "Epoch [94/100], Step [8000/10000], Loss: 1.0311\n",
            "Epoch [94/100], Step [9000/10000], Loss: 1.1267\n",
            "Epoch [94/100], Step [10000/10000], Loss: 0.2636\n",
            "Epoch [95/100], Step [1000/10000], Loss: 0.0741\n",
            "Epoch [95/100], Step [2000/10000], Loss: 1.0375\n",
            "Epoch [95/100], Step [3000/10000], Loss: 0.5511\n",
            "Epoch [95/100], Step [4000/10000], Loss: 0.7346\n",
            "Epoch [95/100], Step [5000/10000], Loss: 0.2184\n",
            "Epoch [95/100], Step [6000/10000], Loss: 0.7676\n",
            "Epoch [95/100], Step [7000/10000], Loss: 1.6240\n",
            "Epoch [95/100], Step [8000/10000], Loss: 0.5627\n",
            "Epoch [95/100], Step [9000/10000], Loss: 0.5317\n",
            "Epoch [95/100], Step [10000/10000], Loss: 0.7342\n",
            "Epoch [96/100], Step [1000/10000], Loss: 0.6737\n",
            "Epoch [96/100], Step [2000/10000], Loss: 0.5579\n",
            "Epoch [96/100], Step [3000/10000], Loss: 0.3805\n",
            "Epoch [96/100], Step [4000/10000], Loss: 0.2270\n",
            "Epoch [96/100], Step [5000/10000], Loss: 0.2467\n",
            "Epoch [96/100], Step [6000/10000], Loss: 0.4289\n",
            "Epoch [96/100], Step [7000/10000], Loss: 0.0701\n",
            "Epoch [96/100], Step [8000/10000], Loss: 0.7928\n",
            "Epoch [96/100], Step [9000/10000], Loss: 0.2656\n",
            "Epoch [96/100], Step [10000/10000], Loss: 1.8863\n",
            "Epoch [97/100], Step [1000/10000], Loss: 1.2231\n",
            "Epoch [97/100], Step [2000/10000], Loss: 0.8490\n",
            "Epoch [97/100], Step [3000/10000], Loss: 0.3197\n",
            "Epoch [97/100], Step [4000/10000], Loss: 0.0794\n",
            "Epoch [97/100], Step [5000/10000], Loss: 0.5087\n",
            "Epoch [97/100], Step [6000/10000], Loss: 0.6412\n",
            "Epoch [97/100], Step [7000/10000], Loss: 0.0904\n",
            "Epoch [97/100], Step [8000/10000], Loss: 1.9348\n",
            "Epoch [97/100], Step [9000/10000], Loss: 0.2887\n",
            "Epoch [97/100], Step [10000/10000], Loss: 0.4344\n",
            "Epoch [98/100], Step [1000/10000], Loss: 0.5477\n",
            "Epoch [98/100], Step [2000/10000], Loss: 1.6792\n",
            "Epoch [98/100], Step [3000/10000], Loss: 0.5218\n",
            "Epoch [98/100], Step [4000/10000], Loss: 0.1007\n",
            "Epoch [98/100], Step [5000/10000], Loss: 0.9833\n",
            "Epoch [98/100], Step [6000/10000], Loss: 1.4198\n",
            "Epoch [98/100], Step [7000/10000], Loss: 0.3173\n",
            "Epoch [98/100], Step [8000/10000], Loss: 0.5242\n",
            "Epoch [98/100], Step [9000/10000], Loss: 0.4150\n",
            "Epoch [98/100], Step [10000/10000], Loss: 0.9638\n",
            "Epoch [99/100], Step [1000/10000], Loss: 0.0382\n",
            "Epoch [99/100], Step [2000/10000], Loss: 0.1024\n",
            "Epoch [99/100], Step [3000/10000], Loss: 0.4399\n",
            "Epoch [99/100], Step [4000/10000], Loss: 0.9952\n",
            "Epoch [99/100], Step [5000/10000], Loss: 0.8016\n",
            "Epoch [99/100], Step [6000/10000], Loss: 0.0007\n",
            "Epoch [99/100], Step [7000/10000], Loss: 0.5000\n",
            "Epoch [99/100], Step [8000/10000], Loss: 0.2987\n",
            "Epoch [99/100], Step [9000/10000], Loss: 2.4596\n",
            "Epoch [99/100], Step [10000/10000], Loss: 2.0871\n",
            "Epoch [100/100], Step [1000/10000], Loss: 0.6596\n",
            "Epoch [100/100], Step [2000/10000], Loss: 0.2258\n",
            "Epoch [100/100], Step [3000/10000], Loss: 0.2078\n",
            "Epoch [100/100], Step [4000/10000], Loss: 0.4199\n",
            "Epoch [100/100], Step [5000/10000], Loss: 0.4810\n",
            "Epoch [100/100], Step [6000/10000], Loss: 0.0188\n",
            "Epoch [100/100], Step [7000/10000], Loss: 0.7341\n",
            "Epoch [100/100], Step [8000/10000], Loss: 0.9484\n",
            "Epoch [100/100], Step [9000/10000], Loss: 0.1200\n",
            "Epoch [100/100], Step [10000/10000], Loss: 0.4842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 输出测试集精度\n",
        "#总精度及各类精度相关参数定义\n",
        "correct = 0\n",
        "total = 0\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        for label, prediction in zip(labels, predicted):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "print('Accuracy of the net on the train iamges is {:.2f} %'.format(accuracy_history[-1]))\n",
        "print('Accuracy of the net on the test iamges is {:.2f} %'.format(100 * correct / total))\n",
        "print('\\n')\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "id": "zU9l2rwJnRgT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9eb4d50-ec54-4828-c41f-6476e80a288c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the net on the train iamges is 76.37 %\n",
            "Accuracy of the net on the test iamges is 69.11 %\n",
            "\n",
            "\n",
            "Accuracy for class: plane is 71.6 %\n",
            "Accuracy for class: car   is 82.4 %\n",
            "Accuracy for class: bird  is 53.1 %\n",
            "Accuracy for class: cat   is 56.5 %\n",
            "Accuracy for class: deer  is 65.6 %\n",
            "Accuracy for class: dog   is 58.7 %\n",
            "Accuracy for class: frog  is 74.5 %\n",
            "Accuracy for class: horse is 72.6 %\n",
            "Accuracy for class: ship  is 74.4 %\n",
            "Accuracy for class: truck is 81.7 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  保存模型\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "metadata": {
        "id": "f1oRlNU4ndmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 可视化数据查看\n",
        "import itertools\n",
        "def print_label(input_dex):\n",
        "  plt.figure(figsize=(10, 4))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(range(num_epochs), loss_history)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Loss Curve')\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(range(num_epochs), accuracy_history)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.title('Accuracy Curve')\n",
        "\n",
        "  curve_fig = \"curve.png\"\n",
        "  plt.savefig(curve_fig)\n",
        "  curve = Image.open(curve_fig)\n",
        "  k = int(int(input_dex)/batch_size)\n",
        "  m = int(int(input_dex)%batch_size)\n",
        "  data_iter = iter(test_loader)\n",
        "  images, labels = next(itertools.islice(data_iter, k, k+1))\n",
        "  image = images[m].numpy()\n",
        "  image = np.transpose(image, (1,2,0))\n",
        "  show_img = image\n",
        "  imagebatch = image.reshape(-1,3,32,32)\n",
        "  # 转换为torch tensor\n",
        "  image_tensor = torch.from_numpy(imagebatch)\n",
        "  image_tensor = image_tensor.cuda()\n",
        "  # 调用模型进行评估\n",
        "  model.eval()\n",
        "  output = model(image_tensor)\n",
        "  precise, predicted = torch.max(output.data, 1)\n",
        "  pre = predicted.cpu().numpy()\n",
        "\n",
        "  return curve,classes[pre[0]],classes[labels[m].numpy()],show_img\n",
        "\n",
        "\n",
        "demo = gr.Interface(fn=print_label,\n",
        "                    inputs=gr.Textbox(label=\"Input 0-9999\"),\n",
        "                    outputs=[gr.outputs.Image(type=\"pil\",label=\"loss and acc\"),\n",
        "                    gr.Textbox(label=\"predict label\"),\n",
        "                    gr.Textbox(label=\"true label\"),\n",
        "                    gr.outputs.Image(type=\"pil\",label=\"Image\")]\n",
        "                    )\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "OLPAo19Dn2kL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "3c0527ba-ca37-498e-9b6f-61afa3520e39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-0ae37dc151c8>:42: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  outputs=[gr.outputs.Image(type=\"pil\",label=\"loss and acc\"),\n",
            "<ipython-input-35-0ae37dc151c8>:45: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  gr.outputs.Image(type=\"pil\",label=\"Image\")]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7861, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}