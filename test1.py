{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOsE/M5pgID9x3jwYotBZk1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reachel1/Freshman_learn_AI/blob/main/test1.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHO9ZcicUtvv",
        "outputId": "aaf43245-d4f6-4503-f650-853edfeaa345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "# Importing drive method from colab for accessing google drive\n",
        "from google.colab import drive\n",
        "# Mounting drive\n",
        "# This will require authentication : Follow the steps as guided\n",
        "drive.mount('/Data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP7T3VlxWiCp",
        "outputId": "6871fa9e-c6e2-4f3a-8469-4352c44bcc82"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /Data/'My Drive'/dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBSdroL7fSA8",
        "outputId": "0b17b515-6b7a-46de-b7b0-3316bd31d33c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cifar-10-python.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt # plt 用于显示图片\n",
        "import matplotlib.image as mpimg # mpimg 用于读取图片\n",
        "import numpy as np\n",
        "\n",
        "#resize功能\n",
        "from scipy import misc\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "num_classes = 10\n",
        "# 超参数设置\n",
        "num_epochs = 20\n",
        "batch_size = 5\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "y9tms4NwcMFw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "# 数据预处理\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "    ])"
      ],
      "metadata": {
        "id": "TxoYrdwKits0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import os\n",
        "import pickle\n",
        "def load_cifar_data(filepath,train):\n",
        "  if os.path.isdir(filepath):\n",
        "    if train:\n",
        "      data = []\n",
        "      label = []\n",
        "      for name in sorted(os.listdir(filepath))[1:6]:\n",
        "        print(name)\n",
        "        data0 = list(load_cifar_batch(os.path.join(filepath,name)).get(b'data'))\n",
        "        label0 = list(load_cifar_batch(os.path.join(filepath,name)).get(b'labels'))\n",
        "        data.append(data0)\n",
        "        label.append(label0)\n",
        "      dic = (data,label)\n",
        "      return dic\n",
        "    else:\n",
        "      print(sorted(os.listdir(filepath))[7])\n",
        "      data = list(load_cifar_batch(os.path.join(filepath,sorted(os.listdir(filepath))[7])).get(b'data'))\n",
        "      label = list(load_cifar_batch(os.path.join(filepath,sorted(os.listdir(filepath))[7])).get(b'labels'))\n",
        "      dic = (data,label)\n",
        "      return dic\n",
        "def load_cifar_batch(filename):\n",
        "  with open(filename,'rb') as fo:\n",
        "          data0 = pickle.load(fo,encoding='bytes')\n",
        "          #data0['data'] = data0['data'].reshape(10000,3,32,32).transpose(0,2,3,1).astype(\"float\")\n",
        "          return data0\n",
        "train_data = load_cifar_data('/Data/My Drive/dataset/cifar-10-batches-py',True)\n",
        "test_data = load_cifar_data('/Data/My Drive/dataset/cifar-10-batches-py',False)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fOPqPK4pkYj",
        "outputId": "60fb2178-4c25-48dd-a163-d0e47490793e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_batch_1\n",
            "data_batch_2\n",
            "data_batch_3\n",
            "data_batch_4\n",
            "data_batch_5\n",
            "test_batch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LOAD_CIFAR = True\n",
        "DOWNLOAD_CIFAR = False\n",
        "# 从data继承读取数据集的类\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 训练数据集\n",
        "train_data = torchvision.datasets.CIFAR10(\n",
        "    root='/Data/My Drive/dataset',\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=DOWNLOAD_CIFAR,\n",
        ")\n",
        "\n",
        "# 测试数据集\n",
        "test_data = torchvision.datasets.CIFAR10(\n",
        "    root='/Data/My Drive/dataset',\n",
        "    train=False,\n",
        "    transform=transform\n",
        ")"
      ],
      "metadata": {
        "id": "c0IVXuIfcbtn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练数据加载器\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "# 测试数据加载器\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "UJDkPPJiBbqH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 查看数据,取一组batch\n",
        "data_iter = iter(test_loader)\n",
        "\n",
        "images, labels = next(data_iter)\n",
        "# 取batch中的一张图像\n",
        "idx = 2\n",
        "image = images[idx].numpy()\n",
        "image = np.transpose(image, (1,2,0))\n",
        "plt.imshow(image)\n",
        "print(classes[labels[idx].numpy()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "OqUP_tXQjYk_",
        "outputId": "3e659816-2183-4841-f555-3e6bf333db47"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ship\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAomElEQVR4nO3df3TU9Z3v8dcMk5khJJkYQxLSBARRUDH0lirNVSlKyo+966Jyd7XtnmLXo0caPFW225Y9rVa7e2Ltua3WQ/HscVe25xax7Cl69VSsYgnHFmjJyqX4IxrEAheSFGhm8oPJJJnv/cNt2ig/Pu+Q8EnC83HO9xwy8+adz3e+k3nlm5l5TygIgkAAAJxjYd8LAACcnwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF5EfC/gw7LZrA4fPqz8/HyFQiHfywEAGAVBoPb2dpWXlyscPvV5zogLoMOHD6uystL3MgAAZ+ngwYOqqKg45fXDFkBr1qzRd7/7XTU3N2v27Nl6/PHHdfXVV5/x/+Xn50uSrvryBkViuW7fLNPtvK6mt/c410pSezLpXDv/hhpT7ymVH3OuHZ8zztR7Qm7UuTYvx3Y3mBCx1cfDWefaTPcfTL1Li8c7146P2v7iHDfU54xzv70l6f333zPVT5pU5lw7frzjz81/iYdznGvDEdttmO7tdK413q1MImFb82O/P2aqj8VjzrWJRMLUu7PD/TZMd7abel+QuMC5Nhxxv590dLTr2qur+h/PT2VYDvkzzzyjVatW6YknntDcuXP16KOPatGiRWpsbFRJSclp/+8f/+wWieUqEpvg9g1D7g/O4Zy4c60khSJp59qcuON6/0t0fJ5zbSxqfNA3BFDc2Hv8MAbQuHE9pt65ee4PtrnDGEDRiC2AcifY7isT8k7/gzygd641gNzXbg2gSK97/UgKoPQJ919qJSked39cyc8vMPUOhQy3Ycg22vNMAfHnwsb7uKQzPo0yLC9C+N73vqc777xTX/ziF3X55ZfriSeeUG5urv7t3/5tOL4dAGAUGvIAymQyamhoUE3Nn/4cFQ6HVVNTo+3bt3+kvru7W6lUasAGABj7hjyAjh49qr6+PpWWlg64vLS0VM3NzR+pr6urUyKR6N94AQIAnB+8vw9o9erVSiaT/dvBgwd9LwkAcA4M+dN+xcXFGjdunFpaWgZc3tLSorKyj76SJxaLKRZzfwUJAGBsGPIzoGg0qjlz5mjLli39l2WzWW3ZskXV1dVD/e0AAKPUsLzwcdWqVVq+fLk++clP6uqrr9ajjz6qzs5OffGLXxyObwcAGIWGJYBuvfVW/f73v9f999+v5uZmffzjH9fmzZs/8sIEAMD5a9je+rVy5UqtXLly0P+/9MJ85cTd3qgZybq/CSzVansTWG/HUefawjzbXzRLitzXHTf+sTSijHNtQdx2NyjKs9VH1Otcm8q4r1uSCuKFzrV5ucZ1G27zeNz2Jr08wxuFP1iL+5t5w4bbW5IK8tzfEG18z7Jaj3Y511ofjPIM687KeHsb33Cba3gjqvWNwl2Gt6bEjW+4LSowPB6eZqbbR0qzbvdB76+CAwCcnwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXw/gp7GcnFupW1HGshGXsTF7UNqYkN2IYaRNJm3oXGtaSG7X9rpA67j5CKBq1jScqjBeZ6rtSbc61Edluw2zavXc27D4uRZIych9/k5dru00so3UkSVn3+2HY+GOd6XUfl3P0qPv9SpLeefNN59qqmdNMvcOGcTnRPNuxjxqOvSRFDcfTOBFKccN+pjK2n5+44XGlq8u9d9DjVssZEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8GLEzoLLi2QUjeQ41famO5z7RuU+90qSciOGeW3G3pGM+1ytvFzbvLZw1H12WG7E/fb7oN42V6s37N4/0psy9U63GWb1RUtMvds63NddXGybBRc1zPeSJPUaZhhmbXPMWtvanGv/5V+eNPXuMswknFbxJVPvggL3hy/jKEWFs7aZkep1vx9GjHPmwoY5gL29xlmKhnVnDb2zvd1OdZwBAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF6M2FE8E+JhxeJu+ZgNu+dobsQ2BkMZ99EwUdnGYIQNvXMVNfXuSrc512Z6bb+HRCvyTPXhrGFEUa9tLFBv2jAyJWMbZ9R86H3n2rLiQlPviHFcTrrD/b4Sz7WNSnr/qPu4nKZD7rWSVBR3f4jpsP34qKPL/djH82y3d9Y4iieTcb+Ppw2jwySpw3Ds8+K2Y581jHjqzRhGAjnWcgYEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8GLGz4PLDGcXDOU61mbD7EKnciG3GU1eqzbk2YpwFl+019A7bZjzFI+5riUeNs+DChtlukrKGmXeScUZar/taMrKtu/nwIefa1wz3E0mKGGakSbbZZP+9otjU+50333Surd/4v029r1v6N861vcb7YSrjPlOtMJtr6t1rnEl4/Kh7fV7c9hiU7nKfvxeN2459V9r9cSKVcl9HR4fb7cEZEADAiyEPoG9961sKhUIDtpkzZw71twEAjHLD8ie4K664Qq+88sqfvklkxP6lDwDgybAkQyQSUVlZ2XC0BgCMEcPyHNC7776r8vJyTZs2TZ///Od14MCBU9Z2d3crlUoN2AAAY9+QB9DcuXO1bt06bd68WWvXrtX+/ft13XXXqb29/aT1dXV1SiQS/VtlZeVQLwkAMAINeQAtWbJEf/3Xf62qqiotWrRIP/vZz9TW1qaf/OQnJ61fvXq1kslk/3bw4MGhXhIAYAQa9lcHFBYW6tJLL1VTU9NJr4/FYorFYsO9DADACDPs7wPq6OjQvn37NGnSpOH+VgCAUWTIA+grX/mK6uvr9f777+tXv/qVbr75Zo0bN06f/exnh/pbAQBGsSH/E9yhQ4f02c9+VseOHdPEiRN17bXXaseOHZo4caKpTzSdVDTU41TbaxhVEUkbR2wccB/HIsOoCknKRtxHw0SLbKN48gwjavLiUVPvcFerqT5juV0yxrXE3X+HyoZtx7619dSv3vywPXtsx76kwvY2hazhvXTZuO2+0tFsWPu4AlPvN99/37n2yQ22MT8lBe73lcU1NabeceNIqNTRw861RXFb797UcefaTNo25idjmVDUZngs7HR7/BnyANqwYcNQtwQAjEHMggMAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8GPaPYxisRDir8WG3mUnhrPtsJessuIJsxrm2rNeW55PlPssqcsA2f62g133dhbbRVIocdZ9NJUmRNvdZY3kR26wxZdxv845DtmNfXuK+lmnTp5t6v7pjt6n+lV+51//+jX829R5Ox/7vL51rnzbUWj327e+Y6u/58pdN9VdXXepcmyp0/9mUpLZW95/9jtbdpt4V2cuda8PH3efdhbvSbnXOHQEAGEIEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADAixE7imfna68pmuM2qqary33EyuFD7uMkJCnTlXKu/fWvf23q/V5BrnNta/MhU+9Zl7qPhikrKTT1jsZto0Q6HMdySFI8r8jUOxLPc65tNYwEkqS2iOH3s2zc1Pu1XW+b6n//RpOhOmTqbRMMY2/ruodvLY8/9pip/jOfvs659vKLKky9jze/71zbeugdU++uW/7Kubb5wHvOtelMr1MdZ0AAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMCLUBAEwzncySyVSimRSPhexqgXMszVKkm4z1OTpALjry25cbeZfpIULygw9Y7EC51rj7a5zwyUpPhk95ld2aht3b/9tW0WnNrbDMXuMwY/4D6rT7LN07P9jttt7O2udMoVpvrWA5bZe9Inqy51rs12HDf1btj3/0z1FuNz3B8nTvTYoyKZTKrgND/TnAEBALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvIr4XgOERyH1uU0uy3dS7xboYiyPHhrF5vq281TBT7cQhW2/ZZo3JcDzPH+5zzFpbD5g6B4FtLl0q1epce/cX/sbUu+HBx031FoOZ7zaUOAMCAHhhDqBt27bpxhtvVHl5uUKhkJ599tkB1wdBoPvvv1+TJk3S+PHjVVNTo3fffXeo1gsAGCPMAdTZ2anZs2drzZo1J73+kUce0Q9+8AM98cQT2rlzpyZMmKBFixYpnbaMfAcAjHXm54CWLFmiJUuWnPS6IAj06KOP6hvf+IaWLl0qSfrRj36k0tJSPfvss7rtttvObrUAgDFjSJ8D2r9/v5qbm1VTU9N/WSKR0Ny5c7V9+/aT/p/u7m6lUqkBGwBg7BvSAGpubpYklZaWDri8tLS0/7oPq6urUyKR6N8qKyuHckkAgBHK+6vgVq9erWQy2b8dPHjQ95IAAOfAkAZQWVmZJKmlZeA7RVpaWvqv+7BYLKaCgoIBGwBg7BvSAJo6darKysq0ZcuW/stSqZR27typ6urqofxWAIBRzvwquI6ODjU1/eld3Pv379fu3btVVFSkyZMn695779U//dM/6ZJLLtHUqVP1zW9+U+Xl5brpppuGct0AgFHOHEC7du3S9ddf3//1qlWrJEnLly/XunXr9NWvflWdnZ2666671NbWpmuvvVabN29WPB4fulWPVCHjCyiCLkPxcI6oMRp3ia2+bxjfiDz+Mvfagmm23pbDI+songpj/clfxHNyPcbeI8MV1/0PU/1fzvukc21bh/uoHEl69ecvmOob33J/7jo3z/awe9UVk5xrf/PGEVNv38wBNH/+fAXBqecHhUIhPfTQQ3rooYfOamEAgLHN+6vgAADnJwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOCFeRTPyJRjqLXu8gn30uCwsfcIyf8LrzKVT5hsm6nWmZnlXtz0nqm3eg0zBjO21mq3zA+zzGqTpOPGest8t3HG3iWG2g5j76hz5eXTbPPxCnPdf5Yvn1Zl6l2Rl2uq/+Y//y/n2m0/32rq/fHpM51rR9ssuBHyCAgAON8QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAAL8bIKB7DOBa1D9sqpL5hrh8mx/aYyjuPGcfITLzIvTZaaOvdbhgN05W29c4x3K96im29VWCst/yu2GtrnTCsPdtlal15qfuYn+I897E9ktR66H3n2vRR26ikcNZUrutnX+Zc+9OXXzf1/svrr7EtZhThDAgA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHgxRmbBGeaBnTfy3UtzPmFrXVZkKh9XVOhcGy9wr5WkeG6ec202bPt9K5txHwiWTttmpE2uKDPVRyLua+nN2H4e4ln3+t6Ubaba5AL32zybTpl6ZzLu9R1p25y5422227C4fLJz7Yn/+5ap98Zf/NJUP5pwBgQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4MYJH8cySNM6xttfQN9e4juOG2kJT59hln3SuDcdt684rdB+Xk2ccf1NYGDfVR+PudzPruJxw1FBv7B0Nu6/7aKttRE0ka7nPSgW57rd51jC2R5KOHnjHufbdrT8z9W7Kdb/NLyqy3a+qLr3IubawsNjUu60jbarPxgtM9fgAZ0AAAC8IIACAF+YA2rZtm2688UaVl5crFArp2WefHXD97bffrlAoNGBbvHjxUK0XADBGmAOos7NTs2fP1po1a05Zs3jxYh05cqR/e/rpp89qkQCAscf8IoQlS5ZoyZIlp62JxWIqK7N93gkA4PwyLM8Bbd26VSUlJZoxY4ZWrFihY8eOnbK2u7tbqVRqwAYAGPuGPIAWL16sH/3oR9qyZYu+853vqL6+XkuWLFFfX99J6+vq6pRIJPq3ysrKoV4SAGAEGvL3Ad122239/77yyitVVVWliy++WFu3btWCBQs+Ur969WqtWrWq/+tUKkUIAcB5YNhfhj1t2jQVFxerqanppNfHYjEVFBQM2AAAY9+wB9ChQ4d07NgxTZo0abi/FQBgFDH/Ca6jo2PA2cz+/fu1e/duFRUVqaioSA8++KCWLVumsrIy7du3T1/96lc1ffp0LVq0aEgXDgAY3cwBtGvXLl1//fX9X//x+Zvly5dr7dq12rNnj/793/9dbW1tKi8v18KFC/Xtb39bsVjM9o1yy6VQjlttZ4d734mFpmXEKmY51/ambTO44nl5zrVFZeWm3tlw1Lk2ErXN4OrNuveWpIjlbma8R2YN9VnTzEApnXafA5jOHDX1PvTO26Z6yxHKNc6Caznwpntx0G7q3dNpKC6x/fn9aIf78ew1zCOUpLTpFpeyGffbfPwFtue4y4rc50De/j9rTL1feOVXzrW/aXjd1NuFOYDmz5+vIAhOef1LL710VgsCAJwfmAUHAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeBEKTjdXx4NUKqVEIiFprpwnBU0wzEkrtM140tH33Gu737f1NkxCGvff/srU+aLpM51r47nuM+kkKSLbLLhs2H1mV0fG9om4RztanWvbU7Z5beo45F6b6bL1Nn7y74Q893lgs6ZPty2lzX3m3Xtv7zX1vqis2Ll2crl7rSR1GG7DcG6RrXeb7fhEwu6z4NJdtvtKcYH740Su2ky9U4Yxmse73H+O+/r69E7TO0omk6f9iB3OgAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAv3Gc8nGvhXCnkuLxOw3iQzreNC7FMKppoa52oci7tS9vGd3R1tDnX9mYzpt6HjrqPv5GkvuOGEThp2wgURdxHoCjXdne3jL8pLLGNeinKtY2Eajva7FzbK/eRKZJUUlbmXBsN235nzYu6j22KRm3HJ9dwfNrSaVPvsGHdkhQ23OZdXYb5N5J2vOM+Dqz7RKept8UVV85xrg36+pzqOAMCAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABejNxZcNkmuefj74ZvHYkbnUtDM2eZWhcXu88Paztum7925L0m9+Iu4/y1rG2WlYoLnUvHVxSYWpcUuNcXGWaHSVLcMPcsk7HN6kunbbdhV5f7vL62iHHumeH30EjENiMtk3Gf1ddlGOsnSfFonnNttrfN1LstZavf/6ZhxmTPH0y9R4r3mo871wZZt4PJGRAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgRSgIgsD3Iv5cKpVSIpGQchdLoRy3/+Q49kGSFLWNElFhoa3epNe9tMA2RkZt7mMzZBwLk18101RfUOg+MiUasc1jyfa6j53pTdtG1LQdOuxc22kZfSRJ7W/a6tXnXJlz2TWmzjMvusi5Nmwcl2P5aetNG34eJKVS7vfxfbt/ZeqtoNNWj5NKJpMqOM24LM6AAABemAKorq5OV111lfLz81VSUqKbbrpJjY2NA2rS6bRqa2t14YUXKi8vT8uWLVNLS8uQLhoAMPqZAqi+vl61tbXasWOHXn75ZfX09GjhwoXq7PzT6ep9992n559/Xhs3blR9fb0OHz6sW265ZcgXDgAY3Uwfx7B58+YBX69bt04lJSVqaGjQvHnzlEwm9a//+q9av369brjhBknSU089pcsuu0w7duzQpz71qaFbOQBgVDur54CSyaQkqajog8+1aWhoUE9Pj2pqavprZs6cqcmTJ2v79u0n7dHd3a1UKjVgAwCMfYMOoGw2q3vvvVfXXHONZs364IPYmpubFY1GVfihV46Vlpaqubn5pH3q6uqUSCT6t8rKysEuCQAwigw6gGpra7V3715t2LDhrBawevVqJZPJ/u3gwYNn1Q8AMDoM6iO5V65cqRdeeEHbtm1TRUVF/+VlZWXKZDJqa2sbcBbU0tKisrKyk/aKxWKKxWKDWQYAYBQznQEFQaCVK1dq06ZNevXVVzV16tQB18+ZM0c5OTnasmVL/2WNjY06cOCAqqurh2bFAIAxwXQGVFtbq/Xr1+u5555Tfn5+//M6iURC48ePVyKR0B133KFVq1apqKhIBQUFuueee1RdXc0r4AAAA5gCaO3atZKk+fPnD7j8qaee0u233y5J+v73v69wOKxly5apu7tbixYt0g9/+MMhWSwAYOwYubPg4gvdZ8EVnnrW0EfEjTPVsob5VOZBWYb8jxtn2GUz7rWtJ3+F4imdZrbTSRl2c1ye7WnJSK57fTZjmwXXs/0ZU/2IMd72StLL5l3rXHv86FFTb0t9z+/eN/WW/mCsx7nGLDgAwIhEAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvBjUxzGcE+mfu9fm3uheW5hnW4dhEo85z3sN43L27rX17jtgKD5u693SY6s36EtcZaqPz5rpXJvpNR1MSZaPCek29h5GJ9pM5W+99H8M1Z2m3sDpcAYEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8GLmz4CyOve9em+kwNs+6l0bjttbpLvfavt/Yeo9WacN8PEmRtGG+W8ZwLCX1jJ/uXnyiydR7eGfHtQ9jb2DocAYEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeDE2RvGo2b20y5i5kah7bdR4c+YaRvd0hmy9FRhqxxl7FxjrC91Luw3jiSQlG541VJ8w9QYwvDgDAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXoyRWXC97qVhw2w3SerOGGoP23rLMvfMMDdOkm3uWZ+x9x+GuR7A+YAzIACAF6YAqqur01VXXaX8/HyVlJTopptuUmNj44Ca+fPnKxQKDdjuvvvuIV00AGD0MwVQfX29amtrtWPHDr388svq6enRwoUL1dnZOaDuzjvv1JEjR/q3Rx55ZEgXDQAY/UzPAW3evHnA1+vWrVNJSYkaGho0b968/stzc3NVVlY2NCsEAIxJZ/UcUDKZlCQVFRUNuPzHP/6xiouLNWvWLK1evVpdXad+sr27u1upVGrABgAY+wb9KrhsNqt7771X11xzjWbNmtV/+ec+9zlNmTJF5eXl2rNnj772ta+psbFRP/3pT0/ap66uTg8++OBglwEAGKVCQRBYPru534oVK/Tiiy/qtddeU0VFxSnrXn31VS1YsEBNTU26+OKLP3J9d3e3uru7+79OpVKqrKw0ruYC99Kc6bbWPYaXYctSK9leht1q7M3HTwPwK5lMqqCg4JTXD+oMaOXKlXrhhRe0bdu204aPJM2dO1eSThlAsVhMsVhsMMsAAIxipgAKgkD33HOPNm3apK1bt2rq1Kln/D+7d++WJE2aNGlQCwQAjE2mAKqtrdX69ev13HPPKT8/X83NzZKkRCKh8ePHa9++fVq/fr3+4i/+QhdeeKH27Nmj++67T/PmzVNVVdWw7AAAYHQyPQcUCoVOevlTTz2l22+/XQcPHtTf/u3fau/evers7FRlZaVuvvlmfeMb3zjt3wH/XCqVUiKRcF3Sf+E5oI/iOSAAfp3pOaBBvwhhuAwugAAAI82ZAohZcAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8MAXQ2rVrVVVVpYKCAhUUFKi6ulovvvhi//XpdFq1tbW68MILlZeXp2XLlqmlpWXIFw0AGP1MAVRRUaGHH35YDQ0N2rVrl2644QYtXbpUb7zxhiTpvvvu0/PPP6+NGzeqvr5ehw8f1i233DIsCwcAjHLBWbrggguCJ598MmhrawtycnKCjRs39l/31ltvBZKC7du3O/dLJpOBJDY2Nja2Ub4lk8nTPt4P+jmgvr4+bdiwQZ2dnaqurlZDQ4N6enpUU1PTXzNz5kxNnjxZ27dvP2Wf7u5upVKpARsAYOwzB9Bvf/tb5eXlKRaL6e6779amTZt0+eWXq7m5WdFoVIWFhQPqS0tL1dzcfMp+dXV1SiQS/VtlZaV5JwAAo485gGbMmKHdu3dr586dWrFihZYvX64333xz0AtYvXq1kslk/3bw4MFB9wIAjB4R63+IRqOaPn26JGnOnDn6zW9+o8cee0y33nqrMpmM2traBpwFtbS0qKys7JT9YrGYYrGYfeUAgFHtrN8HlM1m1d3drTlz5ignJ0dbtmzpv66xsVEHDhxQdXX12X4bAMAYYzoDWr16tZYsWaLJkyervb1d69ev19atW/XSSy8pkUjojjvu0KpVq1RUVKSCggLdc889qq6u1qc+9anhWj8AYJQyBVBra6u+8IUv6MiRI0okEqqqqtJLL72kz3zmM5Kk73//+wqHw1q2bJm6u7u1aNEi/fCHPxyWhQMARrdQEASB70X8uVQqpUQi4XsZAICzlEwmVVBQcMrrmQUHAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPBixAXQCBvMAAAYpDM9no+4AGpvb/e9BADAEDjT4/mImwWXzWZ1+PBh5efnKxQK9V+eSqVUWVmpgwcPnna20GjHfo4d58M+SuznWDMU+xkEgdrb21VeXq5w+NTnOeYPpBtu4XBYFRUVp7y+oKBgTB/8P2I/x47zYR8l9nOsOdv9dBkqPeL+BAcAOD8QQAAAL0ZNAMViMT3wwAOKxWK+lzKs2M+x43zYR4n9HGvO5X6OuBchAADOD6PmDAgAMLYQQAAALwggAIAXBBAAwItRE0Br1qzRRRddpHg8rrlz5+rXv/617yUNqW9961sKhUIDtpkzZ/pe1lnZtm2bbrzxRpWXlysUCunZZ58dcH0QBLr//vs1adIkjR8/XjU1NXr33Xf9LPYsnGk/b7/99o8c28WLF/tZ7CDV1dXpqquuUn5+vkpKSnTTTTepsbFxQE06nVZtba0uvPBC5eXladmyZWppafG04sFx2c/58+d/5HjefffdnlY8OGvXrlVVVVX/m02rq6v14osv9l9/ro7lqAigZ555RqtWrdIDDzyg//zP/9Ts2bO1aNEitba2+l7akLriiit05MiR/u21117zvaSz0tnZqdmzZ2vNmjUnvf6RRx7RD37wAz3xxBPauXOnJkyYoEWLFimdTp/jlZ6dM+2nJC1evHjAsX366afP4QrPXn19vWpra7Vjxw69/PLL6unp0cKFC9XZ2dlfc9999+n555/Xxo0bVV9fr8OHD+uWW27xuGo7l/2UpDvvvHPA8XzkkUc8rXhwKioq9PDDD6uhoUG7du3SDTfcoKVLl+qNN96QdA6PZTAKXH311UFtbW3/1319fUF5eXlQV1fncVVD64EHHghmz57texnDRlKwadOm/q+z2WxQVlYWfPe73+2/rK2tLYjFYsHTTz/tYYVD48P7GQRBsHz58mDp0qVe1jNcWltbA0lBfX19EAQfHLucnJxg48aN/TVvvfVWICnYvn27r2WetQ/vZxAEwac//engy1/+sr9FDZMLLrggePLJJ8/psRzxZ0CZTEYNDQ2qqanpvywcDqumpkbbt2/3uLKh9+6776q8vFzTpk3T5z//eR04cMD3kobN/v371dzcPOC4JhIJzZ07d8wdV0naunWrSkpKNGPGDK1YsULHjh3zvaSzkkwmJUlFRUWSpIaGBvX09Aw4njNnztTkyZNH9fH88H7+0Y9//GMVFxdr1qxZWr16tbq6unwsb0j09fVpw4YN6uzsVHV19Tk9liNuGOmHHT16VH19fSotLR1weWlpqd5++21Pqxp6c+fO1bp16zRjxgwdOXJEDz74oK677jrt3btX+fn5vpc35JqbmyXppMf1j9eNFYsXL9Ytt9yiqVOnat++ffrHf/xHLVmyRNu3b9e4ceN8L88sm83q3nvv1TXXXKNZs2ZJ+uB4RqNRFRYWDqgdzcfzZPspSZ/73Oc0ZcoUlZeXa8+ePfra176mxsZG/fSnP/W4Wrvf/va3qq6uVjqdVl5enjZt2qTLL79cu3fvPmfHcsQH0PliyZIl/f+uqqrS3LlzNWXKFP3kJz/RHXfc4XFlOFu33XZb/7+vvPJKVVVV6eKLL9bWrVu1YMECjysbnNraWu3du3fUP0d5Jqfaz7vuuqv/31deeaUmTZqkBQsWaN++fbr44ovP9TIHbcaMGdq9e7eSyaT+4z/+Q8uXL1d9ff05XcOI/xNccXGxxo0b95FXYLS0tKisrMzTqoZfYWGhLr30UjU1NfleyrD447E7346rJE2bNk3FxcWj8tiuXLlSL7zwgn7xi18M+NiUsrIyZTIZtbW1DagfrcfzVPt5MnPnzpWkUXc8o9Gopk+frjlz5qiurk6zZ8/WY489dk6P5YgPoGg0qjlz5mjLli39l2WzWW3ZskXV1dUeVza8Ojo6tG/fPk2aNMn3UobF1KlTVVZWNuC4plIp7dy5c0wfV0k6dOiQjh07NqqObRAEWrlypTZt2qRXX31VU6dOHXD9nDlzlJOTM+B4NjY26sCBA6PqeJ5pP09m9+7dkjSqjufJZLNZdXd3n9tjOaQvaRgmGzZsCGKxWLBu3brgzTffDO66666gsLAwaG5u9r20IfP3f//3wdatW4P9+/cHv/zlL4OampqguLg4aG1t9b20QWtvbw9ef/314PXXXw8kBd/73veC119/Pfjd734XBEEQPPzww0FhYWHw3HPPBXv27AmWLl0aTJ06NThx4oTnlducbj/b29uDr3zlK8H27duD/fv3B6+88krwiU98IrjkkkuCdDrte+nOVqxYESQSiWDr1q3BkSNH+reurq7+mrvvvjuYPHly8Oqrrwa7du0Kqqurg+rqao+rtjvTfjY1NQUPPfRQsGvXrmD//v3Bc889F0ybNi2YN2+e55XbfP3rXw/q6+uD/fv3B3v27Am+/vWvB6FQKPj5z38eBMG5O5ajIoCCIAgef/zxYPLkyUE0Gg2uvvrqYMeOHb6XNKRuvfXWYNKkSUE0Gg0+9rGPBbfeemvQ1NTke1ln5Re/+EUg6SPb8uXLgyD44KXY3/zmN4PS0tIgFosFCxYsCBobG/0uehBOt59dXV3BwoULg4kTJwY5OTnBlClTgjvvvHPU/fJ0sv2TFDz11FP9NSdOnAi+9KUvBRdccEGQm5sb3HzzzcGRI0f8LXoQzrSfBw4cCObNmxcUFRUFsVgsmD59evAP//APQTKZ9Ltwo7/7u78LpkyZEkSj0WDixInBggUL+sMnCM7dseTjGAAAXoz454AAAGMTAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALz4/3x/fCKGwfNXAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 搭建卷积神经网络模型\n",
        "# 三个卷积层\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "            # 32*32*3\n",
        "        self.conv1 = nn.Sequential(\n",
        "            # 卷积层计算\n",
        "            nn.Conv2d(3, 5, kernel_size=5, stride=1, padding=2),\n",
        "            #  批归一化\n",
        "            nn.BatchNorm2d(5),\n",
        "            #ReLU激活函数\n",
        "            nn.ReLU(),\n",
        "            # 池化层：最大池化\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1))\n",
        "            # 31*31*5\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(5, 8, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1))# 搭建卷积神经网络模型\n",
        "            # 30*30*8\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(8, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))# 搭建卷积神经网络模型\n",
        "            # 15*15*16\n",
        "\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(16, 24, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1))# 搭建卷积神经网络模型\n",
        "            # 14*14*24\n",
        "\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(24, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))# 搭建卷积神经网络模型\n",
        "            # 7*7*32\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(7*7*32, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(400, 120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, num_classes),\n",
        "        )\n",
        "\n",
        "    # 定义前向传播顺序\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.conv5(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "5aS7E2BBkpLo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 实例化一个模型\n",
        "model = ConvNet(num_classes)"
      ],
      "metadata": {
        "id": "VK07kYOHk2Br"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义损失函数和优化器\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "2qUHkTdhksvr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置cuda-gpu\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTuV8mucco2z",
        "outputId": "93c8e1aa-2311-48d2-cfb6-bab29167b0b3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Sat Jul  8 02:18:09 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8    10W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 开始训练\n",
        "model = model.cuda()\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        # 前向传播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 反向传播和优化\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbRva835lY5b",
        "outputId": "ebbb3c85-b64b-4823-e1c2-6cb3d233053f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [100/10000], Loss: 2.1089\n",
            "Epoch [1/20], Step [200/10000], Loss: 1.7900\n",
            "Epoch [1/20], Step [300/10000], Loss: 2.2647\n",
            "Epoch [1/20], Step [400/10000], Loss: 2.1288\n",
            "Epoch [1/20], Step [500/10000], Loss: 2.2716\n",
            "Epoch [1/20], Step [600/10000], Loss: 1.2608\n",
            "Epoch [1/20], Step [700/10000], Loss: 1.3803\n",
            "Epoch [1/20], Step [800/10000], Loss: 1.7021\n",
            "Epoch [1/20], Step [900/10000], Loss: 2.1567\n",
            "Epoch [1/20], Step [1000/10000], Loss: 1.5254\n",
            "Epoch [1/20], Step [1100/10000], Loss: 2.0819\n",
            "Epoch [1/20], Step [1200/10000], Loss: 1.3448\n",
            "Epoch [1/20], Step [1300/10000], Loss: 2.1260\n",
            "Epoch [1/20], Step [1400/10000], Loss: 2.5472\n",
            "Epoch [1/20], Step [1500/10000], Loss: 2.7331\n",
            "Epoch [1/20], Step [1600/10000], Loss: 2.2551\n",
            "Epoch [1/20], Step [1700/10000], Loss: 1.7929\n",
            "Epoch [1/20], Step [1800/10000], Loss: 1.3697\n",
            "Epoch [1/20], Step [1900/10000], Loss: 1.4661\n",
            "Epoch [1/20], Step [2000/10000], Loss: 1.4184\n",
            "Epoch [1/20], Step [2100/10000], Loss: 2.1375\n",
            "Epoch [1/20], Step [2200/10000], Loss: 1.5133\n",
            "Epoch [1/20], Step [2300/10000], Loss: 1.5087\n",
            "Epoch [1/20], Step [2400/10000], Loss: 2.3770\n",
            "Epoch [1/20], Step [2500/10000], Loss: 1.5118\n",
            "Epoch [1/20], Step [2600/10000], Loss: 1.4765\n",
            "Epoch [1/20], Step [2700/10000], Loss: 1.7846\n",
            "Epoch [1/20], Step [2800/10000], Loss: 1.2630\n",
            "Epoch [1/20], Step [2900/10000], Loss: 1.6412\n",
            "Epoch [1/20], Step [3000/10000], Loss: 1.1786\n",
            "Epoch [1/20], Step [3100/10000], Loss: 1.3739\n",
            "Epoch [1/20], Step [3200/10000], Loss: 1.5657\n",
            "Epoch [1/20], Step [3300/10000], Loss: 1.4685\n",
            "Epoch [1/20], Step [3400/10000], Loss: 1.0001\n",
            "Epoch [1/20], Step [3500/10000], Loss: 1.1499\n",
            "Epoch [1/20], Step [3600/10000], Loss: 1.9647\n",
            "Epoch [1/20], Step [3700/10000], Loss: 1.4847\n",
            "Epoch [1/20], Step [3800/10000], Loss: 2.2884\n",
            "Epoch [1/20], Step [3900/10000], Loss: 1.9327\n",
            "Epoch [1/20], Step [4000/10000], Loss: 1.5038\n",
            "Epoch [1/20], Step [4100/10000], Loss: 1.8796\n",
            "Epoch [1/20], Step [4200/10000], Loss: 1.4271\n",
            "Epoch [1/20], Step [4300/10000], Loss: 2.1498\n",
            "Epoch [1/20], Step [4400/10000], Loss: 1.3067\n",
            "Epoch [1/20], Step [4500/10000], Loss: 1.9654\n",
            "Epoch [1/20], Step [4600/10000], Loss: 1.9696\n",
            "Epoch [1/20], Step [4700/10000], Loss: 2.0093\n",
            "Epoch [1/20], Step [4800/10000], Loss: 1.4011\n",
            "Epoch [1/20], Step [4900/10000], Loss: 1.8540\n",
            "Epoch [1/20], Step [5000/10000], Loss: 1.4606\n",
            "Epoch [1/20], Step [5100/10000], Loss: 1.0383\n",
            "Epoch [1/20], Step [5200/10000], Loss: 1.7303\n",
            "Epoch [1/20], Step [5300/10000], Loss: 2.2687\n",
            "Epoch [1/20], Step [5400/10000], Loss: 1.5316\n",
            "Epoch [1/20], Step [5500/10000], Loss: 1.7351\n",
            "Epoch [1/20], Step [5600/10000], Loss: 1.8698\n",
            "Epoch [1/20], Step [5700/10000], Loss: 1.2240\n",
            "Epoch [1/20], Step [5800/10000], Loss: 1.5463\n",
            "Epoch [1/20], Step [5900/10000], Loss: 1.2300\n",
            "Epoch [1/20], Step [6000/10000], Loss: 2.2352\n",
            "Epoch [1/20], Step [6100/10000], Loss: 2.2963\n",
            "Epoch [1/20], Step [6200/10000], Loss: 1.5362\n",
            "Epoch [1/20], Step [6300/10000], Loss: 1.6990\n",
            "Epoch [1/20], Step [6400/10000], Loss: 1.1333\n",
            "Epoch [1/20], Step [6500/10000], Loss: 1.7627\n",
            "Epoch [1/20], Step [6600/10000], Loss: 1.7310\n",
            "Epoch [1/20], Step [6700/10000], Loss: 1.6245\n",
            "Epoch [1/20], Step [6800/10000], Loss: 1.0289\n",
            "Epoch [1/20], Step [6900/10000], Loss: 1.3509\n",
            "Epoch [1/20], Step [7000/10000], Loss: 2.1729\n",
            "Epoch [1/20], Step [7100/10000], Loss: 1.5371\n",
            "Epoch [1/20], Step [7200/10000], Loss: 0.6591\n",
            "Epoch [1/20], Step [7300/10000], Loss: 1.2440\n",
            "Epoch [1/20], Step [7400/10000], Loss: 1.2945\n",
            "Epoch [1/20], Step [7500/10000], Loss: 0.9789\n",
            "Epoch [1/20], Step [7600/10000], Loss: 1.6999\n",
            "Epoch [1/20], Step [7700/10000], Loss: 1.7250\n",
            "Epoch [1/20], Step [7800/10000], Loss: 2.1930\n",
            "Epoch [1/20], Step [7900/10000], Loss: 2.0696\n",
            "Epoch [1/20], Step [8000/10000], Loss: 2.4019\n",
            "Epoch [1/20], Step [8100/10000], Loss: 1.9949\n",
            "Epoch [1/20], Step [8200/10000], Loss: 1.1038\n",
            "Epoch [1/20], Step [8300/10000], Loss: 1.6236\n",
            "Epoch [1/20], Step [8400/10000], Loss: 1.0554\n",
            "Epoch [1/20], Step [8500/10000], Loss: 1.5421\n",
            "Epoch [1/20], Step [8600/10000], Loss: 1.9033\n",
            "Epoch [1/20], Step [8700/10000], Loss: 1.4811\n",
            "Epoch [1/20], Step [8800/10000], Loss: 1.4852\n",
            "Epoch [1/20], Step [8900/10000], Loss: 1.6686\n",
            "Epoch [1/20], Step [9000/10000], Loss: 0.6240\n",
            "Epoch [1/20], Step [9100/10000], Loss: 2.0432\n",
            "Epoch [1/20], Step [9200/10000], Loss: 0.9765\n",
            "Epoch [1/20], Step [9300/10000], Loss: 1.7435\n",
            "Epoch [1/20], Step [9400/10000], Loss: 1.8442\n",
            "Epoch [1/20], Step [9500/10000], Loss: 1.4140\n",
            "Epoch [1/20], Step [9600/10000], Loss: 3.2298\n",
            "Epoch [1/20], Step [9700/10000], Loss: 1.2478\n",
            "Epoch [1/20], Step [9800/10000], Loss: 1.9949\n",
            "Epoch [1/20], Step [9900/10000], Loss: 2.3283\n",
            "Epoch [1/20], Step [10000/10000], Loss: 1.2824\n",
            "Epoch [2/20], Step [100/10000], Loss: 0.7786\n",
            "Epoch [2/20], Step [200/10000], Loss: 1.2065\n",
            "Epoch [2/20], Step [300/10000], Loss: 0.8056\n",
            "Epoch [2/20], Step [400/10000], Loss: 0.9930\n",
            "Epoch [2/20], Step [500/10000], Loss: 1.4240\n",
            "Epoch [2/20], Step [600/10000], Loss: 1.8018\n",
            "Epoch [2/20], Step [700/10000], Loss: 1.7981\n",
            "Epoch [2/20], Step [800/10000], Loss: 1.5177\n",
            "Epoch [2/20], Step [900/10000], Loss: 1.4437\n",
            "Epoch [2/20], Step [1000/10000], Loss: 1.5795\n",
            "Epoch [2/20], Step [1100/10000], Loss: 0.9546\n",
            "Epoch [2/20], Step [1200/10000], Loss: 1.0437\n",
            "Epoch [2/20], Step [1300/10000], Loss: 2.2571\n",
            "Epoch [2/20], Step [1400/10000], Loss: 1.9961\n",
            "Epoch [2/20], Step [1500/10000], Loss: 1.6249\n",
            "Epoch [2/20], Step [1600/10000], Loss: 1.4596\n",
            "Epoch [2/20], Step [1700/10000], Loss: 1.6635\n",
            "Epoch [2/20], Step [1800/10000], Loss: 1.0859\n",
            "Epoch [2/20], Step [1900/10000], Loss: 1.2810\n",
            "Epoch [2/20], Step [2000/10000], Loss: 1.6602\n",
            "Epoch [2/20], Step [2100/10000], Loss: 1.8095\n",
            "Epoch [2/20], Step [2200/10000], Loss: 1.4906\n",
            "Epoch [2/20], Step [2300/10000], Loss: 0.7350\n",
            "Epoch [2/20], Step [2400/10000], Loss: 0.9396\n",
            "Epoch [2/20], Step [2500/10000], Loss: 1.0983\n",
            "Epoch [2/20], Step [2600/10000], Loss: 1.2761\n",
            "Epoch [2/20], Step [2700/10000], Loss: 0.9564\n",
            "Epoch [2/20], Step [2800/10000], Loss: 1.3840\n",
            "Epoch [2/20], Step [2900/10000], Loss: 1.6115\n",
            "Epoch [2/20], Step [3000/10000], Loss: 1.4222\n",
            "Epoch [2/20], Step [3100/10000], Loss: 0.2226\n",
            "Epoch [2/20], Step [3200/10000], Loss: 1.5247\n",
            "Epoch [2/20], Step [3300/10000], Loss: 1.6744\n",
            "Epoch [2/20], Step [3400/10000], Loss: 1.0638\n",
            "Epoch [2/20], Step [3500/10000], Loss: 2.0208\n",
            "Epoch [2/20], Step [3600/10000], Loss: 1.1846\n",
            "Epoch [2/20], Step [3700/10000], Loss: 1.6136\n",
            "Epoch [2/20], Step [3800/10000], Loss: 1.5990\n",
            "Epoch [2/20], Step [3900/10000], Loss: 0.8409\n",
            "Epoch [2/20], Step [4000/10000], Loss: 1.9691\n",
            "Epoch [2/20], Step [4100/10000], Loss: 1.6660\n",
            "Epoch [2/20], Step [4200/10000], Loss: 1.0044\n",
            "Epoch [2/20], Step [4300/10000], Loss: 0.4996\n",
            "Epoch [2/20], Step [4400/10000], Loss: 1.1216\n",
            "Epoch [2/20], Step [4500/10000], Loss: 2.7185\n",
            "Epoch [2/20], Step [4600/10000], Loss: 1.7448\n",
            "Epoch [2/20], Step [4700/10000], Loss: 1.4582\n",
            "Epoch [2/20], Step [4800/10000], Loss: 1.9691\n",
            "Epoch [2/20], Step [4900/10000], Loss: 0.8331\n",
            "Epoch [2/20], Step [5000/10000], Loss: 1.5929\n",
            "Epoch [2/20], Step [5100/10000], Loss: 1.7511\n",
            "Epoch [2/20], Step [5200/10000], Loss: 0.3367\n",
            "Epoch [2/20], Step [5300/10000], Loss: 0.6814\n",
            "Epoch [2/20], Step [5400/10000], Loss: 0.7468\n",
            "Epoch [2/20], Step [5500/10000], Loss: 1.3568\n",
            "Epoch [2/20], Step [5600/10000], Loss: 1.7062\n",
            "Epoch [2/20], Step [5700/10000], Loss: 1.0667\n",
            "Epoch [2/20], Step [5800/10000], Loss: 1.4556\n",
            "Epoch [2/20], Step [5900/10000], Loss: 1.5501\n",
            "Epoch [2/20], Step [6000/10000], Loss: 1.6725\n",
            "Epoch [2/20], Step [6100/10000], Loss: 1.7790\n",
            "Epoch [2/20], Step [6200/10000], Loss: 1.0349\n",
            "Epoch [2/20], Step [6300/10000], Loss: 1.4954\n",
            "Epoch [2/20], Step [6400/10000], Loss: 1.1714\n",
            "Epoch [2/20], Step [6500/10000], Loss: 0.5740\n",
            "Epoch [2/20], Step [6600/10000], Loss: 1.4160\n",
            "Epoch [2/20], Step [6700/10000], Loss: 1.1743\n",
            "Epoch [2/20], Step [6800/10000], Loss: 1.6636\n",
            "Epoch [2/20], Step [6900/10000], Loss: 1.1410\n",
            "Epoch [2/20], Step [7000/10000], Loss: 1.5843\n",
            "Epoch [2/20], Step [7100/10000], Loss: 0.8279\n",
            "Epoch [2/20], Step [7200/10000], Loss: 1.0735\n",
            "Epoch [2/20], Step [7300/10000], Loss: 1.6960\n",
            "Epoch [2/20], Step [7400/10000], Loss: 0.7502\n",
            "Epoch [2/20], Step [7500/10000], Loss: 1.3804\n",
            "Epoch [2/20], Step [7600/10000], Loss: 0.8644\n",
            "Epoch [2/20], Step [7700/10000], Loss: 1.4450\n",
            "Epoch [2/20], Step [7800/10000], Loss: 1.4476\n",
            "Epoch [2/20], Step [7900/10000], Loss: 0.5682\n",
            "Epoch [2/20], Step [8000/10000], Loss: 0.9990\n",
            "Epoch [2/20], Step [8100/10000], Loss: 1.1441\n",
            "Epoch [2/20], Step [8200/10000], Loss: 0.5143\n",
            "Epoch [2/20], Step [8300/10000], Loss: 0.6846\n",
            "Epoch [2/20], Step [8400/10000], Loss: 2.1884\n",
            "Epoch [2/20], Step [8500/10000], Loss: 0.3549\n",
            "Epoch [2/20], Step [8600/10000], Loss: 2.0665\n",
            "Epoch [2/20], Step [8700/10000], Loss: 0.8671\n",
            "Epoch [2/20], Step [8800/10000], Loss: 1.8211\n",
            "Epoch [2/20], Step [8900/10000], Loss: 0.8701\n",
            "Epoch [2/20], Step [9000/10000], Loss: 1.5094\n",
            "Epoch [2/20], Step [9100/10000], Loss: 1.9460\n",
            "Epoch [2/20], Step [9200/10000], Loss: 0.8413\n",
            "Epoch [2/20], Step [9300/10000], Loss: 1.4884\n",
            "Epoch [2/20], Step [9400/10000], Loss: 0.5201\n",
            "Epoch [2/20], Step [9500/10000], Loss: 1.2304\n",
            "Epoch [2/20], Step [9600/10000], Loss: 1.8408\n",
            "Epoch [2/20], Step [9700/10000], Loss: 0.2648\n",
            "Epoch [2/20], Step [9800/10000], Loss: 0.8849\n",
            "Epoch [2/20], Step [9900/10000], Loss: 1.3246\n",
            "Epoch [2/20], Step [10000/10000], Loss: 2.1631\n",
            "Epoch [3/20], Step [100/10000], Loss: 1.6453\n",
            "Epoch [3/20], Step [200/10000], Loss: 0.4443\n",
            "Epoch [3/20], Step [300/10000], Loss: 1.8750\n",
            "Epoch [3/20], Step [400/10000], Loss: 1.4151\n",
            "Epoch [3/20], Step [500/10000], Loss: 0.4586\n",
            "Epoch [3/20], Step [600/10000], Loss: 0.7755\n",
            "Epoch [3/20], Step [700/10000], Loss: 1.3984\n",
            "Epoch [3/20], Step [800/10000], Loss: 1.3082\n",
            "Epoch [3/20], Step [900/10000], Loss: 0.1630\n",
            "Epoch [3/20], Step [1000/10000], Loss: 2.3004\n",
            "Epoch [3/20], Step [1100/10000], Loss: 2.1175\n",
            "Epoch [3/20], Step [1200/10000], Loss: 1.8427\n",
            "Epoch [3/20], Step [1300/10000], Loss: 2.0931\n",
            "Epoch [3/20], Step [1400/10000], Loss: 1.3022\n",
            "Epoch [3/20], Step [1500/10000], Loss: 1.1956\n",
            "Epoch [3/20], Step [1600/10000], Loss: 3.1256\n",
            "Epoch [3/20], Step [1700/10000], Loss: 0.9469\n",
            "Epoch [3/20], Step [1800/10000], Loss: 0.7850\n",
            "Epoch [3/20], Step [1900/10000], Loss: 0.4539\n",
            "Epoch [3/20], Step [2000/10000], Loss: 2.3736\n",
            "Epoch [3/20], Step [2100/10000], Loss: 1.4768\n",
            "Epoch [3/20], Step [2200/10000], Loss: 2.3547\n",
            "Epoch [3/20], Step [2300/10000], Loss: 1.0295\n",
            "Epoch [3/20], Step [2400/10000], Loss: 1.4997\n",
            "Epoch [3/20], Step [2500/10000], Loss: 0.9880\n",
            "Epoch [3/20], Step [2600/10000], Loss: 1.0172\n",
            "Epoch [3/20], Step [2700/10000], Loss: 1.2353\n",
            "Epoch [3/20], Step [2800/10000], Loss: 1.2881\n",
            "Epoch [3/20], Step [2900/10000], Loss: 1.8898\n",
            "Epoch [3/20], Step [3000/10000], Loss: 0.8602\n",
            "Epoch [3/20], Step [3100/10000], Loss: 1.4329\n",
            "Epoch [3/20], Step [3200/10000], Loss: 1.5281\n",
            "Epoch [3/20], Step [3300/10000], Loss: 1.9035\n",
            "Epoch [3/20], Step [3400/10000], Loss: 1.6870\n",
            "Epoch [3/20], Step [3500/10000], Loss: 0.7735\n",
            "Epoch [3/20], Step [3600/10000], Loss: 0.4284\n",
            "Epoch [3/20], Step [3700/10000], Loss: 1.0355\n",
            "Epoch [3/20], Step [3800/10000], Loss: 1.2843\n",
            "Epoch [3/20], Step [3900/10000], Loss: 1.0048\n",
            "Epoch [3/20], Step [4000/10000], Loss: 1.0475\n",
            "Epoch [3/20], Step [4100/10000], Loss: 0.6831\n",
            "Epoch [3/20], Step [4200/10000], Loss: 0.8959\n",
            "Epoch [3/20], Step [4300/10000], Loss: 0.6101\n",
            "Epoch [3/20], Step [4400/10000], Loss: 0.5304\n",
            "Epoch [3/20], Step [4500/10000], Loss: 0.4235\n",
            "Epoch [3/20], Step [4600/10000], Loss: 1.4155\n",
            "Epoch [3/20], Step [4700/10000], Loss: 1.3776\n",
            "Epoch [3/20], Step [4800/10000], Loss: 2.2262\n",
            "Epoch [3/20], Step [4900/10000], Loss: 0.7184\n",
            "Epoch [3/20], Step [5000/10000], Loss: 0.7634\n",
            "Epoch [3/20], Step [5100/10000], Loss: 0.5933\n",
            "Epoch [3/20], Step [5200/10000], Loss: 0.9211\n",
            "Epoch [3/20], Step [5300/10000], Loss: 2.0342\n",
            "Epoch [3/20], Step [5400/10000], Loss: 0.8446\n",
            "Epoch [3/20], Step [5500/10000], Loss: 1.7208\n",
            "Epoch [3/20], Step [5600/10000], Loss: 0.7713\n",
            "Epoch [3/20], Step [5700/10000], Loss: 1.0946\n",
            "Epoch [3/20], Step [5800/10000], Loss: 0.9016\n",
            "Epoch [3/20], Step [5900/10000], Loss: 1.8028\n",
            "Epoch [3/20], Step [6000/10000], Loss: 0.6192\n",
            "Epoch [3/20], Step [6100/10000], Loss: 1.1583\n",
            "Epoch [3/20], Step [6200/10000], Loss: 1.8668\n",
            "Epoch [3/20], Step [6300/10000], Loss: 0.8342\n",
            "Epoch [3/20], Step [6400/10000], Loss: 0.7514\n",
            "Epoch [3/20], Step [6500/10000], Loss: 0.9700\n",
            "Epoch [3/20], Step [6600/10000], Loss: 1.1005\n",
            "Epoch [3/20], Step [6700/10000], Loss: 0.8815\n",
            "Epoch [3/20], Step [6800/10000], Loss: 0.6562\n",
            "Epoch [3/20], Step [6900/10000], Loss: 1.3499\n",
            "Epoch [3/20], Step [7000/10000], Loss: 1.3281\n",
            "Epoch [3/20], Step [7100/10000], Loss: 1.4089\n",
            "Epoch [3/20], Step [7200/10000], Loss: 1.2593\n",
            "Epoch [3/20], Step [7300/10000], Loss: 1.7221\n",
            "Epoch [3/20], Step [7400/10000], Loss: 2.1221\n",
            "Epoch [3/20], Step [7500/10000], Loss: 0.8177\n",
            "Epoch [3/20], Step [7600/10000], Loss: 0.6545\n",
            "Epoch [3/20], Step [7700/10000], Loss: 1.7206\n",
            "Epoch [3/20], Step [7800/10000], Loss: 0.9143\n",
            "Epoch [3/20], Step [7900/10000], Loss: 2.8130\n",
            "Epoch [3/20], Step [8000/10000], Loss: 0.8545\n",
            "Epoch [3/20], Step [8100/10000], Loss: 1.1644\n",
            "Epoch [3/20], Step [8200/10000], Loss: 0.2953\n",
            "Epoch [3/20], Step [8300/10000], Loss: 1.1179\n",
            "Epoch [3/20], Step [8400/10000], Loss: 2.0080\n",
            "Epoch [3/20], Step [8500/10000], Loss: 0.9276\n",
            "Epoch [3/20], Step [8600/10000], Loss: 0.9021\n",
            "Epoch [3/20], Step [8700/10000], Loss: 2.0803\n",
            "Epoch [3/20], Step [8800/10000], Loss: 1.9496\n",
            "Epoch [3/20], Step [8900/10000], Loss: 1.3234\n",
            "Epoch [3/20], Step [9000/10000], Loss: 1.1983\n",
            "Epoch [3/20], Step [9100/10000], Loss: 1.0129\n",
            "Epoch [3/20], Step [9200/10000], Loss: 0.5532\n",
            "Epoch [3/20], Step [9300/10000], Loss: 1.5321\n",
            "Epoch [3/20], Step [9400/10000], Loss: 1.6237\n",
            "Epoch [3/20], Step [9500/10000], Loss: 3.3330\n",
            "Epoch [3/20], Step [9600/10000], Loss: 2.4401\n",
            "Epoch [3/20], Step [9700/10000], Loss: 2.0341\n",
            "Epoch [3/20], Step [9800/10000], Loss: 1.2383\n",
            "Epoch [3/20], Step [9900/10000], Loss: 0.3905\n",
            "Epoch [3/20], Step [10000/10000], Loss: 0.4550\n",
            "Epoch [4/20], Step [100/10000], Loss: 0.7285\n",
            "Epoch [4/20], Step [200/10000], Loss: 0.5615\n",
            "Epoch [4/20], Step [300/10000], Loss: 0.6577\n",
            "Epoch [4/20], Step [400/10000], Loss: 0.9058\n",
            "Epoch [4/20], Step [500/10000], Loss: 1.0883\n",
            "Epoch [4/20], Step [600/10000], Loss: 1.1176\n",
            "Epoch [4/20], Step [700/10000], Loss: 1.6678\n",
            "Epoch [4/20], Step [800/10000], Loss: 0.4380\n",
            "Epoch [4/20], Step [900/10000], Loss: 1.8069\n",
            "Epoch [4/20], Step [1000/10000], Loss: 0.4495\n",
            "Epoch [4/20], Step [1100/10000], Loss: 0.6216\n",
            "Epoch [4/20], Step [1200/10000], Loss: 1.0827\n",
            "Epoch [4/20], Step [1300/10000], Loss: 1.7654\n",
            "Epoch [4/20], Step [1400/10000], Loss: 0.8843\n",
            "Epoch [4/20], Step [1500/10000], Loss: 0.4934\n",
            "Epoch [4/20], Step [1600/10000], Loss: 0.9322\n",
            "Epoch [4/20], Step [1700/10000], Loss: 0.6117\n",
            "Epoch [4/20], Step [1800/10000], Loss: 1.1696\n",
            "Epoch [4/20], Step [1900/10000], Loss: 0.8236\n",
            "Epoch [4/20], Step [2000/10000], Loss: 0.6899\n",
            "Epoch [4/20], Step [2100/10000], Loss: 0.7092\n",
            "Epoch [4/20], Step [2200/10000], Loss: 0.8961\n",
            "Epoch [4/20], Step [2300/10000], Loss: 1.3937\n",
            "Epoch [4/20], Step [2400/10000], Loss: 0.6066\n",
            "Epoch [4/20], Step [2500/10000], Loss: 1.6665\n",
            "Epoch [4/20], Step [2600/10000], Loss: 0.3673\n",
            "Epoch [4/20], Step [2700/10000], Loss: 1.1099\n",
            "Epoch [4/20], Step [2800/10000], Loss: 1.1944\n",
            "Epoch [4/20], Step [2900/10000], Loss: 1.7374\n",
            "Epoch [4/20], Step [3000/10000], Loss: 1.0693\n",
            "Epoch [4/20], Step [3100/10000], Loss: 2.4810\n",
            "Epoch [4/20], Step [3200/10000], Loss: 1.5715\n",
            "Epoch [4/20], Step [3300/10000], Loss: 1.7257\n",
            "Epoch [4/20], Step [3400/10000], Loss: 1.1474\n",
            "Epoch [4/20], Step [3500/10000], Loss: 0.8114\n",
            "Epoch [4/20], Step [3600/10000], Loss: 1.1334\n",
            "Epoch [4/20], Step [3700/10000], Loss: 2.1129\n",
            "Epoch [4/20], Step [3800/10000], Loss: 1.0368\n",
            "Epoch [4/20], Step [3900/10000], Loss: 0.7352\n",
            "Epoch [4/20], Step [4000/10000], Loss: 1.2946\n",
            "Epoch [4/20], Step [4100/10000], Loss: 0.8470\n",
            "Epoch [4/20], Step [4200/10000], Loss: 1.1550\n",
            "Epoch [4/20], Step [4300/10000], Loss: 0.3756\n",
            "Epoch [4/20], Step [4400/10000], Loss: 0.6910\n",
            "Epoch [4/20], Step [4500/10000], Loss: 1.6449\n",
            "Epoch [4/20], Step [4600/10000], Loss: 1.4315\n",
            "Epoch [4/20], Step [4700/10000], Loss: 1.0824\n",
            "Epoch [4/20], Step [4800/10000], Loss: 0.1695\n",
            "Epoch [4/20], Step [4900/10000], Loss: 2.6007\n",
            "Epoch [4/20], Step [5000/10000], Loss: 0.8099\n",
            "Epoch [4/20], Step [5100/10000], Loss: 1.3491\n",
            "Epoch [4/20], Step [5200/10000], Loss: 1.0694\n",
            "Epoch [4/20], Step [5300/10000], Loss: 0.9737\n",
            "Epoch [4/20], Step [5400/10000], Loss: 0.9548\n",
            "Epoch [4/20], Step [5500/10000], Loss: 1.3877\n",
            "Epoch [4/20], Step [5600/10000], Loss: 1.1082\n",
            "Epoch [4/20], Step [5700/10000], Loss: 0.7108\n",
            "Epoch [4/20], Step [5800/10000], Loss: 0.9977\n",
            "Epoch [4/20], Step [5900/10000], Loss: 0.7611\n",
            "Epoch [4/20], Step [6000/10000], Loss: 0.6272\n",
            "Epoch [4/20], Step [6100/10000], Loss: 0.9468\n",
            "Epoch [4/20], Step [6200/10000], Loss: 1.1016\n",
            "Epoch [4/20], Step [6300/10000], Loss: 0.7184\n",
            "Epoch [4/20], Step [6400/10000], Loss: 1.4499\n",
            "Epoch [4/20], Step [6500/10000], Loss: 0.8221\n",
            "Epoch [4/20], Step [6600/10000], Loss: 0.6844\n",
            "Epoch [4/20], Step [6700/10000], Loss: 0.6620\n",
            "Epoch [4/20], Step [6800/10000], Loss: 1.3910\n",
            "Epoch [4/20], Step [6900/10000], Loss: 2.8485\n",
            "Epoch [4/20], Step [7000/10000], Loss: 1.2252\n",
            "Epoch [4/20], Step [7100/10000], Loss: 0.8637\n",
            "Epoch [4/20], Step [7200/10000], Loss: 1.3214\n",
            "Epoch [4/20], Step [7300/10000], Loss: 1.8153\n",
            "Epoch [4/20], Step [7400/10000], Loss: 0.8293\n",
            "Epoch [4/20], Step [7500/10000], Loss: 1.0041\n",
            "Epoch [4/20], Step [7600/10000], Loss: 1.6948\n",
            "Epoch [4/20], Step [7700/10000], Loss: 1.3535\n",
            "Epoch [4/20], Step [7800/10000], Loss: 1.2191\n",
            "Epoch [4/20], Step [7900/10000], Loss: 0.5276\n",
            "Epoch [4/20], Step [8000/10000], Loss: 0.8687\n",
            "Epoch [4/20], Step [8100/10000], Loss: 1.7216\n",
            "Epoch [4/20], Step [8200/10000], Loss: 1.7018\n",
            "Epoch [4/20], Step [8300/10000], Loss: 2.3292\n",
            "Epoch [4/20], Step [8400/10000], Loss: 2.0672\n",
            "Epoch [4/20], Step [8500/10000], Loss: 1.6941\n",
            "Epoch [4/20], Step [8600/10000], Loss: 1.0309\n",
            "Epoch [4/20], Step [8700/10000], Loss: 0.8713\n",
            "Epoch [4/20], Step [8800/10000], Loss: 1.4132\n",
            "Epoch [4/20], Step [8900/10000], Loss: 0.6580\n",
            "Epoch [4/20], Step [9000/10000], Loss: 0.6725\n",
            "Epoch [4/20], Step [9100/10000], Loss: 0.6361\n",
            "Epoch [4/20], Step [9200/10000], Loss: 1.1472\n",
            "Epoch [4/20], Step [9300/10000], Loss: 1.1281\n",
            "Epoch [4/20], Step [9400/10000], Loss: 0.5510\n",
            "Epoch [4/20], Step [9500/10000], Loss: 0.7772\n",
            "Epoch [4/20], Step [9600/10000], Loss: 0.9051\n",
            "Epoch [4/20], Step [9700/10000], Loss: 1.1831\n",
            "Epoch [4/20], Step [9800/10000], Loss: 1.0036\n",
            "Epoch [4/20], Step [9900/10000], Loss: 1.4980\n",
            "Epoch [4/20], Step [10000/10000], Loss: 1.4309\n",
            "Epoch [5/20], Step [100/10000], Loss: 0.9419\n",
            "Epoch [5/20], Step [200/10000], Loss: 1.1222\n",
            "Epoch [5/20], Step [300/10000], Loss: 0.8462\n",
            "Epoch [5/20], Step [400/10000], Loss: 1.2222\n",
            "Epoch [5/20], Step [500/10000], Loss: 0.9845\n",
            "Epoch [5/20], Step [600/10000], Loss: 1.2332\n",
            "Epoch [5/20], Step [700/10000], Loss: 1.0265\n",
            "Epoch [5/20], Step [800/10000], Loss: 2.0046\n",
            "Epoch [5/20], Step [900/10000], Loss: 1.0201\n",
            "Epoch [5/20], Step [1000/10000], Loss: 1.1722\n",
            "Epoch [5/20], Step [1100/10000], Loss: 0.5190\n",
            "Epoch [5/20], Step [1200/10000], Loss: 0.7958\n",
            "Epoch [5/20], Step [1300/10000], Loss: 0.8675\n",
            "Epoch [5/20], Step [1400/10000], Loss: 2.2851\n",
            "Epoch [5/20], Step [1500/10000], Loss: 0.6816\n",
            "Epoch [5/20], Step [1600/10000], Loss: 2.1700\n",
            "Epoch [5/20], Step [1700/10000], Loss: 0.7053\n",
            "Epoch [5/20], Step [1800/10000], Loss: 0.2380\n",
            "Epoch [5/20], Step [1900/10000], Loss: 1.1386\n",
            "Epoch [5/20], Step [2000/10000], Loss: 1.0894\n",
            "Epoch [5/20], Step [2100/10000], Loss: 0.9431\n",
            "Epoch [5/20], Step [2200/10000], Loss: 1.6827\n",
            "Epoch [5/20], Step [2300/10000], Loss: 0.4289\n",
            "Epoch [5/20], Step [2400/10000], Loss: 1.2700\n",
            "Epoch [5/20], Step [2500/10000], Loss: 0.7836\n",
            "Epoch [5/20], Step [2600/10000], Loss: 0.6088\n",
            "Epoch [5/20], Step [2700/10000], Loss: 0.8621\n",
            "Epoch [5/20], Step [2800/10000], Loss: 0.9629\n",
            "Epoch [5/20], Step [2900/10000], Loss: 0.9670\n",
            "Epoch [5/20], Step [3000/10000], Loss: 0.6442\n",
            "Epoch [5/20], Step [3100/10000], Loss: 2.0189\n",
            "Epoch [5/20], Step [3200/10000], Loss: 0.9851\n",
            "Epoch [5/20], Step [3300/10000], Loss: 1.3227\n",
            "Epoch [5/20], Step [3400/10000], Loss: 0.8109\n",
            "Epoch [5/20], Step [3500/10000], Loss: 0.9559\n",
            "Epoch [5/20], Step [3600/10000], Loss: 0.8036\n",
            "Epoch [5/20], Step [3700/10000], Loss: 2.3109\n",
            "Epoch [5/20], Step [3800/10000], Loss: 0.6133\n",
            "Epoch [5/20], Step [3900/10000], Loss: 0.3680\n",
            "Epoch [5/20], Step [4000/10000], Loss: 1.6033\n",
            "Epoch [5/20], Step [4100/10000], Loss: 1.8942\n",
            "Epoch [5/20], Step [4200/10000], Loss: 1.0959\n",
            "Epoch [5/20], Step [4300/10000], Loss: 0.8827\n",
            "Epoch [5/20], Step [4400/10000], Loss: 0.6833\n",
            "Epoch [5/20], Step [4500/10000], Loss: 0.8418\n",
            "Epoch [5/20], Step [4600/10000], Loss: 0.5010\n",
            "Epoch [5/20], Step [4700/10000], Loss: 1.1537\n",
            "Epoch [5/20], Step [4800/10000], Loss: 1.5176\n",
            "Epoch [5/20], Step [4900/10000], Loss: 1.5611\n",
            "Epoch [5/20], Step [5000/10000], Loss: 0.7204\n",
            "Epoch [5/20], Step [5100/10000], Loss: 1.2299\n",
            "Epoch [5/20], Step [5200/10000], Loss: 1.5612\n",
            "Epoch [5/20], Step [5300/10000], Loss: 0.8774\n",
            "Epoch [5/20], Step [5400/10000], Loss: 1.1283\n",
            "Epoch [5/20], Step [5500/10000], Loss: 0.4291\n",
            "Epoch [5/20], Step [5600/10000], Loss: 2.0751\n",
            "Epoch [5/20], Step [5700/10000], Loss: 1.0985\n",
            "Epoch [5/20], Step [5800/10000], Loss: 0.4629\n",
            "Epoch [5/20], Step [5900/10000], Loss: 0.4842\n",
            "Epoch [5/20], Step [6000/10000], Loss: 0.9570\n",
            "Epoch [5/20], Step [6100/10000], Loss: 0.4741\n",
            "Epoch [5/20], Step [6200/10000], Loss: 1.2251\n",
            "Epoch [5/20], Step [6300/10000], Loss: 0.9329\n",
            "Epoch [5/20], Step [6400/10000], Loss: 0.9389\n",
            "Epoch [5/20], Step [6500/10000], Loss: 1.1793\n",
            "Epoch [5/20], Step [6600/10000], Loss: 0.9555\n",
            "Epoch [5/20], Step [6700/10000], Loss: 1.1649\n",
            "Epoch [5/20], Step [6800/10000], Loss: 0.9640\n",
            "Epoch [5/20], Step [6900/10000], Loss: 1.0280\n",
            "Epoch [5/20], Step [7000/10000], Loss: 0.6856\n",
            "Epoch [5/20], Step [7100/10000], Loss: 1.2346\n",
            "Epoch [5/20], Step [7200/10000], Loss: 1.4229\n",
            "Epoch [5/20], Step [7300/10000], Loss: 0.5721\n",
            "Epoch [5/20], Step [7400/10000], Loss: 2.6329\n",
            "Epoch [5/20], Step [7500/10000], Loss: 1.2034\n",
            "Epoch [5/20], Step [7600/10000], Loss: 1.5091\n",
            "Epoch [5/20], Step [7700/10000], Loss: 0.9449\n",
            "Epoch [5/20], Step [7800/10000], Loss: 0.3774\n",
            "Epoch [5/20], Step [7900/10000], Loss: 2.1448\n",
            "Epoch [5/20], Step [8000/10000], Loss: 1.8816\n",
            "Epoch [5/20], Step [8100/10000], Loss: 0.8512\n",
            "Epoch [5/20], Step [8200/10000], Loss: 1.2152\n",
            "Epoch [5/20], Step [8300/10000], Loss: 1.7795\n",
            "Epoch [5/20], Step [8400/10000], Loss: 1.8984\n",
            "Epoch [5/20], Step [8500/10000], Loss: 0.3795\n",
            "Epoch [5/20], Step [8600/10000], Loss: 0.7859\n",
            "Epoch [5/20], Step [8700/10000], Loss: 0.6583\n",
            "Epoch [5/20], Step [8800/10000], Loss: 1.1271\n",
            "Epoch [5/20], Step [8900/10000], Loss: 0.8762\n",
            "Epoch [5/20], Step [9000/10000], Loss: 1.5904\n",
            "Epoch [5/20], Step [9100/10000], Loss: 0.7770\n",
            "Epoch [5/20], Step [9200/10000], Loss: 2.2976\n",
            "Epoch [5/20], Step [9300/10000], Loss: 0.7323\n",
            "Epoch [5/20], Step [9400/10000], Loss: 0.9765\n",
            "Epoch [5/20], Step [9500/10000], Loss: 1.0979\n",
            "Epoch [5/20], Step [9600/10000], Loss: 1.0343\n",
            "Epoch [5/20], Step [9700/10000], Loss: 1.5877\n",
            "Epoch [5/20], Step [9800/10000], Loss: 0.6317\n",
            "Epoch [5/20], Step [9900/10000], Loss: 0.5702\n",
            "Epoch [5/20], Step [10000/10000], Loss: 0.6700\n",
            "Epoch [6/20], Step [100/10000], Loss: 0.7789\n",
            "Epoch [6/20], Step [200/10000], Loss: 1.3133\n",
            "Epoch [6/20], Step [300/10000], Loss: 0.6994\n",
            "Epoch [6/20], Step [400/10000], Loss: 0.7481\n",
            "Epoch [6/20], Step [500/10000], Loss: 1.3212\n",
            "Epoch [6/20], Step [600/10000], Loss: 0.7722\n",
            "Epoch [6/20], Step [700/10000], Loss: 1.0960\n",
            "Epoch [6/20], Step [800/10000], Loss: 0.7457\n",
            "Epoch [6/20], Step [900/10000], Loss: 2.3465\n",
            "Epoch [6/20], Step [1000/10000], Loss: 1.6540\n",
            "Epoch [6/20], Step [1100/10000], Loss: 0.4600\n",
            "Epoch [6/20], Step [1200/10000], Loss: 0.1750\n",
            "Epoch [6/20], Step [1300/10000], Loss: 1.0891\n",
            "Epoch [6/20], Step [1400/10000], Loss: 0.4839\n",
            "Epoch [6/20], Step [1500/10000], Loss: 0.7712\n",
            "Epoch [6/20], Step [1600/10000], Loss: 0.6816\n",
            "Epoch [6/20], Step [1700/10000], Loss: 1.3588\n",
            "Epoch [6/20], Step [1800/10000], Loss: 0.5488\n",
            "Epoch [6/20], Step [1900/10000], Loss: 0.5412\n",
            "Epoch [6/20], Step [2000/10000], Loss: 0.2706\n",
            "Epoch [6/20], Step [2100/10000], Loss: 0.7218\n",
            "Epoch [6/20], Step [2200/10000], Loss: 0.0914\n",
            "Epoch [6/20], Step [2300/10000], Loss: 1.1194\n",
            "Epoch [6/20], Step [2400/10000], Loss: 0.7973\n",
            "Epoch [6/20], Step [2500/10000], Loss: 0.7710\n",
            "Epoch [6/20], Step [2600/10000], Loss: 1.3504\n",
            "Epoch [6/20], Step [2700/10000], Loss: 1.7399\n",
            "Epoch [6/20], Step [2800/10000], Loss: 1.9719\n",
            "Epoch [6/20], Step [2900/10000], Loss: 0.4340\n",
            "Epoch [6/20], Step [3000/10000], Loss: 1.8764\n",
            "Epoch [6/20], Step [3100/10000], Loss: 1.5160\n",
            "Epoch [6/20], Step [3200/10000], Loss: 0.4025\n",
            "Epoch [6/20], Step [3300/10000], Loss: 0.4450\n",
            "Epoch [6/20], Step [3400/10000], Loss: 0.7282\n",
            "Epoch [6/20], Step [3500/10000], Loss: 0.7810\n",
            "Epoch [6/20], Step [3600/10000], Loss: 1.0350\n",
            "Epoch [6/20], Step [3700/10000], Loss: 1.1439\n",
            "Epoch [6/20], Step [3800/10000], Loss: 0.9599\n",
            "Epoch [6/20], Step [3900/10000], Loss: 0.9879\n",
            "Epoch [6/20], Step [4000/10000], Loss: 0.8257\n",
            "Epoch [6/20], Step [4100/10000], Loss: 2.2407\n",
            "Epoch [6/20], Step [4200/10000], Loss: 0.6324\n",
            "Epoch [6/20], Step [4300/10000], Loss: 0.7712\n",
            "Epoch [6/20], Step [4400/10000], Loss: 1.0083\n",
            "Epoch [6/20], Step [4500/10000], Loss: 0.5002\n",
            "Epoch [6/20], Step [4600/10000], Loss: 0.8454\n",
            "Epoch [6/20], Step [4700/10000], Loss: 1.1773\n",
            "Epoch [6/20], Step [4800/10000], Loss: 1.2062\n",
            "Epoch [6/20], Step [4900/10000], Loss: 0.8201\n",
            "Epoch [6/20], Step [5000/10000], Loss: 1.0710\n",
            "Epoch [6/20], Step [5100/10000], Loss: 0.5114\n",
            "Epoch [6/20], Step [5200/10000], Loss: 1.1292\n",
            "Epoch [6/20], Step [5300/10000], Loss: 1.7171\n",
            "Epoch [6/20], Step [5400/10000], Loss: 1.5210\n",
            "Epoch [6/20], Step [5500/10000], Loss: 1.0421\n",
            "Epoch [6/20], Step [5600/10000], Loss: 3.2184\n",
            "Epoch [6/20], Step [5700/10000], Loss: 0.6705\n",
            "Epoch [6/20], Step [5800/10000], Loss: 1.1928\n",
            "Epoch [6/20], Step [5900/10000], Loss: 0.6189\n",
            "Epoch [6/20], Step [6000/10000], Loss: 0.7912\n",
            "Epoch [6/20], Step [6100/10000], Loss: 1.0941\n",
            "Epoch [6/20], Step [6200/10000], Loss: 1.1168\n",
            "Epoch [6/20], Step [6300/10000], Loss: 1.7400\n",
            "Epoch [6/20], Step [6400/10000], Loss: 0.5929\n",
            "Epoch [6/20], Step [6500/10000], Loss: 0.6682\n",
            "Epoch [6/20], Step [6600/10000], Loss: 1.0196\n",
            "Epoch [6/20], Step [6700/10000], Loss: 0.5932\n",
            "Epoch [6/20], Step [6800/10000], Loss: 0.6691\n",
            "Epoch [6/20], Step [6900/10000], Loss: 0.8407\n",
            "Epoch [6/20], Step [7000/10000], Loss: 0.7874\n",
            "Epoch [6/20], Step [7100/10000], Loss: 1.4354\n",
            "Epoch [6/20], Step [7200/10000], Loss: 1.0834\n",
            "Epoch [6/20], Step [7300/10000], Loss: 0.7011\n",
            "Epoch [6/20], Step [7400/10000], Loss: 0.8358\n",
            "Epoch [6/20], Step [7500/10000], Loss: 1.1205\n",
            "Epoch [6/20], Step [7600/10000], Loss: 0.9796\n",
            "Epoch [6/20], Step [7700/10000], Loss: 0.4544\n",
            "Epoch [6/20], Step [7800/10000], Loss: 0.9039\n",
            "Epoch [6/20], Step [7900/10000], Loss: 0.4177\n",
            "Epoch [6/20], Step [8000/10000], Loss: 0.6453\n",
            "Epoch [6/20], Step [8100/10000], Loss: 1.3979\n",
            "Epoch [6/20], Step [8200/10000], Loss: 0.2827\n",
            "Epoch [6/20], Step [8300/10000], Loss: 1.3928\n",
            "Epoch [6/20], Step [8400/10000], Loss: 0.4080\n",
            "Epoch [6/20], Step [8500/10000], Loss: 1.2727\n",
            "Epoch [6/20], Step [8600/10000], Loss: 0.4621\n",
            "Epoch [6/20], Step [8700/10000], Loss: 1.0090\n",
            "Epoch [6/20], Step [8800/10000], Loss: 0.3824\n",
            "Epoch [6/20], Step [8900/10000], Loss: 0.6351\n",
            "Epoch [6/20], Step [9000/10000], Loss: 2.4552\n",
            "Epoch [6/20], Step [9100/10000], Loss: 0.4272\n",
            "Epoch [6/20], Step [9200/10000], Loss: 1.0497\n",
            "Epoch [6/20], Step [9300/10000], Loss: 0.1373\n",
            "Epoch [6/20], Step [9400/10000], Loss: 0.9908\n",
            "Epoch [6/20], Step [9500/10000], Loss: 1.5196\n",
            "Epoch [6/20], Step [9600/10000], Loss: 0.9490\n",
            "Epoch [6/20], Step [9700/10000], Loss: 1.2805\n",
            "Epoch [6/20], Step [9800/10000], Loss: 0.4346\n",
            "Epoch [6/20], Step [9900/10000], Loss: 0.8582\n",
            "Epoch [6/20], Step [10000/10000], Loss: 0.6086\n",
            "Epoch [7/20], Step [100/10000], Loss: 1.0398\n",
            "Epoch [7/20], Step [200/10000], Loss: 0.8246\n",
            "Epoch [7/20], Step [300/10000], Loss: 1.2011\n",
            "Epoch [7/20], Step [400/10000], Loss: 0.6948\n",
            "Epoch [7/20], Step [500/10000], Loss: 0.8807\n",
            "Epoch [7/20], Step [600/10000], Loss: 0.9794\n",
            "Epoch [7/20], Step [700/10000], Loss: 0.7434\n",
            "Epoch [7/20], Step [800/10000], Loss: 0.7490\n",
            "Epoch [7/20], Step [900/10000], Loss: 0.9979\n",
            "Epoch [7/20], Step [1000/10000], Loss: 0.8817\n",
            "Epoch [7/20], Step [1100/10000], Loss: 0.9315\n",
            "Epoch [7/20], Step [1200/10000], Loss: 0.9736\n",
            "Epoch [7/20], Step [1300/10000], Loss: 1.3075\n",
            "Epoch [7/20], Step [1400/10000], Loss: 0.8990\n",
            "Epoch [7/20], Step [1500/10000], Loss: 1.1304\n",
            "Epoch [7/20], Step [1600/10000], Loss: 1.7996\n",
            "Epoch [7/20], Step [1700/10000], Loss: 0.2806\n",
            "Epoch [7/20], Step [1800/10000], Loss: 0.7599\n",
            "Epoch [7/20], Step [1900/10000], Loss: 0.5427\n",
            "Epoch [7/20], Step [2000/10000], Loss: 1.5133\n",
            "Epoch [7/20], Step [2100/10000], Loss: 2.7063\n",
            "Epoch [7/20], Step [2200/10000], Loss: 1.3186\n",
            "Epoch [7/20], Step [2300/10000], Loss: 1.5468\n",
            "Epoch [7/20], Step [2400/10000], Loss: 1.7642\n",
            "Epoch [7/20], Step [2500/10000], Loss: 1.2289\n",
            "Epoch [7/20], Step [2600/10000], Loss: 0.1176\n",
            "Epoch [7/20], Step [2700/10000], Loss: 1.2591\n",
            "Epoch [7/20], Step [2800/10000], Loss: 0.3194\n",
            "Epoch [7/20], Step [2900/10000], Loss: 0.7159\n",
            "Epoch [7/20], Step [3000/10000], Loss: 1.7148\n",
            "Epoch [7/20], Step [3100/10000], Loss: 1.2268\n",
            "Epoch [7/20], Step [3200/10000], Loss: 1.0266\n",
            "Epoch [7/20], Step [3300/10000], Loss: 0.5361\n",
            "Epoch [7/20], Step [3400/10000], Loss: 1.3845\n",
            "Epoch [7/20], Step [3500/10000], Loss: 0.5554\n",
            "Epoch [7/20], Step [3600/10000], Loss: 1.2880\n",
            "Epoch [7/20], Step [3700/10000], Loss: 1.3166\n",
            "Epoch [7/20], Step [3800/10000], Loss: 0.6331\n",
            "Epoch [7/20], Step [3900/10000], Loss: 0.7499\n",
            "Epoch [7/20], Step [4000/10000], Loss: 0.5271\n",
            "Epoch [7/20], Step [4100/10000], Loss: 1.4420\n",
            "Epoch [7/20], Step [4200/10000], Loss: 0.3976\n",
            "Epoch [7/20], Step [4300/10000], Loss: 0.4361\n",
            "Epoch [7/20], Step [4400/10000], Loss: 1.0035\n",
            "Epoch [7/20], Step [4500/10000], Loss: 1.9362\n",
            "Epoch [7/20], Step [4600/10000], Loss: 1.1954\n",
            "Epoch [7/20], Step [4700/10000], Loss: 0.5101\n",
            "Epoch [7/20], Step [4800/10000], Loss: 1.1515\n",
            "Epoch [7/20], Step [4900/10000], Loss: 0.6988\n",
            "Epoch [7/20], Step [5000/10000], Loss: 1.0574\n",
            "Epoch [7/20], Step [5100/10000], Loss: 1.6327\n",
            "Epoch [7/20], Step [5200/10000], Loss: 0.6455\n",
            "Epoch [7/20], Step [5300/10000], Loss: 1.5337\n",
            "Epoch [7/20], Step [5400/10000], Loss: 1.5903\n",
            "Epoch [7/20], Step [5500/10000], Loss: 1.0910\n",
            "Epoch [7/20], Step [5600/10000], Loss: 0.8675\n",
            "Epoch [7/20], Step [5700/10000], Loss: 1.0562\n",
            "Epoch [7/20], Step [5800/10000], Loss: 0.6269\n",
            "Epoch [7/20], Step [5900/10000], Loss: 1.6536\n",
            "Epoch [7/20], Step [6000/10000], Loss: 2.4135\n",
            "Epoch [7/20], Step [6100/10000], Loss: 0.4607\n",
            "Epoch [7/20], Step [6200/10000], Loss: 0.5910\n",
            "Epoch [7/20], Step [6300/10000], Loss: 0.6646\n",
            "Epoch [7/20], Step [6400/10000], Loss: 1.6709\n",
            "Epoch [7/20], Step [6500/10000], Loss: 0.8225\n",
            "Epoch [7/20], Step [6600/10000], Loss: 0.9210\n",
            "Epoch [7/20], Step [6700/10000], Loss: 0.7266\n",
            "Epoch [7/20], Step [6800/10000], Loss: 0.6387\n",
            "Epoch [7/20], Step [6900/10000], Loss: 1.2224\n",
            "Epoch [7/20], Step [7000/10000], Loss: 0.8841\n",
            "Epoch [7/20], Step [7100/10000], Loss: 0.2596\n",
            "Epoch [7/20], Step [7200/10000], Loss: 1.0503\n",
            "Epoch [7/20], Step [7300/10000], Loss: 1.1054\n",
            "Epoch [7/20], Step [7400/10000], Loss: 0.9057\n",
            "Epoch [7/20], Step [7500/10000], Loss: 1.2961\n",
            "Epoch [7/20], Step [7600/10000], Loss: 1.0123\n",
            "Epoch [7/20], Step [7700/10000], Loss: 0.3562\n",
            "Epoch [7/20], Step [7800/10000], Loss: 1.2421\n",
            "Epoch [7/20], Step [7900/10000], Loss: 1.4686\n",
            "Epoch [7/20], Step [8000/10000], Loss: 1.1468\n",
            "Epoch [7/20], Step [8100/10000], Loss: 1.3181\n",
            "Epoch [7/20], Step [8200/10000], Loss: 2.0086\n",
            "Epoch [7/20], Step [8300/10000], Loss: 0.2973\n",
            "Epoch [7/20], Step [8400/10000], Loss: 0.9350\n",
            "Epoch [7/20], Step [8500/10000], Loss: 0.7063\n",
            "Epoch [7/20], Step [8600/10000], Loss: 0.4950\n",
            "Epoch [7/20], Step [8700/10000], Loss: 1.0335\n",
            "Epoch [7/20], Step [8800/10000], Loss: 1.7252\n",
            "Epoch [7/20], Step [8900/10000], Loss: 0.4161\n",
            "Epoch [7/20], Step [9000/10000], Loss: 0.9459\n",
            "Epoch [7/20], Step [9100/10000], Loss: 0.2161\n",
            "Epoch [7/20], Step [9200/10000], Loss: 1.2707\n",
            "Epoch [7/20], Step [9300/10000], Loss: 0.6798\n",
            "Epoch [7/20], Step [9400/10000], Loss: 0.6477\n",
            "Epoch [7/20], Step [9500/10000], Loss: 2.0909\n",
            "Epoch [7/20], Step [9600/10000], Loss: 1.5523\n",
            "Epoch [7/20], Step [9700/10000], Loss: 1.0716\n",
            "Epoch [7/20], Step [9800/10000], Loss: 0.8996\n",
            "Epoch [7/20], Step [9900/10000], Loss: 0.4511\n",
            "Epoch [7/20], Step [10000/10000], Loss: 0.9426\n",
            "Epoch [8/20], Step [100/10000], Loss: 0.8677\n",
            "Epoch [8/20], Step [200/10000], Loss: 0.9322\n",
            "Epoch [8/20], Step [300/10000], Loss: 0.2504\n",
            "Epoch [8/20], Step [400/10000], Loss: 1.0389\n",
            "Epoch [8/20], Step [500/10000], Loss: 1.0162\n",
            "Epoch [8/20], Step [600/10000], Loss: 0.6819\n",
            "Epoch [8/20], Step [700/10000], Loss: 0.3640\n",
            "Epoch [8/20], Step [800/10000], Loss: 1.8136\n",
            "Epoch [8/20], Step [900/10000], Loss: 0.9666\n",
            "Epoch [8/20], Step [1000/10000], Loss: 1.1837\n",
            "Epoch [8/20], Step [1100/10000], Loss: 1.0808\n",
            "Epoch [8/20], Step [1200/10000], Loss: 1.9245\n",
            "Epoch [8/20], Step [1300/10000], Loss: 0.3360\n",
            "Epoch [8/20], Step [1400/10000], Loss: 1.5086\n",
            "Epoch [8/20], Step [1500/10000], Loss: 0.8995\n",
            "Epoch [8/20], Step [1600/10000], Loss: 0.1197\n",
            "Epoch [8/20], Step [1700/10000], Loss: 0.6123\n",
            "Epoch [8/20], Step [1800/10000], Loss: 0.6066\n",
            "Epoch [8/20], Step [1900/10000], Loss: 0.8673\n",
            "Epoch [8/20], Step [2000/10000], Loss: 0.6166\n",
            "Epoch [8/20], Step [2100/10000], Loss: 1.1591\n",
            "Epoch [8/20], Step [2200/10000], Loss: 1.4396\n",
            "Epoch [8/20], Step [2300/10000], Loss: 1.0840\n",
            "Epoch [8/20], Step [2400/10000], Loss: 0.6651\n",
            "Epoch [8/20], Step [2500/10000], Loss: 0.6671\n",
            "Epoch [8/20], Step [2600/10000], Loss: 0.9969\n",
            "Epoch [8/20], Step [2700/10000], Loss: 0.6897\n",
            "Epoch [8/20], Step [2800/10000], Loss: 0.7436\n",
            "Epoch [8/20], Step [2900/10000], Loss: 0.6972\n",
            "Epoch [8/20], Step [3000/10000], Loss: 0.6197\n",
            "Epoch [8/20], Step [3100/10000], Loss: 0.2956\n",
            "Epoch [8/20], Step [3200/10000], Loss: 0.3495\n",
            "Epoch [8/20], Step [3300/10000], Loss: 1.6850\n",
            "Epoch [8/20], Step [3400/10000], Loss: 0.9265\n",
            "Epoch [8/20], Step [3500/10000], Loss: 0.7964\n",
            "Epoch [8/20], Step [3600/10000], Loss: 0.6130\n",
            "Epoch [8/20], Step [3700/10000], Loss: 2.0826\n",
            "Epoch [8/20], Step [3800/10000], Loss: 1.5450\n",
            "Epoch [8/20], Step [3900/10000], Loss: 1.0953\n",
            "Epoch [8/20], Step [4000/10000], Loss: 1.0673\n",
            "Epoch [8/20], Step [4100/10000], Loss: 1.7281\n",
            "Epoch [8/20], Step [4200/10000], Loss: 0.4064\n",
            "Epoch [8/20], Step [4300/10000], Loss: 2.6459\n",
            "Epoch [8/20], Step [4400/10000], Loss: 1.0974\n",
            "Epoch [8/20], Step [4500/10000], Loss: 1.9466\n",
            "Epoch [8/20], Step [4600/10000], Loss: 0.4775\n",
            "Epoch [8/20], Step [4700/10000], Loss: 0.5126\n",
            "Epoch [8/20], Step [4800/10000], Loss: 0.3011\n",
            "Epoch [8/20], Step [4900/10000], Loss: 1.3489\n",
            "Epoch [8/20], Step [5000/10000], Loss: 0.7180\n",
            "Epoch [8/20], Step [5100/10000], Loss: 0.5386\n",
            "Epoch [8/20], Step [5200/10000], Loss: 2.2462\n",
            "Epoch [8/20], Step [5300/10000], Loss: 0.9219\n",
            "Epoch [8/20], Step [5400/10000], Loss: 1.0345\n",
            "Epoch [8/20], Step [5500/10000], Loss: 1.3540\n",
            "Epoch [8/20], Step [5600/10000], Loss: 1.1211\n",
            "Epoch [8/20], Step [5700/10000], Loss: 0.6719\n",
            "Epoch [8/20], Step [5800/10000], Loss: 0.5400\n",
            "Epoch [8/20], Step [5900/10000], Loss: 0.9668\n",
            "Epoch [8/20], Step [6000/10000], Loss: 0.7387\n",
            "Epoch [8/20], Step [6100/10000], Loss: 0.9661\n",
            "Epoch [8/20], Step [6200/10000], Loss: 1.2759\n",
            "Epoch [8/20], Step [6300/10000], Loss: 0.4528\n",
            "Epoch [8/20], Step [6400/10000], Loss: 1.7457\n",
            "Epoch [8/20], Step [6500/10000], Loss: 1.0287\n",
            "Epoch [8/20], Step [6600/10000], Loss: 0.5139\n",
            "Epoch [8/20], Step [6700/10000], Loss: 1.9237\n",
            "Epoch [8/20], Step [6800/10000], Loss: 1.4084\n",
            "Epoch [8/20], Step [6900/10000], Loss: 1.4273\n",
            "Epoch [8/20], Step [7000/10000], Loss: 1.2839\n",
            "Epoch [8/20], Step [7100/10000], Loss: 0.7679\n",
            "Epoch [8/20], Step [7200/10000], Loss: 1.3186\n",
            "Epoch [8/20], Step [7300/10000], Loss: 0.4972\n",
            "Epoch [8/20], Step [7400/10000], Loss: 0.9343\n",
            "Epoch [8/20], Step [7500/10000], Loss: 0.3397\n",
            "Epoch [8/20], Step [7600/10000], Loss: 0.8610\n",
            "Epoch [8/20], Step [7700/10000], Loss: 1.1977\n",
            "Epoch [8/20], Step [7800/10000], Loss: 0.7262\n",
            "Epoch [8/20], Step [7900/10000], Loss: 0.3358\n",
            "Epoch [8/20], Step [8000/10000], Loss: 0.7755\n",
            "Epoch [8/20], Step [8100/10000], Loss: 0.3735\n",
            "Epoch [8/20], Step [8200/10000], Loss: 0.8038\n",
            "Epoch [8/20], Step [8300/10000], Loss: 0.9154\n",
            "Epoch [8/20], Step [8400/10000], Loss: 1.0948\n",
            "Epoch [8/20], Step [8500/10000], Loss: 0.8651\n",
            "Epoch [8/20], Step [8600/10000], Loss: 0.3752\n",
            "Epoch [8/20], Step [8700/10000], Loss: 1.1904\n",
            "Epoch [8/20], Step [8800/10000], Loss: 1.1972\n",
            "Epoch [8/20], Step [8900/10000], Loss: 0.7153\n",
            "Epoch [8/20], Step [9000/10000], Loss: 1.4295\n",
            "Epoch [8/20], Step [9100/10000], Loss: 1.9545\n",
            "Epoch [8/20], Step [9200/10000], Loss: 1.1775\n",
            "Epoch [8/20], Step [9300/10000], Loss: 0.9669\n",
            "Epoch [8/20], Step [9400/10000], Loss: 0.9072\n",
            "Epoch [8/20], Step [9500/10000], Loss: 0.4265\n",
            "Epoch [8/20], Step [9600/10000], Loss: 0.4628\n",
            "Epoch [8/20], Step [9700/10000], Loss: 0.6270\n",
            "Epoch [8/20], Step [9800/10000], Loss: 1.2798\n",
            "Epoch [8/20], Step [9900/10000], Loss: 0.7450\n",
            "Epoch [8/20], Step [10000/10000], Loss: 0.6060\n",
            "Epoch [9/20], Step [100/10000], Loss: 1.0476\n",
            "Epoch [9/20], Step [200/10000], Loss: 0.4581\n",
            "Epoch [9/20], Step [300/10000], Loss: 1.2429\n",
            "Epoch [9/20], Step [400/10000], Loss: 1.4579\n",
            "Epoch [9/20], Step [500/10000], Loss: 0.8071\n",
            "Epoch [9/20], Step [600/10000], Loss: 2.0135\n",
            "Epoch [9/20], Step [700/10000], Loss: 0.8769\n",
            "Epoch [9/20], Step [800/10000], Loss: 0.7882\n",
            "Epoch [9/20], Step [900/10000], Loss: 1.5241\n",
            "Epoch [9/20], Step [1000/10000], Loss: 0.5199\n",
            "Epoch [9/20], Step [1100/10000], Loss: 2.0130\n",
            "Epoch [9/20], Step [1200/10000], Loss: 0.3375\n",
            "Epoch [9/20], Step [1300/10000], Loss: 0.5950\n",
            "Epoch [9/20], Step [1400/10000], Loss: 1.1881\n",
            "Epoch [9/20], Step [1500/10000], Loss: 1.9556\n",
            "Epoch [9/20], Step [1600/10000], Loss: 1.6199\n",
            "Epoch [9/20], Step [1700/10000], Loss: 1.1041\n",
            "Epoch [9/20], Step [1800/10000], Loss: 1.3114\n",
            "Epoch [9/20], Step [1900/10000], Loss: 0.6639\n",
            "Epoch [9/20], Step [2000/10000], Loss: 0.6243\n",
            "Epoch [9/20], Step [2100/10000], Loss: 0.6996\n",
            "Epoch [9/20], Step [2200/10000], Loss: 2.0084\n",
            "Epoch [9/20], Step [2300/10000], Loss: 1.1777\n",
            "Epoch [9/20], Step [2400/10000], Loss: 0.9089\n",
            "Epoch [9/20], Step [2500/10000], Loss: 1.1225\n",
            "Epoch [9/20], Step [2600/10000], Loss: 1.9009\n",
            "Epoch [9/20], Step [2700/10000], Loss: 1.0106\n",
            "Epoch [9/20], Step [2800/10000], Loss: 1.0079\n",
            "Epoch [9/20], Step [2900/10000], Loss: 1.5944\n",
            "Epoch [9/20], Step [3000/10000], Loss: 0.8606\n",
            "Epoch [9/20], Step [3100/10000], Loss: 1.6375\n",
            "Epoch [9/20], Step [3200/10000], Loss: 0.2897\n",
            "Epoch [9/20], Step [3300/10000], Loss: 1.5704\n",
            "Epoch [9/20], Step [3400/10000], Loss: 1.2312\n",
            "Epoch [9/20], Step [3500/10000], Loss: 0.5673\n",
            "Epoch [9/20], Step [3600/10000], Loss: 0.8700\n",
            "Epoch [9/20], Step [3700/10000], Loss: 0.9686\n",
            "Epoch [9/20], Step [3800/10000], Loss: 1.5692\n",
            "Epoch [9/20], Step [3900/10000], Loss: 1.3353\n",
            "Epoch [9/20], Step [4000/10000], Loss: 0.4839\n",
            "Epoch [9/20], Step [4100/10000], Loss: 1.3879\n",
            "Epoch [9/20], Step [4200/10000], Loss: 0.1542\n",
            "Epoch [9/20], Step [4300/10000], Loss: 0.8295\n",
            "Epoch [9/20], Step [4400/10000], Loss: 0.3388\n",
            "Epoch [9/20], Step [4500/10000], Loss: 0.9633\n",
            "Epoch [9/20], Step [4600/10000], Loss: 0.3917\n",
            "Epoch [9/20], Step [4700/10000], Loss: 0.7057\n",
            "Epoch [9/20], Step [4800/10000], Loss: 1.4252\n",
            "Epoch [9/20], Step [4900/10000], Loss: 1.8889\n",
            "Epoch [9/20], Step [5000/10000], Loss: 0.9161\n",
            "Epoch [9/20], Step [5100/10000], Loss: 0.9120\n",
            "Epoch [9/20], Step [5200/10000], Loss: 0.8587\n",
            "Epoch [9/20], Step [5300/10000], Loss: 1.4399\n",
            "Epoch [9/20], Step [5400/10000], Loss: 1.8091\n",
            "Epoch [9/20], Step [5500/10000], Loss: 1.0728\n",
            "Epoch [9/20], Step [5600/10000], Loss: 0.3123\n",
            "Epoch [9/20], Step [5700/10000], Loss: 0.8153\n",
            "Epoch [9/20], Step [5800/10000], Loss: 0.1056\n",
            "Epoch [9/20], Step [5900/10000], Loss: 0.4986\n",
            "Epoch [9/20], Step [6000/10000], Loss: 0.8235\n",
            "Epoch [9/20], Step [6100/10000], Loss: 0.4784\n",
            "Epoch [9/20], Step [6200/10000], Loss: 1.6559\n",
            "Epoch [9/20], Step [6300/10000], Loss: 1.0603\n",
            "Epoch [9/20], Step [6400/10000], Loss: 0.6940\n",
            "Epoch [9/20], Step [6500/10000], Loss: 2.4380\n",
            "Epoch [9/20], Step [6600/10000], Loss: 1.7268\n",
            "Epoch [9/20], Step [6700/10000], Loss: 0.8964\n",
            "Epoch [9/20], Step [6800/10000], Loss: 0.7232\n",
            "Epoch [9/20], Step [6900/10000], Loss: 0.7503\n",
            "Epoch [9/20], Step [7000/10000], Loss: 0.7327\n",
            "Epoch [9/20], Step [7100/10000], Loss: 0.2597\n",
            "Epoch [9/20], Step [7200/10000], Loss: 0.6055\n",
            "Epoch [9/20], Step [7300/10000], Loss: 1.1379\n",
            "Epoch [9/20], Step [7400/10000], Loss: 0.6628\n",
            "Epoch [9/20], Step [7500/10000], Loss: 1.0416\n",
            "Epoch [9/20], Step [7600/10000], Loss: 1.6722\n",
            "Epoch [9/20], Step [7700/10000], Loss: 0.7241\n",
            "Epoch [9/20], Step [7800/10000], Loss: 0.4000\n",
            "Epoch [9/20], Step [7900/10000], Loss: 1.0514\n",
            "Epoch [9/20], Step [8000/10000], Loss: 1.7880\n",
            "Epoch [9/20], Step [8100/10000], Loss: 1.3134\n",
            "Epoch [9/20], Step [8200/10000], Loss: 1.0023\n",
            "Epoch [9/20], Step [8300/10000], Loss: 1.4481\n",
            "Epoch [9/20], Step [8400/10000], Loss: 1.7148\n",
            "Epoch [9/20], Step [8500/10000], Loss: 0.2167\n",
            "Epoch [9/20], Step [8600/10000], Loss: 1.5447\n",
            "Epoch [9/20], Step [8700/10000], Loss: 0.8550\n",
            "Epoch [9/20], Step [8800/10000], Loss: 1.3541\n",
            "Epoch [9/20], Step [8900/10000], Loss: 0.8947\n",
            "Epoch [9/20], Step [9000/10000], Loss: 0.1510\n",
            "Epoch [9/20], Step [9100/10000], Loss: 0.8800\n",
            "Epoch [9/20], Step [9200/10000], Loss: 0.3584\n",
            "Epoch [9/20], Step [9300/10000], Loss: 1.0072\n",
            "Epoch [9/20], Step [9400/10000], Loss: 0.9906\n",
            "Epoch [9/20], Step [9500/10000], Loss: 0.3555\n",
            "Epoch [9/20], Step [9600/10000], Loss: 0.4683\n",
            "Epoch [9/20], Step [9700/10000], Loss: 2.8832\n",
            "Epoch [9/20], Step [9800/10000], Loss: 0.6916\n",
            "Epoch [9/20], Step [9900/10000], Loss: 1.0687\n",
            "Epoch [9/20], Step [10000/10000], Loss: 0.3091\n",
            "Epoch [10/20], Step [100/10000], Loss: 1.1912\n",
            "Epoch [10/20], Step [200/10000], Loss: 0.6232\n",
            "Epoch [10/20], Step [300/10000], Loss: 1.2267\n",
            "Epoch [10/20], Step [400/10000], Loss: 0.1487\n",
            "Epoch [10/20], Step [500/10000], Loss: 1.1414\n",
            "Epoch [10/20], Step [600/10000], Loss: 0.9969\n",
            "Epoch [10/20], Step [700/10000], Loss: 0.6375\n",
            "Epoch [10/20], Step [800/10000], Loss: 0.8427\n",
            "Epoch [10/20], Step [900/10000], Loss: 1.1805\n",
            "Epoch [10/20], Step [1000/10000], Loss: 1.0020\n",
            "Epoch [10/20], Step [1100/10000], Loss: 0.8509\n",
            "Epoch [10/20], Step [1200/10000], Loss: 0.5635\n",
            "Epoch [10/20], Step [1300/10000], Loss: 2.6427\n",
            "Epoch [10/20], Step [1400/10000], Loss: 0.9447\n",
            "Epoch [10/20], Step [1500/10000], Loss: 0.7557\n",
            "Epoch [10/20], Step [1600/10000], Loss: 0.7311\n",
            "Epoch [10/20], Step [1700/10000], Loss: 0.6556\n",
            "Epoch [10/20], Step [1800/10000], Loss: 0.2461\n",
            "Epoch [10/20], Step [1900/10000], Loss: 1.1520\n",
            "Epoch [10/20], Step [2000/10000], Loss: 0.0775\n",
            "Epoch [10/20], Step [2100/10000], Loss: 1.1975\n",
            "Epoch [10/20], Step [2200/10000], Loss: 1.1890\n",
            "Epoch [10/20], Step [2300/10000], Loss: 0.4531\n",
            "Epoch [10/20], Step [2400/10000], Loss: 1.2040\n",
            "Epoch [10/20], Step [2500/10000], Loss: 1.4753\n",
            "Epoch [10/20], Step [2600/10000], Loss: 0.8056\n",
            "Epoch [10/20], Step [2700/10000], Loss: 0.3097\n",
            "Epoch [10/20], Step [2800/10000], Loss: 0.8359\n",
            "Epoch [10/20], Step [2900/10000], Loss: 1.3129\n",
            "Epoch [10/20], Step [3000/10000], Loss: 1.4847\n",
            "Epoch [10/20], Step [3100/10000], Loss: 0.8102\n",
            "Epoch [10/20], Step [3200/10000], Loss: 1.4961\n",
            "Epoch [10/20], Step [3300/10000], Loss: 0.6377\n",
            "Epoch [10/20], Step [3400/10000], Loss: 0.7012\n",
            "Epoch [10/20], Step [3500/10000], Loss: 0.6034\n",
            "Epoch [10/20], Step [3600/10000], Loss: 0.8778\n",
            "Epoch [10/20], Step [3700/10000], Loss: 1.1813\n",
            "Epoch [10/20], Step [3800/10000], Loss: 0.7990\n",
            "Epoch [10/20], Step [3900/10000], Loss: 1.1233\n",
            "Epoch [10/20], Step [4000/10000], Loss: 0.8448\n",
            "Epoch [10/20], Step [4100/10000], Loss: 2.4476\n",
            "Epoch [10/20], Step [4200/10000], Loss: 2.4470\n",
            "Epoch [10/20], Step [4300/10000], Loss: 1.0672\n",
            "Epoch [10/20], Step [4400/10000], Loss: 1.7532\n",
            "Epoch [10/20], Step [4500/10000], Loss: 1.5907\n",
            "Epoch [10/20], Step [4600/10000], Loss: 0.6099\n",
            "Epoch [10/20], Step [4700/10000], Loss: 0.9254\n",
            "Epoch [10/20], Step [4800/10000], Loss: 0.6054\n",
            "Epoch [10/20], Step [4900/10000], Loss: 0.9324\n",
            "Epoch [10/20], Step [5000/10000], Loss: 0.6885\n",
            "Epoch [10/20], Step [5100/10000], Loss: 0.7516\n",
            "Epoch [10/20], Step [5200/10000], Loss: 1.6544\n",
            "Epoch [10/20], Step [5300/10000], Loss: 0.3231\n",
            "Epoch [10/20], Step [5400/10000], Loss: 0.7590\n",
            "Epoch [10/20], Step [5500/10000], Loss: 0.9608\n",
            "Epoch [10/20], Step [5600/10000], Loss: 1.1053\n",
            "Epoch [10/20], Step [5700/10000], Loss: 1.1185\n",
            "Epoch [10/20], Step [5800/10000], Loss: 1.8019\n",
            "Epoch [10/20], Step [5900/10000], Loss: 1.6286\n",
            "Epoch [10/20], Step [6000/10000], Loss: 0.4705\n",
            "Epoch [10/20], Step [6100/10000], Loss: 2.8134\n",
            "Epoch [10/20], Step [6200/10000], Loss: 1.5731\n",
            "Epoch [10/20], Step [6300/10000], Loss: 1.3579\n",
            "Epoch [10/20], Step [6400/10000], Loss: 1.3842\n",
            "Epoch [10/20], Step [6500/10000], Loss: 1.7855\n",
            "Epoch [10/20], Step [6600/10000], Loss: 0.6496\n",
            "Epoch [10/20], Step [6700/10000], Loss: 0.5048\n",
            "Epoch [10/20], Step [6800/10000], Loss: 0.7980\n",
            "Epoch [10/20], Step [6900/10000], Loss: 0.4412\n",
            "Epoch [10/20], Step [7000/10000], Loss: 0.6854\n",
            "Epoch [10/20], Step [7100/10000], Loss: 1.1822\n",
            "Epoch [10/20], Step [7200/10000], Loss: 0.8947\n",
            "Epoch [10/20], Step [7300/10000], Loss: 1.7673\n",
            "Epoch [10/20], Step [7400/10000], Loss: 0.4884\n",
            "Epoch [10/20], Step [7500/10000], Loss: 0.4789\n",
            "Epoch [10/20], Step [7600/10000], Loss: 0.5794\n",
            "Epoch [10/20], Step [7700/10000], Loss: 0.5530\n",
            "Epoch [10/20], Step [7800/10000], Loss: 0.9794\n",
            "Epoch [10/20], Step [7900/10000], Loss: 1.2120\n",
            "Epoch [10/20], Step [8000/10000], Loss: 0.9974\n",
            "Epoch [10/20], Step [8100/10000], Loss: 0.6897\n",
            "Epoch [10/20], Step [8200/10000], Loss: 0.2513\n",
            "Epoch [10/20], Step [8300/10000], Loss: 0.9519\n",
            "Epoch [10/20], Step [8400/10000], Loss: 0.6337\n",
            "Epoch [10/20], Step [8500/10000], Loss: 1.6091\n",
            "Epoch [10/20], Step [8600/10000], Loss: 1.4827\n",
            "Epoch [10/20], Step [8700/10000], Loss: 1.5028\n",
            "Epoch [10/20], Step [8800/10000], Loss: 1.4539\n",
            "Epoch [10/20], Step [8900/10000], Loss: 0.9762\n",
            "Epoch [10/20], Step [9000/10000], Loss: 0.7222\n",
            "Epoch [10/20], Step [9100/10000], Loss: 0.9344\n",
            "Epoch [10/20], Step [9200/10000], Loss: 1.6832\n",
            "Epoch [10/20], Step [9300/10000], Loss: 0.4673\n",
            "Epoch [10/20], Step [9400/10000], Loss: 0.5967\n",
            "Epoch [10/20], Step [9500/10000], Loss: 0.3456\n",
            "Epoch [10/20], Step [9600/10000], Loss: 0.7884\n",
            "Epoch [10/20], Step [9700/10000], Loss: 0.6329\n",
            "Epoch [10/20], Step [9800/10000], Loss: 0.7925\n",
            "Epoch [10/20], Step [9900/10000], Loss: 0.8729\n",
            "Epoch [10/20], Step [10000/10000], Loss: 0.3298\n",
            "Epoch [11/20], Step [100/10000], Loss: 0.6424\n",
            "Epoch [11/20], Step [200/10000], Loss: 1.6712\n",
            "Epoch [11/20], Step [300/10000], Loss: 0.7998\n",
            "Epoch [11/20], Step [400/10000], Loss: 2.4013\n",
            "Epoch [11/20], Step [500/10000], Loss: 0.6288\n",
            "Epoch [11/20], Step [600/10000], Loss: 1.2212\n",
            "Epoch [11/20], Step [700/10000], Loss: 0.8066\n",
            "Epoch [11/20], Step [800/10000], Loss: 0.1560\n",
            "Epoch [11/20], Step [900/10000], Loss: 0.3846\n",
            "Epoch [11/20], Step [1000/10000], Loss: 0.6278\n",
            "Epoch [11/20], Step [1100/10000], Loss: 1.0355\n",
            "Epoch [11/20], Step [1200/10000], Loss: 1.1023\n",
            "Epoch [11/20], Step [1300/10000], Loss: 1.8405\n",
            "Epoch [11/20], Step [1400/10000], Loss: 0.6288\n",
            "Epoch [11/20], Step [1500/10000], Loss: 0.1871\n",
            "Epoch [11/20], Step [1600/10000], Loss: 0.4765\n",
            "Epoch [11/20], Step [1700/10000], Loss: 0.3288\n",
            "Epoch [11/20], Step [1800/10000], Loss: 2.0384\n",
            "Epoch [11/20], Step [1900/10000], Loss: 1.2572\n",
            "Epoch [11/20], Step [2000/10000], Loss: 0.6282\n",
            "Epoch [11/20], Step [2100/10000], Loss: 0.5802\n",
            "Epoch [11/20], Step [2200/10000], Loss: 1.0351\n",
            "Epoch [11/20], Step [2300/10000], Loss: 0.3272\n",
            "Epoch [11/20], Step [2400/10000], Loss: 0.7566\n",
            "Epoch [11/20], Step [2500/10000], Loss: 0.6189\n",
            "Epoch [11/20], Step [2600/10000], Loss: 0.5184\n",
            "Epoch [11/20], Step [2700/10000], Loss: 0.7249\n",
            "Epoch [11/20], Step [2800/10000], Loss: 0.1308\n",
            "Epoch [11/20], Step [2900/10000], Loss: 1.1610\n",
            "Epoch [11/20], Step [3000/10000], Loss: 0.7640\n",
            "Epoch [11/20], Step [3100/10000], Loss: 1.6830\n",
            "Epoch [11/20], Step [3200/10000], Loss: 0.4326\n",
            "Epoch [11/20], Step [3300/10000], Loss: 0.5192\n",
            "Epoch [11/20], Step [3400/10000], Loss: 0.7267\n",
            "Epoch [11/20], Step [3500/10000], Loss: 0.8710\n",
            "Epoch [11/20], Step [3600/10000], Loss: 1.4684\n",
            "Epoch [11/20], Step [3700/10000], Loss: 1.5591\n",
            "Epoch [11/20], Step [3800/10000], Loss: 0.3805\n",
            "Epoch [11/20], Step [3900/10000], Loss: 1.1711\n",
            "Epoch [11/20], Step [4000/10000], Loss: 2.5482\n",
            "Epoch [11/20], Step [4100/10000], Loss: 0.7374\n",
            "Epoch [11/20], Step [4200/10000], Loss: 1.7474\n",
            "Epoch [11/20], Step [4300/10000], Loss: 0.7115\n",
            "Epoch [11/20], Step [4400/10000], Loss: 1.5404\n",
            "Epoch [11/20], Step [4500/10000], Loss: 1.7888\n",
            "Epoch [11/20], Step [4600/10000], Loss: 0.9401\n",
            "Epoch [11/20], Step [4700/10000], Loss: 0.8770\n",
            "Epoch [11/20], Step [4800/10000], Loss: 0.4576\n",
            "Epoch [11/20], Step [4900/10000], Loss: 1.0131\n",
            "Epoch [11/20], Step [5000/10000], Loss: 0.8274\n",
            "Epoch [11/20], Step [5100/10000], Loss: 0.1274\n",
            "Epoch [11/20], Step [5200/10000], Loss: 0.8361\n",
            "Epoch [11/20], Step [5300/10000], Loss: 0.6891\n",
            "Epoch [11/20], Step [5400/10000], Loss: 0.5034\n",
            "Epoch [11/20], Step [5500/10000], Loss: 1.1720\n",
            "Epoch [11/20], Step [5600/10000], Loss: 0.6973\n",
            "Epoch [11/20], Step [5700/10000], Loss: 2.0852\n",
            "Epoch [11/20], Step [5800/10000], Loss: 0.4950\n",
            "Epoch [11/20], Step [5900/10000], Loss: 0.1710\n",
            "Epoch [11/20], Step [6000/10000], Loss: 0.6372\n",
            "Epoch [11/20], Step [6100/10000], Loss: 0.3072\n",
            "Epoch [11/20], Step [6200/10000], Loss: 0.3722\n",
            "Epoch [11/20], Step [6300/10000], Loss: 0.4595\n",
            "Epoch [11/20], Step [6400/10000], Loss: 1.0888\n",
            "Epoch [11/20], Step [6500/10000], Loss: 1.3609\n",
            "Epoch [11/20], Step [6600/10000], Loss: 1.2527\n",
            "Epoch [11/20], Step [6700/10000], Loss: 0.7769\n",
            "Epoch [11/20], Step [6800/10000], Loss: 0.8853\n",
            "Epoch [11/20], Step [6900/10000], Loss: 0.7694\n",
            "Epoch [11/20], Step [7000/10000], Loss: 0.6370\n",
            "Epoch [11/20], Step [7100/10000], Loss: 0.8235\n",
            "Epoch [11/20], Step [7200/10000], Loss: 1.1769\n",
            "Epoch [11/20], Step [7300/10000], Loss: 0.7563\n",
            "Epoch [11/20], Step [7400/10000], Loss: 0.9760\n",
            "Epoch [11/20], Step [7500/10000], Loss: 1.3954\n",
            "Epoch [11/20], Step [7600/10000], Loss: 1.8929\n",
            "Epoch [11/20], Step [7700/10000], Loss: 1.3285\n",
            "Epoch [11/20], Step [7800/10000], Loss: 0.7438\n",
            "Epoch [11/20], Step [7900/10000], Loss: 0.8336\n",
            "Epoch [11/20], Step [8000/10000], Loss: 0.6526\n",
            "Epoch [11/20], Step [8100/10000], Loss: 1.0733\n",
            "Epoch [11/20], Step [8200/10000], Loss: 1.2599\n",
            "Epoch [11/20], Step [8300/10000], Loss: 0.6354\n",
            "Epoch [11/20], Step [8400/10000], Loss: 0.8730\n",
            "Epoch [11/20], Step [8500/10000], Loss: 0.5838\n",
            "Epoch [11/20], Step [8600/10000], Loss: 0.5547\n",
            "Epoch [11/20], Step [8700/10000], Loss: 0.7431\n",
            "Epoch [11/20], Step [8800/10000], Loss: 0.6570\n",
            "Epoch [11/20], Step [8900/10000], Loss: 0.4930\n",
            "Epoch [11/20], Step [9000/10000], Loss: 0.8120\n",
            "Epoch [11/20], Step [9100/10000], Loss: 0.6038\n",
            "Epoch [11/20], Step [9200/10000], Loss: 0.9624\n",
            "Epoch [11/20], Step [9300/10000], Loss: 1.8345\n",
            "Epoch [11/20], Step [9400/10000], Loss: 0.9538\n",
            "Epoch [11/20], Step [9500/10000], Loss: 1.6054\n",
            "Epoch [11/20], Step [9600/10000], Loss: 0.7819\n",
            "Epoch [11/20], Step [9700/10000], Loss: 1.2169\n",
            "Epoch [11/20], Step [9800/10000], Loss: 1.7207\n",
            "Epoch [11/20], Step [9900/10000], Loss: 0.4750\n",
            "Epoch [11/20], Step [10000/10000], Loss: 0.2801\n",
            "Epoch [12/20], Step [100/10000], Loss: 0.7380\n",
            "Epoch [12/20], Step [200/10000], Loss: 1.2686\n",
            "Epoch [12/20], Step [300/10000], Loss: 1.3363\n",
            "Epoch [12/20], Step [400/10000], Loss: 0.1280\n",
            "Epoch [12/20], Step [500/10000], Loss: 0.3859\n",
            "Epoch [12/20], Step [600/10000], Loss: 0.9368\n",
            "Epoch [12/20], Step [700/10000], Loss: 1.8818\n",
            "Epoch [12/20], Step [800/10000], Loss: 1.2678\n",
            "Epoch [12/20], Step [900/10000], Loss: 0.8943\n",
            "Epoch [12/20], Step [1000/10000], Loss: 1.2519\n",
            "Epoch [12/20], Step [1100/10000], Loss: 1.0706\n",
            "Epoch [12/20], Step [1200/10000], Loss: 0.7830\n",
            "Epoch [12/20], Step [1300/10000], Loss: 1.5425\n",
            "Epoch [12/20], Step [1400/10000], Loss: 1.1224\n",
            "Epoch [12/20], Step [1500/10000], Loss: 0.3833\n",
            "Epoch [12/20], Step [1600/10000], Loss: 0.6922\n",
            "Epoch [12/20], Step [1700/10000], Loss: 0.6482\n",
            "Epoch [12/20], Step [1800/10000], Loss: 0.7631\n",
            "Epoch [12/20], Step [1900/10000], Loss: 0.5279\n",
            "Epoch [12/20], Step [2000/10000], Loss: 0.1860\n",
            "Epoch [12/20], Step [2100/10000], Loss: 0.8685\n",
            "Epoch [12/20], Step [2200/10000], Loss: 1.3540\n",
            "Epoch [12/20], Step [2300/10000], Loss: 0.4675\n",
            "Epoch [12/20], Step [2400/10000], Loss: 0.5366\n",
            "Epoch [12/20], Step [2500/10000], Loss: 1.9436\n",
            "Epoch [12/20], Step [2600/10000], Loss: 0.8420\n",
            "Epoch [12/20], Step [2700/10000], Loss: 1.5471\n",
            "Epoch [12/20], Step [2800/10000], Loss: 1.9874\n",
            "Epoch [12/20], Step [2900/10000], Loss: 1.0644\n",
            "Epoch [12/20], Step [3000/10000], Loss: 2.3375\n",
            "Epoch [12/20], Step [3100/10000], Loss: 0.3443\n",
            "Epoch [12/20], Step [3200/10000], Loss: 0.1790\n",
            "Epoch [12/20], Step [3300/10000], Loss: 0.6161\n",
            "Epoch [12/20], Step [3400/10000], Loss: 0.7135\n",
            "Epoch [12/20], Step [3500/10000], Loss: 0.5118\n",
            "Epoch [12/20], Step [3600/10000], Loss: 0.9355\n",
            "Epoch [12/20], Step [3700/10000], Loss: 0.4964\n",
            "Epoch [12/20], Step [3800/10000], Loss: 0.6602\n",
            "Epoch [12/20], Step [3900/10000], Loss: 0.8003\n",
            "Epoch [12/20], Step [4000/10000], Loss: 1.0253\n",
            "Epoch [12/20], Step [4100/10000], Loss: 0.8882\n",
            "Epoch [12/20], Step [4200/10000], Loss: 0.6094\n",
            "Epoch [12/20], Step [4300/10000], Loss: 1.1561\n",
            "Epoch [12/20], Step [4400/10000], Loss: 2.0698\n",
            "Epoch [12/20], Step [4500/10000], Loss: 1.4403\n",
            "Epoch [12/20], Step [4600/10000], Loss: 1.0954\n",
            "Epoch [12/20], Step [4700/10000], Loss: 0.3520\n",
            "Epoch [12/20], Step [4800/10000], Loss: 2.0061\n",
            "Epoch [12/20], Step [4900/10000], Loss: 0.9037\n",
            "Epoch [12/20], Step [5000/10000], Loss: 1.1892\n",
            "Epoch [12/20], Step [5100/10000], Loss: 0.1098\n",
            "Epoch [12/20], Step [5200/10000], Loss: 1.2923\n",
            "Epoch [12/20], Step [5300/10000], Loss: 0.3709\n",
            "Epoch [12/20], Step [5400/10000], Loss: 1.0146\n",
            "Epoch [12/20], Step [5500/10000], Loss: 0.3857\n",
            "Epoch [12/20], Step [5600/10000], Loss: 0.6645\n",
            "Epoch [12/20], Step [5700/10000], Loss: 1.3670\n",
            "Epoch [12/20], Step [5800/10000], Loss: 0.9414\n",
            "Epoch [12/20], Step [5900/10000], Loss: 1.2738\n",
            "Epoch [12/20], Step [6000/10000], Loss: 1.4750\n",
            "Epoch [12/20], Step [6100/10000], Loss: 1.0645\n",
            "Epoch [12/20], Step [6200/10000], Loss: 1.3090\n",
            "Epoch [12/20], Step [6300/10000], Loss: 2.4062\n",
            "Epoch [12/20], Step [6400/10000], Loss: 0.8118\n",
            "Epoch [12/20], Step [6500/10000], Loss: 0.6882\n",
            "Epoch [12/20], Step [6600/10000], Loss: 0.2639\n",
            "Epoch [12/20], Step [6700/10000], Loss: 0.6791\n",
            "Epoch [12/20], Step [6800/10000], Loss: 0.6583\n",
            "Epoch [12/20], Step [6900/10000], Loss: 1.0615\n",
            "Epoch [12/20], Step [7000/10000], Loss: 0.3450\n",
            "Epoch [12/20], Step [7100/10000], Loss: 0.6616\n",
            "Epoch [12/20], Step [7200/10000], Loss: 0.9842\n",
            "Epoch [12/20], Step [7300/10000], Loss: 0.3282\n",
            "Epoch [12/20], Step [7400/10000], Loss: 0.9223\n",
            "Epoch [12/20], Step [7500/10000], Loss: 1.1306\n",
            "Epoch [12/20], Step [7600/10000], Loss: 0.6731\n",
            "Epoch [12/20], Step [7700/10000], Loss: 0.5509\n",
            "Epoch [12/20], Step [7800/10000], Loss: 1.2015\n",
            "Epoch [12/20], Step [7900/10000], Loss: 1.8299\n",
            "Epoch [12/20], Step [8000/10000], Loss: 0.4382\n",
            "Epoch [12/20], Step [8100/10000], Loss: 1.1623\n",
            "Epoch [12/20], Step [8200/10000], Loss: 1.1398\n",
            "Epoch [12/20], Step [8300/10000], Loss: 0.1538\n",
            "Epoch [12/20], Step [8400/10000], Loss: 0.8076\n",
            "Epoch [12/20], Step [8500/10000], Loss: 1.7640\n",
            "Epoch [12/20], Step [8600/10000], Loss: 1.2464\n",
            "Epoch [12/20], Step [8700/10000], Loss: 0.7160\n",
            "Epoch [12/20], Step [8800/10000], Loss: 0.3030\n",
            "Epoch [12/20], Step [8900/10000], Loss: 1.4164\n",
            "Epoch [12/20], Step [9000/10000], Loss: 1.3490\n",
            "Epoch [12/20], Step [9100/10000], Loss: 0.7590\n",
            "Epoch [12/20], Step [9200/10000], Loss: 0.8192\n",
            "Epoch [12/20], Step [9300/10000], Loss: 0.4307\n",
            "Epoch [12/20], Step [9400/10000], Loss: 0.5275\n",
            "Epoch [12/20], Step [9500/10000], Loss: 1.6958\n",
            "Epoch [12/20], Step [9600/10000], Loss: 0.5849\n",
            "Epoch [12/20], Step [9700/10000], Loss: 0.6060\n",
            "Epoch [12/20], Step [9800/10000], Loss: 0.5954\n",
            "Epoch [12/20], Step [9900/10000], Loss: 0.4630\n",
            "Epoch [12/20], Step [10000/10000], Loss: 1.4632\n",
            "Epoch [13/20], Step [100/10000], Loss: 0.8493\n",
            "Epoch [13/20], Step [200/10000], Loss: 0.6201\n",
            "Epoch [13/20], Step [300/10000], Loss: 0.8522\n",
            "Epoch [13/20], Step [400/10000], Loss: 0.7236\n",
            "Epoch [13/20], Step [500/10000], Loss: 0.8344\n",
            "Epoch [13/20], Step [600/10000], Loss: 0.4044\n",
            "Epoch [13/20], Step [700/10000], Loss: 1.5929\n",
            "Epoch [13/20], Step [800/10000], Loss: 1.1210\n",
            "Epoch [13/20], Step [900/10000], Loss: 0.0734\n",
            "Epoch [13/20], Step [1000/10000], Loss: 0.7844\n",
            "Epoch [13/20], Step [1100/10000], Loss: 1.5130\n",
            "Epoch [13/20], Step [1200/10000], Loss: 1.4568\n",
            "Epoch [13/20], Step [1300/10000], Loss: 1.3590\n",
            "Epoch [13/20], Step [1400/10000], Loss: 1.4109\n",
            "Epoch [13/20], Step [1500/10000], Loss: 0.4147\n",
            "Epoch [13/20], Step [1600/10000], Loss: 0.7521\n",
            "Epoch [13/20], Step [1700/10000], Loss: 0.4178\n",
            "Epoch [13/20], Step [1800/10000], Loss: 0.5727\n",
            "Epoch [13/20], Step [1900/10000], Loss: 0.7008\n",
            "Epoch [13/20], Step [2000/10000], Loss: 0.5117\n",
            "Epoch [13/20], Step [2100/10000], Loss: 0.8485\n",
            "Epoch [13/20], Step [2200/10000], Loss: 0.2284\n",
            "Epoch [13/20], Step [2300/10000], Loss: 0.1888\n",
            "Epoch [13/20], Step [2400/10000], Loss: 0.6602\n",
            "Epoch [13/20], Step [2500/10000], Loss: 1.3917\n",
            "Epoch [13/20], Step [2600/10000], Loss: 0.2528\n",
            "Epoch [13/20], Step [2700/10000], Loss: 1.1833\n",
            "Epoch [13/20], Step [2800/10000], Loss: 0.9076\n",
            "Epoch [13/20], Step [2900/10000], Loss: 0.5556\n",
            "Epoch [13/20], Step [3000/10000], Loss: 0.7425\n",
            "Epoch [13/20], Step [3100/10000], Loss: 1.6346\n",
            "Epoch [13/20], Step [3200/10000], Loss: 0.8995\n",
            "Epoch [13/20], Step [3300/10000], Loss: 1.1156\n",
            "Epoch [13/20], Step [3400/10000], Loss: 0.0688\n",
            "Epoch [13/20], Step [3500/10000], Loss: 1.1765\n",
            "Epoch [13/20], Step [3600/10000], Loss: 0.5706\n",
            "Epoch [13/20], Step [3700/10000], Loss: 1.5158\n",
            "Epoch [13/20], Step [3800/10000], Loss: 1.1072\n",
            "Epoch [13/20], Step [3900/10000], Loss: 1.1000\n",
            "Epoch [13/20], Step [4000/10000], Loss: 0.7797\n",
            "Epoch [13/20], Step [4100/10000], Loss: 0.7340\n",
            "Epoch [13/20], Step [4200/10000], Loss: 0.2254\n",
            "Epoch [13/20], Step [4300/10000], Loss: 0.7874\n",
            "Epoch [13/20], Step [4400/10000], Loss: 0.3280\n",
            "Epoch [13/20], Step [4500/10000], Loss: 0.9730\n",
            "Epoch [13/20], Step [4600/10000], Loss: 0.2235\n",
            "Epoch [13/20], Step [4700/10000], Loss: 0.5401\n",
            "Epoch [13/20], Step [4800/10000], Loss: 1.8351\n",
            "Epoch [13/20], Step [4900/10000], Loss: 0.6152\n",
            "Epoch [13/20], Step [5000/10000], Loss: 0.3754\n",
            "Epoch [13/20], Step [5100/10000], Loss: 1.3545\n",
            "Epoch [13/20], Step [5200/10000], Loss: 0.7207\n",
            "Epoch [13/20], Step [5300/10000], Loss: 0.4218\n",
            "Epoch [13/20], Step [5400/10000], Loss: 0.9870\n",
            "Epoch [13/20], Step [5500/10000], Loss: 0.8617\n",
            "Epoch [13/20], Step [5600/10000], Loss: 1.2005\n",
            "Epoch [13/20], Step [5700/10000], Loss: 1.1902\n",
            "Epoch [13/20], Step [5800/10000], Loss: 0.7135\n",
            "Epoch [13/20], Step [5900/10000], Loss: 0.3270\n",
            "Epoch [13/20], Step [6000/10000], Loss: 1.0789\n",
            "Epoch [13/20], Step [6100/10000], Loss: 0.7441\n",
            "Epoch [13/20], Step [6200/10000], Loss: 1.5969\n",
            "Epoch [13/20], Step [6300/10000], Loss: 1.1440\n",
            "Epoch [13/20], Step [6400/10000], Loss: 1.3039\n",
            "Epoch [13/20], Step [6500/10000], Loss: 0.8716\n",
            "Epoch [13/20], Step [6600/10000], Loss: 0.6638\n",
            "Epoch [13/20], Step [6700/10000], Loss: 0.5246\n",
            "Epoch [13/20], Step [6800/10000], Loss: 0.4351\n",
            "Epoch [13/20], Step [6900/10000], Loss: 0.6291\n",
            "Epoch [13/20], Step [7000/10000], Loss: 0.8142\n",
            "Epoch [13/20], Step [7100/10000], Loss: 0.5193\n",
            "Epoch [13/20], Step [7200/10000], Loss: 1.2974\n",
            "Epoch [13/20], Step [7300/10000], Loss: 0.4810\n",
            "Epoch [13/20], Step [7400/10000], Loss: 0.3008\n",
            "Epoch [13/20], Step [7500/10000], Loss: 1.0418\n",
            "Epoch [13/20], Step [7600/10000], Loss: 0.8572\n",
            "Epoch [13/20], Step [7700/10000], Loss: 1.1094\n",
            "Epoch [13/20], Step [7800/10000], Loss: 0.3666\n",
            "Epoch [13/20], Step [7900/10000], Loss: 1.1179\n",
            "Epoch [13/20], Step [8000/10000], Loss: 0.4162\n",
            "Epoch [13/20], Step [8100/10000], Loss: 1.3278\n",
            "Epoch [13/20], Step [8200/10000], Loss: 1.1647\n",
            "Epoch [13/20], Step [8300/10000], Loss: 1.2084\n",
            "Epoch [13/20], Step [8400/10000], Loss: 0.7093\n",
            "Epoch [13/20], Step [8500/10000], Loss: 2.4699\n",
            "Epoch [13/20], Step [8600/10000], Loss: 0.5185\n",
            "Epoch [13/20], Step [8700/10000], Loss: 2.0958\n",
            "Epoch [13/20], Step [8800/10000], Loss: 1.7651\n",
            "Epoch [13/20], Step [8900/10000], Loss: 0.4962\n",
            "Epoch [13/20], Step [9000/10000], Loss: 1.0412\n",
            "Epoch [13/20], Step [9100/10000], Loss: 0.7906\n",
            "Epoch [13/20], Step [9200/10000], Loss: 1.0192\n",
            "Epoch [13/20], Step [9300/10000], Loss: 1.2931\n",
            "Epoch [13/20], Step [9400/10000], Loss: 0.5706\n",
            "Epoch [13/20], Step [9500/10000], Loss: 0.1805\n",
            "Epoch [13/20], Step [9600/10000], Loss: 1.2411\n",
            "Epoch [13/20], Step [9700/10000], Loss: 0.6754\n",
            "Epoch [13/20], Step [9800/10000], Loss: 0.7898\n",
            "Epoch [13/20], Step [9900/10000], Loss: 0.8665\n",
            "Epoch [13/20], Step [10000/10000], Loss: 0.9803\n",
            "Epoch [14/20], Step [100/10000], Loss: 0.4161\n",
            "Epoch [14/20], Step [200/10000], Loss: 0.6503\n",
            "Epoch [14/20], Step [300/10000], Loss: 0.2753\n",
            "Epoch [14/20], Step [400/10000], Loss: 0.2406\n",
            "Epoch [14/20], Step [500/10000], Loss: 2.2608\n",
            "Epoch [14/20], Step [600/10000], Loss: 2.6448\n",
            "Epoch [14/20], Step [700/10000], Loss: 1.1651\n",
            "Epoch [14/20], Step [800/10000], Loss: 0.6799\n",
            "Epoch [14/20], Step [900/10000], Loss: 0.8980\n",
            "Epoch [14/20], Step [1000/10000], Loss: 0.9596\n",
            "Epoch [14/20], Step [1100/10000], Loss: 0.3995\n",
            "Epoch [14/20], Step [1200/10000], Loss: 1.3528\n",
            "Epoch [14/20], Step [1300/10000], Loss: 0.1143\n",
            "Epoch [14/20], Step [1400/10000], Loss: 0.9064\n",
            "Epoch [14/20], Step [1500/10000], Loss: 0.4905\n",
            "Epoch [14/20], Step [1600/10000], Loss: 1.0067\n",
            "Epoch [14/20], Step [1700/10000], Loss: 1.0363\n",
            "Epoch [14/20], Step [1800/10000], Loss: 0.6613\n",
            "Epoch [14/20], Step [1900/10000], Loss: 1.2460\n",
            "Epoch [14/20], Step [2000/10000], Loss: 0.5775\n",
            "Epoch [14/20], Step [2100/10000], Loss: 0.6779\n",
            "Epoch [14/20], Step [2200/10000], Loss: 0.5969\n",
            "Epoch [14/20], Step [2300/10000], Loss: 0.5856\n",
            "Epoch [14/20], Step [2400/10000], Loss: 0.7474\n",
            "Epoch [14/20], Step [2500/10000], Loss: 0.0855\n",
            "Epoch [14/20], Step [2600/10000], Loss: 1.1147\n",
            "Epoch [14/20], Step [2700/10000], Loss: 0.5887\n",
            "Epoch [14/20], Step [2800/10000], Loss: 0.2900\n",
            "Epoch [14/20], Step [2900/10000], Loss: 1.2661\n",
            "Epoch [14/20], Step [3000/10000], Loss: 0.6884\n",
            "Epoch [14/20], Step [3100/10000], Loss: 0.9957\n",
            "Epoch [14/20], Step [3200/10000], Loss: 0.7469\n",
            "Epoch [14/20], Step [3300/10000], Loss: 0.8177\n",
            "Epoch [14/20], Step [3400/10000], Loss: 1.1503\n",
            "Epoch [14/20], Step [3500/10000], Loss: 0.4144\n",
            "Epoch [14/20], Step [3600/10000], Loss: 0.9924\n",
            "Epoch [14/20], Step [3700/10000], Loss: 1.7442\n",
            "Epoch [14/20], Step [3800/10000], Loss: 1.0842\n",
            "Epoch [14/20], Step [3900/10000], Loss: 0.1967\n",
            "Epoch [14/20], Step [4000/10000], Loss: 0.9146\n",
            "Epoch [14/20], Step [4100/10000], Loss: 1.0497\n",
            "Epoch [14/20], Step [4200/10000], Loss: 0.7153\n",
            "Epoch [14/20], Step [4300/10000], Loss: 0.7610\n",
            "Epoch [14/20], Step [4400/10000], Loss: 0.4132\n",
            "Epoch [14/20], Step [4500/10000], Loss: 0.4225\n",
            "Epoch [14/20], Step [4600/10000], Loss: 0.3050\n",
            "Epoch [14/20], Step [4700/10000], Loss: 0.3276\n",
            "Epoch [14/20], Step [4800/10000], Loss: 0.3686\n",
            "Epoch [14/20], Step [4900/10000], Loss: 0.3663\n",
            "Epoch [14/20], Step [5000/10000], Loss: 0.9038\n",
            "Epoch [14/20], Step [5100/10000], Loss: 0.4021\n",
            "Epoch [14/20], Step [5200/10000], Loss: 1.1179\n",
            "Epoch [14/20], Step [5300/10000], Loss: 0.3997\n",
            "Epoch [14/20], Step [5400/10000], Loss: 2.3593\n",
            "Epoch [14/20], Step [5500/10000], Loss: 0.9921\n",
            "Epoch [14/20], Step [5600/10000], Loss: 1.0037\n",
            "Epoch [14/20], Step [5700/10000], Loss: 0.4847\n",
            "Epoch [14/20], Step [5800/10000], Loss: 0.7910\n",
            "Epoch [14/20], Step [5900/10000], Loss: 0.9072\n",
            "Epoch [14/20], Step [6000/10000], Loss: 0.6616\n",
            "Epoch [14/20], Step [6100/10000], Loss: 1.4417\n",
            "Epoch [14/20], Step [6200/10000], Loss: 0.6021\n",
            "Epoch [14/20], Step [6300/10000], Loss: 1.1709\n",
            "Epoch [14/20], Step [6400/10000], Loss: 1.2175\n",
            "Epoch [14/20], Step [6500/10000], Loss: 1.3700\n",
            "Epoch [14/20], Step [6600/10000], Loss: 0.4453\n",
            "Epoch [14/20], Step [6700/10000], Loss: 0.9646\n",
            "Epoch [14/20], Step [6800/10000], Loss: 0.8721\n",
            "Epoch [14/20], Step [6900/10000], Loss: 0.5888\n",
            "Epoch [14/20], Step [7000/10000], Loss: 0.3266\n",
            "Epoch [14/20], Step [7100/10000], Loss: 0.9710\n",
            "Epoch [14/20], Step [7200/10000], Loss: 0.7471\n",
            "Epoch [14/20], Step [7300/10000], Loss: 0.0946\n",
            "Epoch [14/20], Step [7400/10000], Loss: 0.4228\n",
            "Epoch [14/20], Step [7500/10000], Loss: 1.0651\n",
            "Epoch [14/20], Step [7600/10000], Loss: 0.3592\n",
            "Epoch [14/20], Step [7700/10000], Loss: 0.3451\n",
            "Epoch [14/20], Step [7800/10000], Loss: 1.2597\n",
            "Epoch [14/20], Step [7900/10000], Loss: 0.6691\n",
            "Epoch [14/20], Step [8000/10000], Loss: 0.5311\n",
            "Epoch [14/20], Step [8100/10000], Loss: 1.2216\n",
            "Epoch [14/20], Step [8200/10000], Loss: 0.4597\n",
            "Epoch [14/20], Step [8300/10000], Loss: 1.2801\n",
            "Epoch [14/20], Step [8400/10000], Loss: 1.2491\n",
            "Epoch [14/20], Step [8500/10000], Loss: 1.1247\n",
            "Epoch [14/20], Step [8600/10000], Loss: 0.4473\n",
            "Epoch [14/20], Step [8700/10000], Loss: 0.4017\n",
            "Epoch [14/20], Step [8800/10000], Loss: 1.9309\n",
            "Epoch [14/20], Step [8900/10000], Loss: 0.5988\n",
            "Epoch [14/20], Step [9000/10000], Loss: 0.8748\n",
            "Epoch [14/20], Step [9100/10000], Loss: 0.4079\n",
            "Epoch [14/20], Step [9200/10000], Loss: 0.8726\n",
            "Epoch [14/20], Step [9300/10000], Loss: 0.5300\n",
            "Epoch [14/20], Step [9400/10000], Loss: 1.2709\n",
            "Epoch [14/20], Step [9500/10000], Loss: 1.2159\n",
            "Epoch [14/20], Step [9600/10000], Loss: 1.5236\n",
            "Epoch [14/20], Step [9700/10000], Loss: 1.2773\n",
            "Epoch [14/20], Step [9800/10000], Loss: 1.2840\n",
            "Epoch [14/20], Step [9900/10000], Loss: 0.1147\n",
            "Epoch [14/20], Step [10000/10000], Loss: 0.2576\n",
            "Epoch [15/20], Step [100/10000], Loss: 1.4214\n",
            "Epoch [15/20], Step [200/10000], Loss: 0.6912\n",
            "Epoch [15/20], Step [300/10000], Loss: 0.4026\n",
            "Epoch [15/20], Step [400/10000], Loss: 0.9420\n",
            "Epoch [15/20], Step [500/10000], Loss: 0.8394\n",
            "Epoch [15/20], Step [600/10000], Loss: 0.1524\n",
            "Epoch [15/20], Step [700/10000], Loss: 2.1733\n",
            "Epoch [15/20], Step [800/10000], Loss: 0.8179\n",
            "Epoch [15/20], Step [900/10000], Loss: 0.4202\n",
            "Epoch [15/20], Step [1000/10000], Loss: 0.1995\n",
            "Epoch [15/20], Step [1100/10000], Loss: 1.6316\n",
            "Epoch [15/20], Step [1200/10000], Loss: 0.3850\n",
            "Epoch [15/20], Step [1300/10000], Loss: 0.9640\n",
            "Epoch [15/20], Step [1400/10000], Loss: 0.6829\n",
            "Epoch [15/20], Step [1500/10000], Loss: 1.2945\n",
            "Epoch [15/20], Step [1600/10000], Loss: 0.8667\n",
            "Epoch [15/20], Step [1700/10000], Loss: 1.0895\n",
            "Epoch [15/20], Step [1800/10000], Loss: 1.3150\n",
            "Epoch [15/20], Step [1900/10000], Loss: 0.9334\n",
            "Epoch [15/20], Step [2000/10000], Loss: 0.8944\n",
            "Epoch [15/20], Step [2100/10000], Loss: 3.0579\n",
            "Epoch [15/20], Step [2200/10000], Loss: 0.1011\n",
            "Epoch [15/20], Step [2300/10000], Loss: 0.6225\n",
            "Epoch [15/20], Step [2400/10000], Loss: 1.4687\n",
            "Epoch [15/20], Step [2500/10000], Loss: 1.0120\n",
            "Epoch [15/20], Step [2600/10000], Loss: 0.8172\n",
            "Epoch [15/20], Step [2700/10000], Loss: 0.3742\n",
            "Epoch [15/20], Step [2800/10000], Loss: 0.9180\n",
            "Epoch [15/20], Step [2900/10000], Loss: 0.5472\n",
            "Epoch [15/20], Step [3000/10000], Loss: 0.9935\n",
            "Epoch [15/20], Step [3100/10000], Loss: 0.3657\n",
            "Epoch [15/20], Step [3200/10000], Loss: 1.7845\n",
            "Epoch [15/20], Step [3300/10000], Loss: 0.6337\n",
            "Epoch [15/20], Step [3400/10000], Loss: 1.1738\n",
            "Epoch [15/20], Step [3500/10000], Loss: 1.0471\n",
            "Epoch [15/20], Step [3600/10000], Loss: 1.1992\n",
            "Epoch [15/20], Step [3700/10000], Loss: 0.3872\n",
            "Epoch [15/20], Step [3800/10000], Loss: 1.0960\n",
            "Epoch [15/20], Step [3900/10000], Loss: 1.1094\n",
            "Epoch [15/20], Step [4000/10000], Loss: 0.5397\n",
            "Epoch [15/20], Step [4100/10000], Loss: 0.5146\n",
            "Epoch [15/20], Step [4200/10000], Loss: 1.6127\n",
            "Epoch [15/20], Step [4300/10000], Loss: 0.9715\n",
            "Epoch [15/20], Step [4400/10000], Loss: 0.5351\n",
            "Epoch [15/20], Step [4500/10000], Loss: 0.8708\n",
            "Epoch [15/20], Step [4600/10000], Loss: 2.1740\n",
            "Epoch [15/20], Step [4700/10000], Loss: 0.1361\n",
            "Epoch [15/20], Step [4800/10000], Loss: 1.8723\n",
            "Epoch [15/20], Step [4900/10000], Loss: 0.7245\n",
            "Epoch [15/20], Step [5000/10000], Loss: 0.7210\n",
            "Epoch [15/20], Step [5100/10000], Loss: 0.7220\n",
            "Epoch [15/20], Step [5200/10000], Loss: 0.2651\n",
            "Epoch [15/20], Step [5300/10000], Loss: 0.5563\n",
            "Epoch [15/20], Step [5400/10000], Loss: 1.1876\n",
            "Epoch [15/20], Step [5500/10000], Loss: 1.5933\n",
            "Epoch [15/20], Step [5600/10000], Loss: 0.9538\n",
            "Epoch [15/20], Step [5700/10000], Loss: 0.7708\n",
            "Epoch [15/20], Step [5800/10000], Loss: 0.7867\n",
            "Epoch [15/20], Step [5900/10000], Loss: 0.6820\n",
            "Epoch [15/20], Step [6000/10000], Loss: 0.2602\n",
            "Epoch [15/20], Step [6100/10000], Loss: 0.4066\n",
            "Epoch [15/20], Step [6200/10000], Loss: 1.0082\n",
            "Epoch [15/20], Step [6300/10000], Loss: 0.6244\n",
            "Epoch [15/20], Step [6400/10000], Loss: 0.2231\n",
            "Epoch [15/20], Step [6500/10000], Loss: 1.1759\n",
            "Epoch [15/20], Step [6600/10000], Loss: 0.3792\n",
            "Epoch [15/20], Step [6700/10000], Loss: 2.4571\n",
            "Epoch [15/20], Step [6800/10000], Loss: 0.7241\n",
            "Epoch [15/20], Step [6900/10000], Loss: 0.8713\n",
            "Epoch [15/20], Step [7000/10000], Loss: 1.5614\n",
            "Epoch [15/20], Step [7100/10000], Loss: 0.8291\n",
            "Epoch [15/20], Step [7200/10000], Loss: 1.8149\n",
            "Epoch [15/20], Step [7300/10000], Loss: 0.6466\n",
            "Epoch [15/20], Step [7400/10000], Loss: 0.5527\n",
            "Epoch [15/20], Step [7500/10000], Loss: 1.0377\n",
            "Epoch [15/20], Step [7600/10000], Loss: 0.6190\n",
            "Epoch [15/20], Step [7700/10000], Loss: 0.9720\n",
            "Epoch [15/20], Step [7800/10000], Loss: 0.4620\n",
            "Epoch [15/20], Step [7900/10000], Loss: 0.3845\n",
            "Epoch [15/20], Step [8000/10000], Loss: 0.8002\n",
            "Epoch [15/20], Step [8100/10000], Loss: 0.3717\n",
            "Epoch [15/20], Step [8200/10000], Loss: 1.4983\n",
            "Epoch [15/20], Step [8300/10000], Loss: 1.2342\n",
            "Epoch [15/20], Step [8400/10000], Loss: 0.8621\n",
            "Epoch [15/20], Step [8500/10000], Loss: 1.2552\n",
            "Epoch [15/20], Step [8600/10000], Loss: 1.0553\n",
            "Epoch [15/20], Step [8700/10000], Loss: 1.2975\n",
            "Epoch [15/20], Step [8800/10000], Loss: 2.0282\n",
            "Epoch [15/20], Step [8900/10000], Loss: 1.0653\n",
            "Epoch [15/20], Step [9000/10000], Loss: 0.9502\n",
            "Epoch [15/20], Step [9100/10000], Loss: 0.9331\n",
            "Epoch [15/20], Step [9200/10000], Loss: 0.7469\n",
            "Epoch [15/20], Step [9300/10000], Loss: 0.8872\n",
            "Epoch [15/20], Step [9400/10000], Loss: 0.5627\n",
            "Epoch [15/20], Step [9500/10000], Loss: 0.9074\n",
            "Epoch [15/20], Step [9600/10000], Loss: 1.1212\n",
            "Epoch [15/20], Step [9700/10000], Loss: 2.6462\n",
            "Epoch [15/20], Step [9800/10000], Loss: 0.2864\n",
            "Epoch [15/20], Step [9900/10000], Loss: 0.5075\n",
            "Epoch [15/20], Step [10000/10000], Loss: 0.9240\n",
            "Epoch [16/20], Step [100/10000], Loss: 0.6592\n",
            "Epoch [16/20], Step [200/10000], Loss: 0.4055\n",
            "Epoch [16/20], Step [300/10000], Loss: 0.3654\n",
            "Epoch [16/20], Step [400/10000], Loss: 0.7145\n",
            "Epoch [16/20], Step [500/10000], Loss: 0.4823\n",
            "Epoch [16/20], Step [600/10000], Loss: 1.1170\n",
            "Epoch [16/20], Step [700/10000], Loss: 0.7860\n",
            "Epoch [16/20], Step [800/10000], Loss: 1.4419\n",
            "Epoch [16/20], Step [900/10000], Loss: 1.0049\n",
            "Epoch [16/20], Step [1000/10000], Loss: 0.2988\n",
            "Epoch [16/20], Step [1100/10000], Loss: 0.9643\n",
            "Epoch [16/20], Step [1200/10000], Loss: 0.9450\n",
            "Epoch [16/20], Step [1300/10000], Loss: 1.0967\n",
            "Epoch [16/20], Step [1400/10000], Loss: 0.4212\n",
            "Epoch [16/20], Step [1500/10000], Loss: 1.2231\n",
            "Epoch [16/20], Step [1600/10000], Loss: 0.7420\n",
            "Epoch [16/20], Step [1700/10000], Loss: 2.7124\n",
            "Epoch [16/20], Step [1800/10000], Loss: 1.5677\n",
            "Epoch [16/20], Step [1900/10000], Loss: 1.0207\n",
            "Epoch [16/20], Step [2000/10000], Loss: 0.5095\n",
            "Epoch [16/20], Step [2100/10000], Loss: 1.0058\n",
            "Epoch [16/20], Step [2200/10000], Loss: 0.8826\n",
            "Epoch [16/20], Step [2300/10000], Loss: 0.4523\n",
            "Epoch [16/20], Step [2400/10000], Loss: 1.5820\n",
            "Epoch [16/20], Step [2500/10000], Loss: 0.6035\n",
            "Epoch [16/20], Step [2600/10000], Loss: 0.5035\n",
            "Epoch [16/20], Step [2700/10000], Loss: 1.2477\n",
            "Epoch [16/20], Step [2800/10000], Loss: 1.7458\n",
            "Epoch [16/20], Step [2900/10000], Loss: 1.3049\n",
            "Epoch [16/20], Step [3000/10000], Loss: 0.5984\n",
            "Epoch [16/20], Step [3100/10000], Loss: 2.6792\n",
            "Epoch [16/20], Step [3200/10000], Loss: 0.5252\n",
            "Epoch [16/20], Step [3300/10000], Loss: 0.4332\n",
            "Epoch [16/20], Step [3400/10000], Loss: 0.5702\n",
            "Epoch [16/20], Step [3500/10000], Loss: 1.8951\n",
            "Epoch [16/20], Step [3600/10000], Loss: 0.6191\n",
            "Epoch [16/20], Step [3700/10000], Loss: 0.5159\n",
            "Epoch [16/20], Step [3800/10000], Loss: 0.2914\n",
            "Epoch [16/20], Step [3900/10000], Loss: 0.6027\n",
            "Epoch [16/20], Step [4000/10000], Loss: 0.6742\n",
            "Epoch [16/20], Step [4100/10000], Loss: 0.3959\n",
            "Epoch [16/20], Step [4200/10000], Loss: 0.5898\n",
            "Epoch [16/20], Step [4300/10000], Loss: 0.8540\n",
            "Epoch [16/20], Step [4400/10000], Loss: 1.1112\n",
            "Epoch [16/20], Step [4500/10000], Loss: 1.1565\n",
            "Epoch [16/20], Step [4600/10000], Loss: 0.4672\n",
            "Epoch [16/20], Step [4700/10000], Loss: 1.2525\n",
            "Epoch [16/20], Step [4800/10000], Loss: 0.3067\n",
            "Epoch [16/20], Step [4900/10000], Loss: 0.9090\n",
            "Epoch [16/20], Step [5000/10000], Loss: 0.1960\n",
            "Epoch [16/20], Step [5100/10000], Loss: 0.5524\n",
            "Epoch [16/20], Step [5200/10000], Loss: 0.4898\n",
            "Epoch [16/20], Step [5300/10000], Loss: 0.1246\n",
            "Epoch [16/20], Step [5400/10000], Loss: 1.0461\n",
            "Epoch [16/20], Step [5500/10000], Loss: 0.2812\n",
            "Epoch [16/20], Step [5600/10000], Loss: 1.2493\n",
            "Epoch [16/20], Step [5700/10000], Loss: 0.8572\n",
            "Epoch [16/20], Step [5800/10000], Loss: 0.5468\n",
            "Epoch [16/20], Step [5900/10000], Loss: 0.0072\n",
            "Epoch [16/20], Step [6000/10000], Loss: 1.4942\n",
            "Epoch [16/20], Step [6100/10000], Loss: 1.3835\n",
            "Epoch [16/20], Step [6200/10000], Loss: 0.8697\n",
            "Epoch [16/20], Step [6300/10000], Loss: 2.3157\n",
            "Epoch [16/20], Step [6400/10000], Loss: 0.9940\n",
            "Epoch [16/20], Step [6500/10000], Loss: 0.7342\n",
            "Epoch [16/20], Step [6600/10000], Loss: 0.5288\n",
            "Epoch [16/20], Step [6700/10000], Loss: 1.7997\n",
            "Epoch [16/20], Step [6800/10000], Loss: 2.0162\n",
            "Epoch [16/20], Step [6900/10000], Loss: 0.4598\n",
            "Epoch [16/20], Step [7000/10000], Loss: 0.6314\n",
            "Epoch [16/20], Step [7100/10000], Loss: 1.4324\n",
            "Epoch [16/20], Step [7200/10000], Loss: 1.0644\n",
            "Epoch [16/20], Step [7300/10000], Loss: 0.5902\n",
            "Epoch [16/20], Step [7400/10000], Loss: 0.6154\n",
            "Epoch [16/20], Step [7500/10000], Loss: 2.0663\n",
            "Epoch [16/20], Step [7600/10000], Loss: 0.1951\n",
            "Epoch [16/20], Step [7700/10000], Loss: 0.6668\n",
            "Epoch [16/20], Step [7800/10000], Loss: 0.5011\n",
            "Epoch [16/20], Step [7900/10000], Loss: 0.1933\n",
            "Epoch [16/20], Step [8000/10000], Loss: 1.3658\n",
            "Epoch [16/20], Step [8100/10000], Loss: 0.8629\n",
            "Epoch [16/20], Step [8200/10000], Loss: 0.1039\n",
            "Epoch [16/20], Step [8300/10000], Loss: 0.8106\n",
            "Epoch [16/20], Step [8400/10000], Loss: 0.4622\n",
            "Epoch [16/20], Step [8500/10000], Loss: 0.9845\n",
            "Epoch [16/20], Step [8600/10000], Loss: 0.7358\n",
            "Epoch [16/20], Step [8700/10000], Loss: 0.1393\n",
            "Epoch [16/20], Step [8800/10000], Loss: 1.1638\n",
            "Epoch [16/20], Step [8900/10000], Loss: 0.2458\n",
            "Epoch [16/20], Step [9000/10000], Loss: 1.9241\n",
            "Epoch [16/20], Step [9100/10000], Loss: 1.1351\n",
            "Epoch [16/20], Step [9200/10000], Loss: 0.4586\n",
            "Epoch [16/20], Step [9300/10000], Loss: 1.2120\n",
            "Epoch [16/20], Step [9400/10000], Loss: 0.9935\n",
            "Epoch [16/20], Step [9500/10000], Loss: 1.4466\n",
            "Epoch [16/20], Step [9600/10000], Loss: 1.5353\n",
            "Epoch [16/20], Step [9700/10000], Loss: 1.1165\n",
            "Epoch [16/20], Step [9800/10000], Loss: 0.4195\n",
            "Epoch [16/20], Step [9900/10000], Loss: 0.0832\n",
            "Epoch [16/20], Step [10000/10000], Loss: 0.3177\n",
            "Epoch [17/20], Step [100/10000], Loss: 0.5787\n",
            "Epoch [17/20], Step [200/10000], Loss: 1.0259\n",
            "Epoch [17/20], Step [300/10000], Loss: 0.5042\n",
            "Epoch [17/20], Step [400/10000], Loss: 0.7610\n",
            "Epoch [17/20], Step [500/10000], Loss: 1.5482\n",
            "Epoch [17/20], Step [600/10000], Loss: 0.5078\n",
            "Epoch [17/20], Step [700/10000], Loss: 0.3614\n",
            "Epoch [17/20], Step [800/10000], Loss: 0.6124\n",
            "Epoch [17/20], Step [900/10000], Loss: 0.8163\n",
            "Epoch [17/20], Step [1000/10000], Loss: 0.4969\n",
            "Epoch [17/20], Step [1100/10000], Loss: 1.1184\n",
            "Epoch [17/20], Step [1200/10000], Loss: 1.1653\n",
            "Epoch [17/20], Step [1300/10000], Loss: 1.1621\n",
            "Epoch [17/20], Step [1400/10000], Loss: 1.2499\n",
            "Epoch [17/20], Step [1500/10000], Loss: 0.6945\n",
            "Epoch [17/20], Step [1600/10000], Loss: 1.6905\n",
            "Epoch [17/20], Step [1700/10000], Loss: 0.4435\n",
            "Epoch [17/20], Step [1800/10000], Loss: 0.4812\n",
            "Epoch [17/20], Step [1900/10000], Loss: 0.4235\n",
            "Epoch [17/20], Step [2000/10000], Loss: 0.5792\n",
            "Epoch [17/20], Step [2100/10000], Loss: 0.1953\n",
            "Epoch [17/20], Step [2200/10000], Loss: 0.9864\n",
            "Epoch [17/20], Step [2300/10000], Loss: 0.7917\n",
            "Epoch [17/20], Step [2400/10000], Loss: 0.6174\n",
            "Epoch [17/20], Step [2500/10000], Loss: 0.7458\n",
            "Epoch [17/20], Step [2600/10000], Loss: 0.8344\n",
            "Epoch [17/20], Step [2700/10000], Loss: 0.3121\n",
            "Epoch [17/20], Step [2800/10000], Loss: 0.3435\n",
            "Epoch [17/20], Step [2900/10000], Loss: 0.5092\n",
            "Epoch [17/20], Step [3000/10000], Loss: 0.8995\n",
            "Epoch [17/20], Step [3100/10000], Loss: 1.0984\n",
            "Epoch [17/20], Step [3200/10000], Loss: 0.4181\n",
            "Epoch [17/20], Step [3300/10000], Loss: 0.5074\n",
            "Epoch [17/20], Step [3400/10000], Loss: 1.4259\n",
            "Epoch [17/20], Step [3500/10000], Loss: 0.9593\n",
            "Epoch [17/20], Step [3600/10000], Loss: 0.2463\n",
            "Epoch [17/20], Step [3700/10000], Loss: 0.1980\n",
            "Epoch [17/20], Step [3800/10000], Loss: 0.7583\n",
            "Epoch [17/20], Step [3900/10000], Loss: 0.1313\n",
            "Epoch [17/20], Step [4000/10000], Loss: 0.1314\n",
            "Epoch [17/20], Step [4100/10000], Loss: 0.9379\n",
            "Epoch [17/20], Step [4200/10000], Loss: 0.7718\n",
            "Epoch [17/20], Step [4300/10000], Loss: 0.7139\n",
            "Epoch [17/20], Step [4400/10000], Loss: 0.5413\n",
            "Epoch [17/20], Step [4500/10000], Loss: 0.9658\n",
            "Epoch [17/20], Step [4600/10000], Loss: 0.5332\n",
            "Epoch [17/20], Step [4700/10000], Loss: 0.9111\n",
            "Epoch [17/20], Step [4800/10000], Loss: 0.5812\n",
            "Epoch [17/20], Step [4900/10000], Loss: 0.7325\n",
            "Epoch [17/20], Step [5000/10000], Loss: 0.7404\n",
            "Epoch [17/20], Step [5100/10000], Loss: 1.6409\n",
            "Epoch [17/20], Step [5200/10000], Loss: 0.1788\n",
            "Epoch [17/20], Step [5300/10000], Loss: 1.5050\n",
            "Epoch [17/20], Step [5400/10000], Loss: 0.6355\n",
            "Epoch [17/20], Step [5500/10000], Loss: 1.1752\n",
            "Epoch [17/20], Step [5600/10000], Loss: 0.4115\n",
            "Epoch [17/20], Step [5700/10000], Loss: 0.8451\n",
            "Epoch [17/20], Step [5800/10000], Loss: 1.1176\n",
            "Epoch [17/20], Step [5900/10000], Loss: 0.5441\n",
            "Epoch [17/20], Step [6000/10000], Loss: 0.6748\n",
            "Epoch [17/20], Step [6100/10000], Loss: 1.5710\n",
            "Epoch [17/20], Step [6200/10000], Loss: 1.5445\n",
            "Epoch [17/20], Step [6300/10000], Loss: 1.7639\n",
            "Epoch [17/20], Step [6400/10000], Loss: 0.6676\n",
            "Epoch [17/20], Step [6500/10000], Loss: 0.4546\n",
            "Epoch [17/20], Step [6600/10000], Loss: 0.8455\n",
            "Epoch [17/20], Step [6700/10000], Loss: 0.2919\n",
            "Epoch [17/20], Step [6800/10000], Loss: 0.0646\n",
            "Epoch [17/20], Step [6900/10000], Loss: 1.2753\n",
            "Epoch [17/20], Step [7000/10000], Loss: 1.4211\n",
            "Epoch [17/20], Step [7100/10000], Loss: 0.4121\n",
            "Epoch [17/20], Step [7200/10000], Loss: 1.2187\n",
            "Epoch [17/20], Step [7300/10000], Loss: 1.7689\n",
            "Epoch [17/20], Step [7400/10000], Loss: 0.2142\n",
            "Epoch [17/20], Step [7500/10000], Loss: 0.4556\n",
            "Epoch [17/20], Step [7600/10000], Loss: 0.5085\n",
            "Epoch [17/20], Step [7700/10000], Loss: 1.1586\n",
            "Epoch [17/20], Step [7800/10000], Loss: 0.9101\n",
            "Epoch [17/20], Step [7900/10000], Loss: 1.4831\n",
            "Epoch [17/20], Step [8000/10000], Loss: 1.6932\n",
            "Epoch [17/20], Step [8100/10000], Loss: 3.2007\n",
            "Epoch [17/20], Step [8200/10000], Loss: 0.3152\n",
            "Epoch [17/20], Step [8300/10000], Loss: 1.5184\n",
            "Epoch [17/20], Step [8400/10000], Loss: 0.3064\n",
            "Epoch [17/20], Step [8500/10000], Loss: 1.0219\n",
            "Epoch [17/20], Step [8600/10000], Loss: 1.2384\n",
            "Epoch [17/20], Step [8700/10000], Loss: 1.2867\n",
            "Epoch [17/20], Step [8800/10000], Loss: 0.7568\n",
            "Epoch [17/20], Step [8900/10000], Loss: 0.7003\n",
            "Epoch [17/20], Step [9000/10000], Loss: 0.6025\n",
            "Epoch [17/20], Step [9100/10000], Loss: 0.5834\n",
            "Epoch [17/20], Step [9200/10000], Loss: 0.7057\n",
            "Epoch [17/20], Step [9300/10000], Loss: 0.2689\n",
            "Epoch [17/20], Step [9400/10000], Loss: 0.3781\n",
            "Epoch [17/20], Step [9500/10000], Loss: 0.2997\n",
            "Epoch [17/20], Step [9600/10000], Loss: 0.1591\n",
            "Epoch [17/20], Step [9700/10000], Loss: 0.3953\n",
            "Epoch [17/20], Step [9800/10000], Loss: 1.0063\n",
            "Epoch [17/20], Step [9900/10000], Loss: 1.3868\n",
            "Epoch [17/20], Step [10000/10000], Loss: 1.0663\n",
            "Epoch [18/20], Step [100/10000], Loss: 0.3635\n",
            "Epoch [18/20], Step [200/10000], Loss: 0.7935\n",
            "Epoch [18/20], Step [300/10000], Loss: 0.1374\n",
            "Epoch [18/20], Step [400/10000], Loss: 0.9545\n",
            "Epoch [18/20], Step [500/10000], Loss: 0.1708\n",
            "Epoch [18/20], Step [600/10000], Loss: 0.4119\n",
            "Epoch [18/20], Step [700/10000], Loss: 0.0680\n",
            "Epoch [18/20], Step [800/10000], Loss: 0.3245\n",
            "Epoch [18/20], Step [900/10000], Loss: 0.8925\n",
            "Epoch [18/20], Step [1000/10000], Loss: 0.3537\n",
            "Epoch [18/20], Step [1100/10000], Loss: 0.6867\n",
            "Epoch [18/20], Step [1200/10000], Loss: 0.6422\n",
            "Epoch [18/20], Step [1300/10000], Loss: 0.8174\n",
            "Epoch [18/20], Step [1400/10000], Loss: 0.7131\n",
            "Epoch [18/20], Step [1500/10000], Loss: 0.3525\n",
            "Epoch [18/20], Step [1600/10000], Loss: 1.8537\n",
            "Epoch [18/20], Step [1700/10000], Loss: 0.7059\n",
            "Epoch [18/20], Step [1800/10000], Loss: 0.9340\n",
            "Epoch [18/20], Step [1900/10000], Loss: 0.4049\n",
            "Epoch [18/20], Step [2000/10000], Loss: 0.4955\n",
            "Epoch [18/20], Step [2100/10000], Loss: 0.4345\n",
            "Epoch [18/20], Step [2200/10000], Loss: 0.2239\n",
            "Epoch [18/20], Step [2300/10000], Loss: 1.5285\n",
            "Epoch [18/20], Step [2400/10000], Loss: 0.6269\n",
            "Epoch [18/20], Step [2500/10000], Loss: 0.5641\n",
            "Epoch [18/20], Step [2600/10000], Loss: 0.6694\n",
            "Epoch [18/20], Step [2700/10000], Loss: 0.5368\n",
            "Epoch [18/20], Step [2800/10000], Loss: 0.9093\n",
            "Epoch [18/20], Step [2900/10000], Loss: 0.9332\n",
            "Epoch [18/20], Step [3000/10000], Loss: 0.6203\n",
            "Epoch [18/20], Step [3100/10000], Loss: 0.9919\n",
            "Epoch [18/20], Step [3200/10000], Loss: 1.6947\n",
            "Epoch [18/20], Step [3300/10000], Loss: 1.0341\n",
            "Epoch [18/20], Step [3400/10000], Loss: 1.5109\n",
            "Epoch [18/20], Step [3500/10000], Loss: 1.1520\n",
            "Epoch [18/20], Step [3600/10000], Loss: 0.5012\n",
            "Epoch [18/20], Step [3700/10000], Loss: 1.3843\n",
            "Epoch [18/20], Step [3800/10000], Loss: 0.6713\n",
            "Epoch [18/20], Step [3900/10000], Loss: 1.2321\n",
            "Epoch [18/20], Step [4000/10000], Loss: 0.2167\n",
            "Epoch [18/20], Step [4100/10000], Loss: 0.4043\n",
            "Epoch [18/20], Step [4200/10000], Loss: 0.9116\n",
            "Epoch [18/20], Step [4300/10000], Loss: 0.8104\n",
            "Epoch [18/20], Step [4400/10000], Loss: 0.4936\n",
            "Epoch [18/20], Step [4500/10000], Loss: 0.5470\n",
            "Epoch [18/20], Step [4600/10000], Loss: 1.7495\n",
            "Epoch [18/20], Step [4700/10000], Loss: 0.0381\n",
            "Epoch [18/20], Step [4800/10000], Loss: 1.5048\n",
            "Epoch [18/20], Step [4900/10000], Loss: 0.4048\n",
            "Epoch [18/20], Step [5000/10000], Loss: 0.3137\n",
            "Epoch [18/20], Step [5100/10000], Loss: 0.7916\n",
            "Epoch [18/20], Step [5200/10000], Loss: 1.0277\n",
            "Epoch [18/20], Step [5300/10000], Loss: 1.7248\n",
            "Epoch [18/20], Step [5400/10000], Loss: 1.1104\n",
            "Epoch [18/20], Step [5500/10000], Loss: 0.3199\n",
            "Epoch [18/20], Step [5600/10000], Loss: 0.8627\n",
            "Epoch [18/20], Step [5700/10000], Loss: 0.9413\n",
            "Epoch [18/20], Step [5800/10000], Loss: 1.6581\n",
            "Epoch [18/20], Step [5900/10000], Loss: 1.2242\n",
            "Epoch [18/20], Step [6000/10000], Loss: 1.2733\n",
            "Epoch [18/20], Step [6100/10000], Loss: 1.2211\n",
            "Epoch [18/20], Step [6200/10000], Loss: 1.1350\n",
            "Epoch [18/20], Step [6300/10000], Loss: 0.7363\n",
            "Epoch [18/20], Step [6400/10000], Loss: 0.5633\n",
            "Epoch [18/20], Step [6500/10000], Loss: 0.1962\n",
            "Epoch [18/20], Step [6600/10000], Loss: 0.8654\n",
            "Epoch [18/20], Step [6700/10000], Loss: 1.4533\n",
            "Epoch [18/20], Step [6800/10000], Loss: 1.6031\n",
            "Epoch [18/20], Step [6900/10000], Loss: 0.6205\n",
            "Epoch [18/20], Step [7000/10000], Loss: 1.3772\n",
            "Epoch [18/20], Step [7100/10000], Loss: 1.4989\n",
            "Epoch [18/20], Step [7200/10000], Loss: 0.5520\n",
            "Epoch [18/20], Step [7300/10000], Loss: 2.0424\n",
            "Epoch [18/20], Step [7400/10000], Loss: 0.5316\n",
            "Epoch [18/20], Step [7500/10000], Loss: 0.6639\n",
            "Epoch [18/20], Step [7600/10000], Loss: 1.5634\n",
            "Epoch [18/20], Step [7700/10000], Loss: 0.6207\n",
            "Epoch [18/20], Step [7800/10000], Loss: 0.8039\n",
            "Epoch [18/20], Step [7900/10000], Loss: 0.3870\n",
            "Epoch [18/20], Step [8000/10000], Loss: 0.4621\n",
            "Epoch [18/20], Step [8100/10000], Loss: 0.8178\n",
            "Epoch [18/20], Step [8200/10000], Loss: 0.2964\n",
            "Epoch [18/20], Step [8300/10000], Loss: 0.8132\n",
            "Epoch [18/20], Step [8400/10000], Loss: 1.0537\n",
            "Epoch [18/20], Step [8500/10000], Loss: 0.6796\n",
            "Epoch [18/20], Step [8600/10000], Loss: 0.5844\n",
            "Epoch [18/20], Step [8700/10000], Loss: 1.3378\n",
            "Epoch [18/20], Step [8800/10000], Loss: 1.4530\n",
            "Epoch [18/20], Step [8900/10000], Loss: 0.9951\n",
            "Epoch [18/20], Step [9000/10000], Loss: 0.4743\n",
            "Epoch [18/20], Step [9100/10000], Loss: 1.7445\n",
            "Epoch [18/20], Step [9200/10000], Loss: 0.6193\n",
            "Epoch [18/20], Step [9300/10000], Loss: 0.1164\n",
            "Epoch [18/20], Step [9400/10000], Loss: 0.6842\n",
            "Epoch [18/20], Step [9500/10000], Loss: 0.6562\n",
            "Epoch [18/20], Step [9600/10000], Loss: 1.1538\n",
            "Epoch [18/20], Step [9700/10000], Loss: 0.9273\n",
            "Epoch [18/20], Step [9800/10000], Loss: 1.1792\n",
            "Epoch [18/20], Step [9900/10000], Loss: 0.1148\n",
            "Epoch [18/20], Step [10000/10000], Loss: 0.3845\n",
            "Epoch [19/20], Step [100/10000], Loss: 0.6619\n",
            "Epoch [19/20], Step [200/10000], Loss: 0.8549\n",
            "Epoch [19/20], Step [300/10000], Loss: 1.0523\n",
            "Epoch [19/20], Step [400/10000], Loss: 0.6513\n",
            "Epoch [19/20], Step [500/10000], Loss: 0.6370\n",
            "Epoch [19/20], Step [600/10000], Loss: 0.3112\n",
            "Epoch [19/20], Step [700/10000], Loss: 1.3889\n",
            "Epoch [19/20], Step [800/10000], Loss: 0.5198\n",
            "Epoch [19/20], Step [900/10000], Loss: 0.2308\n",
            "Epoch [19/20], Step [1000/10000], Loss: 1.0506\n",
            "Epoch [19/20], Step [1100/10000], Loss: 0.4042\n",
            "Epoch [19/20], Step [1200/10000], Loss: 0.3937\n",
            "Epoch [19/20], Step [1300/10000], Loss: 0.6637\n",
            "Epoch [19/20], Step [1400/10000], Loss: 0.3243\n",
            "Epoch [19/20], Step [1500/10000], Loss: 0.3088\n",
            "Epoch [19/20], Step [1600/10000], Loss: 0.6453\n",
            "Epoch [19/20], Step [1700/10000], Loss: 2.1022\n",
            "Epoch [19/20], Step [1800/10000], Loss: 0.1090\n",
            "Epoch [19/20], Step [1900/10000], Loss: 0.3341\n",
            "Epoch [19/20], Step [2000/10000], Loss: 0.2803\n",
            "Epoch [19/20], Step [2100/10000], Loss: 0.4502\n",
            "Epoch [19/20], Step [2200/10000], Loss: 0.8019\n",
            "Epoch [19/20], Step [2300/10000], Loss: 0.7430\n",
            "Epoch [19/20], Step [2400/10000], Loss: 0.4590\n",
            "Epoch [19/20], Step [2500/10000], Loss: 0.7873\n",
            "Epoch [19/20], Step [2600/10000], Loss: 0.7203\n",
            "Epoch [19/20], Step [2700/10000], Loss: 1.9065\n",
            "Epoch [19/20], Step [2800/10000], Loss: 0.8653\n",
            "Epoch [19/20], Step [2900/10000], Loss: 0.8900\n",
            "Epoch [19/20], Step [3000/10000], Loss: 0.6790\n",
            "Epoch [19/20], Step [3100/10000], Loss: 0.7891\n",
            "Epoch [19/20], Step [3200/10000], Loss: 0.6702\n",
            "Epoch [19/20], Step [3300/10000], Loss: 0.4949\n",
            "Epoch [19/20], Step [3400/10000], Loss: 0.4807\n",
            "Epoch [19/20], Step [3500/10000], Loss: 1.8296\n",
            "Epoch [19/20], Step [3600/10000], Loss: 1.0250\n",
            "Epoch [19/20], Step [3700/10000], Loss: 1.2822\n",
            "Epoch [19/20], Step [3800/10000], Loss: 0.6461\n",
            "Epoch [19/20], Step [3900/10000], Loss: 1.1093\n",
            "Epoch [19/20], Step [4000/10000], Loss: 1.1812\n",
            "Epoch [19/20], Step [4100/10000], Loss: 0.5597\n",
            "Epoch [19/20], Step [4200/10000], Loss: 1.4162\n",
            "Epoch [19/20], Step [4300/10000], Loss: 0.5166\n",
            "Epoch [19/20], Step [4400/10000], Loss: 1.5332\n",
            "Epoch [19/20], Step [4500/10000], Loss: 0.5166\n",
            "Epoch [19/20], Step [4600/10000], Loss: 0.5241\n",
            "Epoch [19/20], Step [4700/10000], Loss: 0.7020\n",
            "Epoch [19/20], Step [4800/10000], Loss: 0.3833\n",
            "Epoch [19/20], Step [4900/10000], Loss: 1.0307\n",
            "Epoch [19/20], Step [5000/10000], Loss: 0.2357\n",
            "Epoch [19/20], Step [5100/10000], Loss: 1.1903\n",
            "Epoch [19/20], Step [5200/10000], Loss: 0.3818\n",
            "Epoch [19/20], Step [5300/10000], Loss: 0.6424\n",
            "Epoch [19/20], Step [5400/10000], Loss: 1.0527\n",
            "Epoch [19/20], Step [5500/10000], Loss: 1.3985\n",
            "Epoch [19/20], Step [5600/10000], Loss: 0.3040\n",
            "Epoch [19/20], Step [5700/10000], Loss: 0.4799\n",
            "Epoch [19/20], Step [5800/10000], Loss: 0.5316\n",
            "Epoch [19/20], Step [5900/10000], Loss: 0.8663\n",
            "Epoch [19/20], Step [6000/10000], Loss: 0.9414\n",
            "Epoch [19/20], Step [6100/10000], Loss: 2.0770\n",
            "Epoch [19/20], Step [6200/10000], Loss: 0.8829\n",
            "Epoch [19/20], Step [6300/10000], Loss: 0.7633\n",
            "Epoch [19/20], Step [6400/10000], Loss: 1.0596\n",
            "Epoch [19/20], Step [6500/10000], Loss: 1.6113\n",
            "Epoch [19/20], Step [6600/10000], Loss: 0.4488\n",
            "Epoch [19/20], Step [6700/10000], Loss: 1.1406\n",
            "Epoch [19/20], Step [6800/10000], Loss: 0.7573\n",
            "Epoch [19/20], Step [6900/10000], Loss: 0.4958\n",
            "Epoch [19/20], Step [7000/10000], Loss: 0.6541\n",
            "Epoch [19/20], Step [7100/10000], Loss: 1.2914\n",
            "Epoch [19/20], Step [7200/10000], Loss: 0.9186\n",
            "Epoch [19/20], Step [7300/10000], Loss: 0.9006\n",
            "Epoch [19/20], Step [7400/10000], Loss: 1.1725\n",
            "Epoch [19/20], Step [7500/10000], Loss: 0.6819\n",
            "Epoch [19/20], Step [7600/10000], Loss: 0.9084\n",
            "Epoch [19/20], Step [7700/10000], Loss: 0.8543\n",
            "Epoch [19/20], Step [7800/10000], Loss: 0.6707\n",
            "Epoch [19/20], Step [7900/10000], Loss: 1.8405\n",
            "Epoch [19/20], Step [8000/10000], Loss: 0.5416\n",
            "Epoch [19/20], Step [8100/10000], Loss: 1.2272\n",
            "Epoch [19/20], Step [8200/10000], Loss: 1.7120\n",
            "Epoch [19/20], Step [8300/10000], Loss: 0.7976\n",
            "Epoch [19/20], Step [8400/10000], Loss: 1.1131\n",
            "Epoch [19/20], Step [8500/10000], Loss: 0.7155\n",
            "Epoch [19/20], Step [8600/10000], Loss: 0.2036\n",
            "Epoch [19/20], Step [8700/10000], Loss: 0.5245\n",
            "Epoch [19/20], Step [8800/10000], Loss: 0.6874\n",
            "Epoch [19/20], Step [8900/10000], Loss: 0.6707\n",
            "Epoch [19/20], Step [9000/10000], Loss: 0.4926\n",
            "Epoch [19/20], Step [9100/10000], Loss: 0.7362\n",
            "Epoch [19/20], Step [9200/10000], Loss: 0.4432\n",
            "Epoch [19/20], Step [9300/10000], Loss: 0.6804\n",
            "Epoch [19/20], Step [9400/10000], Loss: 0.8785\n",
            "Epoch [19/20], Step [9500/10000], Loss: 0.8134\n",
            "Epoch [19/20], Step [9600/10000], Loss: 1.1079\n",
            "Epoch [19/20], Step [9700/10000], Loss: 0.6633\n",
            "Epoch [19/20], Step [9800/10000], Loss: 0.7748\n",
            "Epoch [19/20], Step [9900/10000], Loss: 1.9640\n",
            "Epoch [19/20], Step [10000/10000], Loss: 0.0675\n",
            "Epoch [20/20], Step [100/10000], Loss: 0.1397\n",
            "Epoch [20/20], Step [200/10000], Loss: 1.3882\n",
            "Epoch [20/20], Step [300/10000], Loss: 0.8784\n",
            "Epoch [20/20], Step [400/10000], Loss: 0.7520\n",
            "Epoch [20/20], Step [500/10000], Loss: 0.4608\n",
            "Epoch [20/20], Step [600/10000], Loss: 0.6169\n",
            "Epoch [20/20], Step [700/10000], Loss: 0.6787\n",
            "Epoch [20/20], Step [800/10000], Loss: 0.0833\n",
            "Epoch [20/20], Step [900/10000], Loss: 0.8808\n",
            "Epoch [20/20], Step [1000/10000], Loss: 0.5361\n",
            "Epoch [20/20], Step [1100/10000], Loss: 0.5785\n",
            "Epoch [20/20], Step [1200/10000], Loss: 0.3699\n",
            "Epoch [20/20], Step [1300/10000], Loss: 0.9148\n",
            "Epoch [20/20], Step [1400/10000], Loss: 0.7803\n",
            "Epoch [20/20], Step [1500/10000], Loss: 0.8959\n",
            "Epoch [20/20], Step [1600/10000], Loss: 0.4127\n",
            "Epoch [20/20], Step [1700/10000], Loss: 0.6218\n",
            "Epoch [20/20], Step [1800/10000], Loss: 0.8758\n",
            "Epoch [20/20], Step [1900/10000], Loss: 1.3805\n",
            "Epoch [20/20], Step [2000/10000], Loss: 0.3081\n",
            "Epoch [20/20], Step [2100/10000], Loss: 1.9853\n",
            "Epoch [20/20], Step [2200/10000], Loss: 1.3670\n",
            "Epoch [20/20], Step [2300/10000], Loss: 0.6241\n",
            "Epoch [20/20], Step [2400/10000], Loss: 0.6856\n",
            "Epoch [20/20], Step [2500/10000], Loss: 0.6376\n",
            "Epoch [20/20], Step [2600/10000], Loss: 0.2899\n",
            "Epoch [20/20], Step [2700/10000], Loss: 1.0933\n",
            "Epoch [20/20], Step [2800/10000], Loss: 0.2237\n",
            "Epoch [20/20], Step [2900/10000], Loss: 0.3361\n",
            "Epoch [20/20], Step [3000/10000], Loss: 0.6950\n",
            "Epoch [20/20], Step [3100/10000], Loss: 0.3548\n",
            "Epoch [20/20], Step [3200/10000], Loss: 0.3557\n",
            "Epoch [20/20], Step [3300/10000], Loss: 0.7316\n",
            "Epoch [20/20], Step [3400/10000], Loss: 0.5514\n",
            "Epoch [20/20], Step [3500/10000], Loss: 2.1628\n",
            "Epoch [20/20], Step [3600/10000], Loss: 0.3747\n",
            "Epoch [20/20], Step [3700/10000], Loss: 1.0140\n",
            "Epoch [20/20], Step [3800/10000], Loss: 0.2525\n",
            "Epoch [20/20], Step [3900/10000], Loss: 1.0420\n",
            "Epoch [20/20], Step [4000/10000], Loss: 1.0189\n",
            "Epoch [20/20], Step [4100/10000], Loss: 1.6225\n",
            "Epoch [20/20], Step [4200/10000], Loss: 0.3149\n",
            "Epoch [20/20], Step [4300/10000], Loss: 1.4042\n",
            "Epoch [20/20], Step [4400/10000], Loss: 1.3147\n",
            "Epoch [20/20], Step [4500/10000], Loss: 1.1536\n",
            "Epoch [20/20], Step [4600/10000], Loss: 1.7786\n",
            "Epoch [20/20], Step [4700/10000], Loss: 2.1457\n",
            "Epoch [20/20], Step [4800/10000], Loss: 1.2553\n",
            "Epoch [20/20], Step [4900/10000], Loss: 1.1716\n",
            "Epoch [20/20], Step [5000/10000], Loss: 1.0965\n",
            "Epoch [20/20], Step [5100/10000], Loss: 1.2576\n",
            "Epoch [20/20], Step [5200/10000], Loss: 1.5912\n",
            "Epoch [20/20], Step [5300/10000], Loss: 1.2461\n",
            "Epoch [20/20], Step [5400/10000], Loss: 1.3020\n",
            "Epoch [20/20], Step [5500/10000], Loss: 0.6692\n",
            "Epoch [20/20], Step [5600/10000], Loss: 0.5605\n",
            "Epoch [20/20], Step [5700/10000], Loss: 0.0828\n",
            "Epoch [20/20], Step [5800/10000], Loss: 0.0881\n",
            "Epoch [20/20], Step [5900/10000], Loss: 1.3680\n",
            "Epoch [20/20], Step [6000/10000], Loss: 0.7409\n",
            "Epoch [20/20], Step [6100/10000], Loss: 1.3824\n",
            "Epoch [20/20], Step [6200/10000], Loss: 1.6497\n",
            "Epoch [20/20], Step [6300/10000], Loss: 0.4630\n",
            "Epoch [20/20], Step [6400/10000], Loss: 0.4007\n",
            "Epoch [20/20], Step [6500/10000], Loss: 0.6518\n",
            "Epoch [20/20], Step [6600/10000], Loss: 0.2487\n",
            "Epoch [20/20], Step [6700/10000], Loss: 1.0285\n",
            "Epoch [20/20], Step [6800/10000], Loss: 0.5692\n",
            "Epoch [20/20], Step [6900/10000], Loss: 0.4589\n",
            "Epoch [20/20], Step [7000/10000], Loss: 0.6916\n",
            "Epoch [20/20], Step [7100/10000], Loss: 0.2383\n",
            "Epoch [20/20], Step [7200/10000], Loss: 1.3191\n",
            "Epoch [20/20], Step [7300/10000], Loss: 0.9645\n",
            "Epoch [20/20], Step [7400/10000], Loss: 1.0216\n",
            "Epoch [20/20], Step [7500/10000], Loss: 1.1902\n",
            "Epoch [20/20], Step [7600/10000], Loss: 0.3382\n",
            "Epoch [20/20], Step [7700/10000], Loss: 0.2851\n",
            "Epoch [20/20], Step [7800/10000], Loss: 0.6606\n",
            "Epoch [20/20], Step [7900/10000], Loss: 0.4798\n",
            "Epoch [20/20], Step [8000/10000], Loss: 0.1390\n",
            "Epoch [20/20], Step [8100/10000], Loss: 0.3480\n",
            "Epoch [20/20], Step [8200/10000], Loss: 0.4601\n",
            "Epoch [20/20], Step [8300/10000], Loss: 0.4131\n",
            "Epoch [20/20], Step [8400/10000], Loss: 0.9208\n",
            "Epoch [20/20], Step [8500/10000], Loss: 0.0470\n",
            "Epoch [20/20], Step [8600/10000], Loss: 0.6067\n",
            "Epoch [20/20], Step [8700/10000], Loss: 0.4303\n",
            "Epoch [20/20], Step [8800/10000], Loss: 0.5770\n",
            "Epoch [20/20], Step [8900/10000], Loss: 1.3756\n",
            "Epoch [20/20], Step [9000/10000], Loss: 1.0299\n",
            "Epoch [20/20], Step [9100/10000], Loss: 0.3902\n",
            "Epoch [20/20], Step [9200/10000], Loss: 0.8423\n",
            "Epoch [20/20], Step [9300/10000], Loss: 0.3313\n",
            "Epoch [20/20], Step [9400/10000], Loss: 0.3674\n",
            "Epoch [20/20], Step [9500/10000], Loss: 0.2825\n",
            "Epoch [20/20], Step [9600/10000], Loss: 0.9509\n",
            "Epoch [20/20], Step [9700/10000], Loss: 0.8769\n",
            "Epoch [20/20], Step [9800/10000], Loss: 0.9027\n",
            "Epoch [20/20], Step [9900/10000], Loss: 0.7975\n",
            "Epoch [20/20], Step [10000/10000], Loss: 1.3124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置为评估模式\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqgepKuunHH2",
        "outputId": "d869f758-2fdc-469d-feef-6c82d4fdf13c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvNet(\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv2d(3, 5, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv2d(5, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv3): Sequential(\n",
              "    (0): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv4): Sequential(\n",
              "    (0): Conv2d(16, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv5): Sequential(\n",
              "    (0): Conv2d(24, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=1568, out_features=400, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=400, out_features=120, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=120, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 输出测试集精度\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU9l2rwJnRgT",
        "outputId": "6082c613-ad86-4a98-a6d2-a04d9a948784"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the model on the test images: 74.69 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  保存模型\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "metadata": {
        "id": "f1oRlNU4ndmP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 可视化数据查看\n",
        "# 查看数据,取一组batch\n",
        "data_iter = iter(test_loader)\n",
        "images, labels = next(data_iter)\n",
        "# 取batch中的一张图像，显示图像和真实标签\n",
        "idx = 4\n",
        "image = images[idx].numpy()\n",
        "image = np.transpose(image, (1,2,0))\n",
        "plt.imshow(image)\n",
        "print('true:',classes[labels[idx].numpy()])\n",
        "# 转换为（B,C,H,W）大小\n",
        "imagebatch = image.reshape(-1,3,32,32)\n",
        "\n",
        "# 转换为torch tensor\n",
        "image_tensor = torch.from_numpy(imagebatch)\n",
        "image_tensor = image_tensor.cuda()\n",
        "# 调用模型进行评估\n",
        "model.eval()\n",
        "output = model(image_tensor)\n",
        "precise, predicted = torch.max(output.data, 1)\n",
        "pre = predicted.cpu().numpy()\n",
        "pci = precise.cpu().numpy()\n",
        "print(pre) # 查看预测结果ID\n",
        "print('result:',classes[pre[0]])\n",
        "#print(pci[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "OLPAo19Dn2kL",
        "outputId": "3d35e21b-16dc-46d1-9980-ed63f8170f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true: frog\n",
            "[8]\n",
            "result: ship\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}